{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import numpy\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import face_recognition\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics \n",
    "from statistics import mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks():\n",
    "    csvfile = open(\"facial_landmarks.csv\",'w', newline='')\n",
    "    obj = csv.writer(csvfile)\n",
    "    headers=[('x1','y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4', 'x5','y5', 'x6', 'y6', 'x7', 'y7', 'x8', 'y8','x9','y9', 'x10', 'y10', 'x11', 'y11', 'x12', 'y12','x13','y13', 'x14', 'y14', 'x15', 'y15', 'x16', 'y16','x17','y17', 'x18', 'y18', 'x19', 'y19', 'x20', 'y20', 'x21', 'y21', 'x22', 'y22','x23','y23', 'x24', 'y24', 'x25', 'y25', 'x26', 'y26','x27','y27', 'x28', 'y28', 'x29', 'y29', 'x30', 'y30','x31','y31', 'x32', 'y32', 'x33', 'y33', 'x34', 'y34','x35','y35', 'x36', 'y36', 'x37', 'y37', 'x38', 'y38','x39','y39', 'x40', 'y40', 'x41', 'y41', 'x42', 'y42', 'x43', 'y43','x44', 'y44', 'x45', 'y45', 'x46', 'y46', 'x47', 'y47', 'x48', 'y48','x49','y49', 'x50', 'y50', 'x51', 'y51', 'x52', 'y52', 'x53', 'y53','x54','y54', 'x55', 'y55', 'x56', 'y56', 'x57', 'y57','x58','y58', 'x59', 'y59', 'x60', 'y60', 'x61', 'y61','x62','y62', 'x63', 'y63', 'x64', 'y64', 'x65', 'y65','x66','y66', 'x67', 'y67', 'x68', 'y68', 'target' )]\n",
    "    obj.writerows(headers)\n",
    "\n",
    "\n",
    "    pathlist = Path(\"./happy\").glob('**/*.JPG')\n",
    "    countFaces = 0\n",
    "    countImages = 0 \n",
    "    #iterate through all the images in the folder\n",
    "    for path in pathlist:\n",
    "        path_in_str = str(path)\n",
    "        image = face_recognition.load_image_file(path_in_str)\n",
    "        pic = cv2.imread(path_in_str)\n",
    "\n",
    "        gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "        gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        faces = detector(gray_image)\n",
    "        for face in faces:\n",
    "            countFaces += 1\n",
    "\n",
    "            landmarks = predictor(gray_image, face)\n",
    "            landmark_list = []\n",
    "            for n in range(0, 68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "                landmark_list.append(x)\n",
    "                landmark_list.append(y)\n",
    "\n",
    "\n",
    "            landmark_list.append(\"happy\")\n",
    "            obj.writerow(landmark_list)\n",
    "        countImages += 1\n",
    "        \n",
    "    pathlist = Path(\"./happy\").glob('**/*.jpg')\n",
    "\n",
    "    #iterate through all the images in the folder\n",
    "    for path in pathlist:\n",
    "        path_in_str = str(path)\n",
    "        image = face_recognition.load_image_file(path_in_str)\n",
    "        pic = cv2.imread(path_in_str)\n",
    "\n",
    "        gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "        gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        faces = detector(gray_image)\n",
    "        for face in faces:\n",
    "            countFaces += 1\n",
    "            landmarks = predictor(gray_image, face)\n",
    "            landmark_list = []\n",
    "            for n in range(0, 68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "                landmark_list.append(x)\n",
    "                landmark_list.append(y)\n",
    "\n",
    "\n",
    "            landmark_list.append(\"happy\")\n",
    "            obj.writerow(landmark_list)\n",
    "        countImages += 1\n",
    "    pathlist = Path(\"./happy\").glob('**/*.png')\n",
    "\n",
    "\n",
    "    #iterate through all the images in the folder\n",
    "    for path in pathlist:\n",
    "        path_in_str = str(path)\n",
    "        image = face_recognition.load_image_file(path_in_str)\n",
    "        pic = cv2.imread(path_in_str)\n",
    "\n",
    "        gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "        gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        faces = detector(gray_image)\n",
    "        for face in faces:\n",
    "            countFaces += 1\n",
    "\n",
    "            landmarks = predictor(gray_image, face)\n",
    "            landmark_list = []\n",
    "            for n in range(0, 68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "                landmark_list.append(x)\n",
    "                landmark_list.append(y)\n",
    "\n",
    "\n",
    "            landmark_list.append(\"happy\")\n",
    "            obj.writerow(landmark_list)\n",
    "        countImages += 1\n",
    "\n",
    "    pathlist = Path(\"./happy\").glob('**/*.jpeg')\n",
    "    #iterate through all the images in the folder\n",
    "    for path in pathlist:\n",
    "        path_in_str = str(path)\n",
    "        image = face_recognition.load_image_file(path_in_str)\n",
    "        pic = cv2.imread(path_in_str)\n",
    "\n",
    "        gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "        gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        faces = detector(gray_image)\n",
    "        for face in faces:\n",
    "            countFaces += 1\n",
    "\n",
    "            landmarks = predictor(gray_image, face)\n",
    "            landmark_list = []\n",
    "            for n in range(0, 68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "                landmark_list.append(x)\n",
    "                landmark_list.append(y)\n",
    "\n",
    "\n",
    "            landmark_list.append(\"happy\")\n",
    "            obj.writerow(landmark_list)\n",
    "        countImages += 1\n",
    "        \n",
    "    pathlist = Path(\"./sad\").glob('**/*.JPG')\n",
    "    countFacesSad = 0\n",
    "    countImagesSad = 0\n",
    "    #iterate through all the images in the folder\n",
    "    for path in pathlist:\n",
    "        path_in_str = str(path)\n",
    "        image = face_recognition.load_image_file(path_in_str)\n",
    "        pic = cv2.imread(path_in_str)\n",
    "\n",
    "        gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "        gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        faces = detector(gray_image)\n",
    "        for face in faces:\n",
    "            countFacesSad += 1\n",
    "\n",
    "            landmarks = predictor(gray_image, face)\n",
    "            landmark_list = []\n",
    "            for n in range(0, 68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "                landmark_list.append(x)\n",
    "                landmark_list.append(y)\n",
    "\n",
    "\n",
    "            landmark_list.append(\"sad\")\n",
    "            obj.writerow(landmark_list)\n",
    "        countImagesSad += 1\n",
    "    \n",
    "    pathlist = Path(\"./sad\").glob('**/*.jpg')\n",
    "    #iterate through all the images in the folder\n",
    "    for path in pathlist:\n",
    "        path_in_str = str(path)\n",
    "        image = face_recognition.load_image_file(path_in_str)\n",
    "        pic = cv2.imread(path_in_str)\n",
    "\n",
    "        gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "        gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        faces = detector(gray_image)\n",
    "        for face in faces:\n",
    "            countFacesSad += 1\n",
    "\n",
    "            landmarks = predictor(gray_image, face)\n",
    "            landmark_list = []\n",
    "            for n in range(0, 68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "                landmark_list.append(x)\n",
    "                landmark_list.append(y)\n",
    "\n",
    "\n",
    "            landmark_list.append(\"sad\")\n",
    "            obj.writerow(landmark_list)\n",
    "        countImagesSad += 1\n",
    "    \n",
    "    pathlist = Path(\"./sad\").glob('**/*.png')\n",
    "\n",
    "    #iterate through all the images in the folder\n",
    "    for path in pathlist:\n",
    "        path_in_str = str(path)\n",
    "        image = face_recognition.load_image_file(path_in_str)\n",
    "        pic = cv2.imread(path_in_str)\n",
    "\n",
    "        gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "        gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        faces = detector(gray_image)\n",
    "        for face in faces:\n",
    "            countFacesSad += 1\n",
    "\n",
    "            landmarks = predictor(gray_image, face)\n",
    "            landmark_list = []\n",
    "            for n in range(0, 68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "                landmark_list.append(x)\n",
    "                landmark_list.append(y)\n",
    "\n",
    "\n",
    "            landmark_list.append(\"sad\")\n",
    "            obj.writerow(landmark_list)\n",
    "        countImagesSad += 1\n",
    "    \n",
    "    pathlist = Path(\"./sad\").glob('**/*.jpeg')\n",
    "    #iterate through all the images in the folder\n",
    "    for path in pathlist:\n",
    "        path_in_str = str(path)\n",
    "        image = face_recognition.load_image_file(path_in_str)\n",
    "        pic = cv2.imread(path_in_str)\n",
    "\n",
    "        gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "        gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        faces = detector(gray_image)\n",
    "        for face in faces:\n",
    "            countFacesSad += 1\n",
    "\n",
    "            landmarks = predictor(gray_image, face)\n",
    "            landmark_list = []\n",
    "            for n in range(0, 68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "                landmark_list.append(x)\n",
    "                landmark_list.append(y)\n",
    "\n",
    "\n",
    "            landmark_list.append(\"sad\")\n",
    "            obj.writerow(landmark_list)\n",
    "        countImagesSad += 1\n",
    "        \n",
    "    csvfile.close()\n",
    "    print(f\"Total happy images: {countImages}, Total happy faces: {countFaces}, Total sad images: {countImagesSad}, Total sad Faces: {countFacesSad}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total happy images: 480, Total happy faces: 804, Total sad images: 508, Total sad Faces: 381\n"
     ]
    }
   ],
   "source": [
    "get_landmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>y5</th>\n",
       "      <th>...</th>\n",
       "      <th>y64</th>\n",
       "      <th>x65</th>\n",
       "      <th>y65</th>\n",
       "      <th>x66</th>\n",
       "      <th>y66</th>\n",
       "      <th>x67</th>\n",
       "      <th>y67</th>\n",
       "      <th>x68</th>\n",
       "      <th>y68</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2198</td>\n",
       "      <td>1666</td>\n",
       "      <td>2187</td>\n",
       "      <td>1727</td>\n",
       "      <td>2183</td>\n",
       "      <td>1791</td>\n",
       "      <td>2188</td>\n",
       "      <td>1857</td>\n",
       "      <td>2199</td>\n",
       "      <td>1926</td>\n",
       "      <td>...</td>\n",
       "      <td>1916</td>\n",
       "      <td>2512</td>\n",
       "      <td>1935</td>\n",
       "      <td>2400</td>\n",
       "      <td>1976</td>\n",
       "      <td>2359</td>\n",
       "      <td>1975</td>\n",
       "      <td>2322</td>\n",
       "      <td>1964</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1321</td>\n",
       "      <td>1650</td>\n",
       "      <td>1295</td>\n",
       "      <td>1724</td>\n",
       "      <td>1274</td>\n",
       "      <td>1797</td>\n",
       "      <td>1260</td>\n",
       "      <td>1874</td>\n",
       "      <td>1262</td>\n",
       "      <td>1951</td>\n",
       "      <td>...</td>\n",
       "      <td>2055</td>\n",
       "      <td>1536</td>\n",
       "      <td>2071</td>\n",
       "      <td>1461</td>\n",
       "      <td>2063</td>\n",
       "      <td>1432</td>\n",
       "      <td>2056</td>\n",
       "      <td>1408</td>\n",
       "      <td>2041</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3179</td>\n",
       "      <td>1353</td>\n",
       "      <td>3182</td>\n",
       "      <td>1411</td>\n",
       "      <td>3188</td>\n",
       "      <td>1468</td>\n",
       "      <td>3198</td>\n",
       "      <td>1525</td>\n",
       "      <td>3219</td>\n",
       "      <td>1575</td>\n",
       "      <td>...</td>\n",
       "      <td>1564</td>\n",
       "      <td>3476</td>\n",
       "      <td>1558</td>\n",
       "      <td>3424</td>\n",
       "      <td>1564</td>\n",
       "      <td>3397</td>\n",
       "      <td>1569</td>\n",
       "      <td>3372</td>\n",
       "      <td>1566</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4294</td>\n",
       "      <td>1458</td>\n",
       "      <td>4288</td>\n",
       "      <td>1516</td>\n",
       "      <td>4290</td>\n",
       "      <td>1578</td>\n",
       "      <td>4306</td>\n",
       "      <td>1636</td>\n",
       "      <td>4330</td>\n",
       "      <td>1689</td>\n",
       "      <td>...</td>\n",
       "      <td>1675</td>\n",
       "      <td>4573</td>\n",
       "      <td>1687</td>\n",
       "      <td>4500</td>\n",
       "      <td>1722</td>\n",
       "      <td>4465</td>\n",
       "      <td>1725</td>\n",
       "      <td>4434</td>\n",
       "      <td>1717</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>266</td>\n",
       "      <td>1344</td>\n",
       "      <td>274</td>\n",
       "      <td>1398</td>\n",
       "      <td>286</td>\n",
       "      <td>1454</td>\n",
       "      <td>301</td>\n",
       "      <td>1509</td>\n",
       "      <td>331</td>\n",
       "      <td>1561</td>\n",
       "      <td>...</td>\n",
       "      <td>1509</td>\n",
       "      <td>622</td>\n",
       "      <td>1514</td>\n",
       "      <td>577</td>\n",
       "      <td>1542</td>\n",
       "      <td>552</td>\n",
       "      <td>1547</td>\n",
       "      <td>522</td>\n",
       "      <td>1546</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     x1    y1    x2    y2    x3    y3    x4    y4    x5    y5  ...   y64  \\\n",
       "0  2198  1666  2187  1727  2183  1791  2188  1857  2199  1926  ...  1916   \n",
       "1  1321  1650  1295  1724  1274  1797  1260  1874  1262  1951  ...  2055   \n",
       "2  3179  1353  3182  1411  3188  1468  3198  1525  3219  1575  ...  1564   \n",
       "3  4294  1458  4288  1516  4290  1578  4306  1636  4330  1689  ...  1675   \n",
       "4   266  1344   274  1398   286  1454   301  1509   331  1561  ...  1509   \n",
       "\n",
       "    x65   y65   x66   y66   x67   y67   x68   y68  target  \n",
       "0  2512  1935  2400  1976  2359  1975  2322  1964   happy  \n",
       "1  1536  2071  1461  2063  1432  2056  1408  2041   happy  \n",
       "2  3476  1558  3424  1564  3397  1569  3372  1566   happy  \n",
       "3  4573  1687  4500  1722  4465  1725  4434  1717   happy  \n",
       "4   622  1514   577  1542   552  1547   522  1546   happy  \n",
       "\n",
       "[5 rows x 137 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('facial_landmarks.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "encoder = LabelEncoder()\n",
    "df['target'] = encoder.fit_transform(df['target'].values.reshape(-1,1))\n",
    "y = df['target']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['x1', 'y1','x2', 'y2', 'x3', 'y3', 'x4', 'y4', 'x5','y5', 'x6', 'y6', 'x7', 'y7', 'x8', 'y8','x9','y9', 'x10', 'y10', 'x11', 'y11', 'x12', 'y12','x13','y13', 'x14', 'y14', 'x15', 'y15', 'x16', 'y16','x17','y17', 'x18', 'y18', 'x19', 'y19', 'x20', 'y20', 'x21', 'y21', 'x22', 'y22','x23','y23', 'x24', 'y24', 'x25', 'y25', 'x26', 'y26','x27','y27', 'x28', 'y28', 'x29', 'y29', 'x30', 'y30','x31','y31', 'x32', 'y32', 'x33', 'y33', 'x34', 'y34','x35','y35', 'x36', 'y36', 'x37', 'y37', 'x38', 'y38','x39','y39', 'x40', 'y40', 'x41', 'y41', 'x42', 'y42', 'x43', 'y43','x44', 'y44', 'x45', 'y45', 'x46', 'y46', 'x47', 'y47', 'x48', 'y48','x49','y49', 'x50', 'y50', 'x51', 'y51', 'x52', 'y52', 'x53', 'y53','x54','y54', 'x55', 'y55', 'x56', 'y56', 'x57', 'y57','x58','y58', 'x59', 'y59', 'x60', 'y60', 'x61', 'y61','x62','y62', 'x63', 'y63', 'x64', 'y64', 'x65', 'y65','x66','y66', 'x67', 'y67', 'x68', 'y68']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00556395 0.00514179 0.00553162 0.00505733 0.04102315 0.0023583\n",
      " 0.0075856  0.00215042 0.00379814 0.00553044 0.01641528 0.00150642\n",
      " 0.0087127  0.01298148 0.00522538 0.00788718 0.04669537 0.00299027\n",
      " 0.00653989 0.03151537 0.0045437  0.00372341 0.00502173 0.00350612\n",
      " 0.00233793 0.0024696  0.00492513 0.00399193 0.00450183 0.00226901\n",
      " 0.00966963 0.00505187 0.00510332 0.00209512 0.00210965 0.00521521\n",
      " 0.00530094 0.00694506 0.00478561 0.00586363 0.00369053 0.00429454\n",
      " 0.02932927 0.02710948 0.00241043 0.00309874 0.00328435 0.00628597\n",
      " 0.00438886 0.00340279 0.00432187 0.00314793 0.0042176  0.00542451\n",
      " 0.00686898 0.00441549 0.01223868 0.00326813 0.00395343 0.00365318\n",
      " 0.02485545 0.00387015 0.00862578 0.00176643 0.04733386 0.0033601\n",
      " 0.00506275 0.01105137 0.00366617 0.00270714 0.00489139 0.00334573\n",
      " 0.0098516  0.00505076 0.00313087 0.00196666 0.00552754 0.00230885\n",
      " 0.00417629 0.00891602 0.02817924 0.00282056 0.00342988 0.00360132\n",
      " 0.00215659 0.00346054 0.0038123  0.00168579 0.00378043 0.0028955\n",
      " 0.00350036 0.00538513 0.00296986 0.00409949 0.0105259  0.02107088\n",
      " 0.00143557 0.00410484 0.01080758 0.00254673 0.00249263 0.00559668\n",
      " 0.00484733 0.00215606 0.00468777 0.00223661 0.00845696 0.00263289\n",
      " 0.00601041 0.00274199 0.00325449 0.00408382 0.00435056 0.00836282\n",
      " 0.00371206 0.0037813  0.00468415 0.0037588  0.00248516 0.0029199\n",
      " 0.07634049 0.00368688 0.00592235 0.00353714 0.00334292 0.00562841\n",
      " 0.00235519 0.01128684 0.00357976 0.00527684 0.00467063 0.00231751\n",
      " 0.04277806 0.00220345 0.00181081 0.00583164]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120    0.076340\n",
      "64     0.047334\n",
      "16     0.046695\n",
      "132    0.042778\n",
      "4      0.041023\n",
      "19     0.031515\n",
      "42     0.029329\n",
      "80     0.028179\n",
      "43     0.027109\n",
      "60     0.024855\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     0.001436\n",
      "11     0.001506\n",
      "87     0.001686\n",
      "63     0.001766\n",
      "134    0.001811\n",
      "75     0.001967\n",
      "33     0.002095\n",
      "34     0.002110\n",
      "7      0.002150\n",
      "103    0.002156\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(df[features], df['target'])\n",
    "print(model.feature_importances_) \n",
    "#use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()\n",
    "print(feat_importances.nlargest(10))\n",
    "\n",
    "\n",
    "feat_importances = pd.Series(model.feature_importances_)\n",
    "feat_importances.nsmallest(10).plot(kind='barh')\n",
    "plt.show()\n",
    "print(feat_importances.nsmallest(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = ['x6','y6','x12','y12','x14','y14','x15', 'y15','x23','y23','x29','y29', 'x33','y33','x35','y35','x37','y37','x39','y39','x44','y44', 'x51','y51', 'x54','y54', 'x56','y56','x57','y57','x59','y59','x66','y66', 'x68','y68']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "for columns in df:\n",
    "    df[columns] = scaler.fit_transform(np.array(df[columns]).reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_streamed_ft = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_streamed_ft = df_streamed_ft.drop(drop, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>y5</th>\n",
       "      <th>...</th>\n",
       "      <th>y64</th>\n",
       "      <th>x65</th>\n",
       "      <th>y65</th>\n",
       "      <th>x66</th>\n",
       "      <th>y66</th>\n",
       "      <th>x67</th>\n",
       "      <th>y67</th>\n",
       "      <th>x68</th>\n",
       "      <th>y68</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.361444</td>\n",
       "      <td>1.510731</td>\n",
       "      <td>1.347821</td>\n",
       "      <td>1.526705</td>\n",
       "      <td>1.338147</td>\n",
       "      <td>1.543050</td>\n",
       "      <td>1.331516</td>\n",
       "      <td>1.559810</td>\n",
       "      <td>1.321326</td>\n",
       "      <td>1.585019</td>\n",
       "      <td>...</td>\n",
       "      <td>1.645717</td>\n",
       "      <td>1.414456</td>\n",
       "      <td>1.668856</td>\n",
       "      <td>1.349429</td>\n",
       "      <td>1.694612</td>\n",
       "      <td>1.330916</td>\n",
       "      <td>1.686256</td>\n",
       "      <td>1.316106</td>\n",
       "      <td>1.672984</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.336498</td>\n",
       "      <td>1.483107</td>\n",
       "      <td>0.304608</td>\n",
       "      <td>1.521599</td>\n",
       "      <td>0.274866</td>\n",
       "      <td>1.553093</td>\n",
       "      <td>0.247679</td>\n",
       "      <td>1.587744</td>\n",
       "      <td>0.230476</td>\n",
       "      <td>1.625352</td>\n",
       "      <td>...</td>\n",
       "      <td>1.874539</td>\n",
       "      <td>0.329436</td>\n",
       "      <td>1.892320</td>\n",
       "      <td>0.291346</td>\n",
       "      <td>1.836094</td>\n",
       "      <td>0.280228</td>\n",
       "      <td>1.817780</td>\n",
       "      <td>0.274470</td>\n",
       "      <td>1.798140</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.507934</td>\n",
       "      <td>0.970321</td>\n",
       "      <td>2.511495</td>\n",
       "      <td>0.988845</td>\n",
       "      <td>2.513722</td>\n",
       "      <td>1.002400</td>\n",
       "      <td>2.511124</td>\n",
       "      <td>1.014280</td>\n",
       "      <td>2.508804</td>\n",
       "      <td>1.018743</td>\n",
       "      <td>...</td>\n",
       "      <td>1.066253</td>\n",
       "      <td>2.486136</td>\n",
       "      <td>1.049401</td>\n",
       "      <td>2.503292</td>\n",
       "      <td>1.024606</td>\n",
       "      <td>2.507415</td>\n",
       "      <td>1.027013</td>\n",
       "      <td>2.512734</td>\n",
       "      <td>1.026070</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.811030</td>\n",
       "      <td>1.151609</td>\n",
       "      <td>3.804986</td>\n",
       "      <td>1.167564</td>\n",
       "      <td>3.802761</td>\n",
       "      <td>1.186522</td>\n",
       "      <td>3.805188</td>\n",
       "      <td>1.196671</td>\n",
       "      <td>3.802224</td>\n",
       "      <td>1.202661</td>\n",
       "      <td>...</td>\n",
       "      <td>1.248982</td>\n",
       "      <td>3.705673</td>\n",
       "      <td>1.261363</td>\n",
       "      <td>3.715750</td>\n",
       "      <td>1.281550</td>\n",
       "      <td>3.717917</td>\n",
       "      <td>1.280318</td>\n",
       "      <td>3.723037</td>\n",
       "      <td>1.271507</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.896476</td>\n",
       "      <td>0.954782</td>\n",
       "      <td>-0.889474</td>\n",
       "      <td>0.966718</td>\n",
       "      <td>-0.880824</td>\n",
       "      <td>0.978966</td>\n",
       "      <td>-0.872365</td>\n",
       "      <td>0.987989</td>\n",
       "      <td>-0.853389</td>\n",
       "      <td>0.996156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975712</td>\n",
       "      <td>-0.686659</td>\n",
       "      <td>0.977104</td>\n",
       "      <td>-0.704762</td>\n",
       "      <td>0.988829</td>\n",
       "      <td>-0.717190</td>\n",
       "      <td>0.991290</td>\n",
       "      <td>-0.735256</td>\n",
       "      <td>0.993562</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.057353</td>\n",
       "      <td>1.194772</td>\n",
       "      <td>-0.054436</td>\n",
       "      <td>1.186287</td>\n",
       "      <td>-0.047979</td>\n",
       "      <td>1.173131</td>\n",
       "      <td>-0.033792</td>\n",
       "      <td>1.157235</td>\n",
       "      <td>-0.018661</td>\n",
       "      <td>1.141355</td>\n",
       "      <td>...</td>\n",
       "      <td>1.137040</td>\n",
       "      <td>-0.006298</td>\n",
       "      <td>1.095409</td>\n",
       "      <td>-0.039939</td>\n",
       "      <td>1.136816</td>\n",
       "      <td>-0.048467</td>\n",
       "      <td>1.147170</td>\n",
       "      <td>-0.058307</td>\n",
       "      <td>1.154478</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.920080</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>1.918548</td>\n",
       "      <td>0.011846</td>\n",
       "      <td>1.917162</td>\n",
       "      <td>0.008139</td>\n",
       "      <td>1.918984</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>1.909244</td>\n",
       "      <td>-0.000878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035058</td>\n",
       "      <td>1.811334</td>\n",
       "      <td>-0.031770</td>\n",
       "      <td>1.827201</td>\n",
       "      <td>-0.042200</td>\n",
       "      <td>1.838692</td>\n",
       "      <td>-0.039792</td>\n",
       "      <td>1.850600</td>\n",
       "      <td>-0.033697</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.354070</td>\n",
       "      <td>0.188193</td>\n",
       "      <td>3.351212</td>\n",
       "      <td>0.176948</td>\n",
       "      <td>3.346567</td>\n",
       "      <td>0.168828</td>\n",
       "      <td>3.339185</td>\n",
       "      <td>0.164764</td>\n",
       "      <td>3.326068</td>\n",
       "      <td>0.162068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164133</td>\n",
       "      <td>3.194290</td>\n",
       "      <td>0.158832</td>\n",
       "      <td>3.218822</td>\n",
       "      <td>0.183846</td>\n",
       "      <td>3.233943</td>\n",
       "      <td>0.182662</td>\n",
       "      <td>3.250084</td>\n",
       "      <td>0.182482</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.141499</td>\n",
       "      <td>0.162294</td>\n",
       "      <td>-0.139811</td>\n",
       "      <td>0.165034</td>\n",
       "      <td>-0.140387</td>\n",
       "      <td>0.168828</td>\n",
       "      <td>-0.140074</td>\n",
       "      <td>0.171337</td>\n",
       "      <td>-0.132753</td>\n",
       "      <td>0.170134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201996</td>\n",
       "      <td>-0.095234</td>\n",
       "      <td>0.186765</td>\n",
       "      <td>-0.089519</td>\n",
       "      <td>0.188725</td>\n",
       "      <td>-0.089270</td>\n",
       "      <td>0.189157</td>\n",
       "      <td>-0.091356</td>\n",
       "      <td>0.188984</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.175621</td>\n",
       "      <td>0.212364</td>\n",
       "      <td>1.179410</td>\n",
       "      <td>0.192267</td>\n",
       "      <td>1.181404</td>\n",
       "      <td>0.168828</td>\n",
       "      <td>1.179685</td>\n",
       "      <td>0.145046</td>\n",
       "      <td>1.175802</td>\n",
       "      <td>0.120121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172364</td>\n",
       "      <td>1.055377</td>\n",
       "      <td>0.155546</td>\n",
       "      <td>1.097022</td>\n",
       "      <td>0.136686</td>\n",
       "      <td>1.115564</td>\n",
       "      <td>0.135573</td>\n",
       "      <td>1.131483</td>\n",
       "      <td>0.138596</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.414800</td>\n",
       "      <td>1.350162</td>\n",
       "      <td>0.415712</td>\n",
       "      <td>1.347986</td>\n",
       "      <td>0.421082</td>\n",
       "      <td>1.343863</td>\n",
       "      <td>0.422868</td>\n",
       "      <td>1.337983</td>\n",
       "      <td>0.424896</td>\n",
       "      <td>1.330114</td>\n",
       "      <td>...</td>\n",
       "      <td>1.334584</td>\n",
       "      <td>0.459505</td>\n",
       "      <td>1.323802</td>\n",
       "      <td>0.446847</td>\n",
       "      <td>1.348225</td>\n",
       "      <td>0.440041</td>\n",
       "      <td>1.351763</td>\n",
       "      <td>0.432881</td>\n",
       "      <td>1.349527</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.188247</td>\n",
       "      <td>0.562854</td>\n",
       "      <td>-0.184252</td>\n",
       "      <td>0.571833</td>\n",
       "      <td>-0.181327</td>\n",
       "      <td>0.580592</td>\n",
       "      <td>-0.175111</td>\n",
       "      <td>0.585414</td>\n",
       "      <td>-0.163022</td>\n",
       "      <td>0.583145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.585561</td>\n",
       "      <td>-0.096346</td>\n",
       "      <td>0.581113</td>\n",
       "      <td>-0.081631</td>\n",
       "      <td>0.559505</td>\n",
       "      <td>-0.080203</td>\n",
       "      <td>0.557748</td>\n",
       "      <td>-0.082239</td>\n",
       "      <td>0.561203</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.684769</td>\n",
       "      <td>0.640549</td>\n",
       "      <td>0.689380</td>\n",
       "      <td>0.650129</td>\n",
       "      <td>0.690119</td>\n",
       "      <td>0.657589</td>\n",
       "      <td>0.687988</td>\n",
       "      <td>0.664286</td>\n",
       "      <td>0.689169</td>\n",
       "      <td>0.671878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.682687</td>\n",
       "      <td>0.712973</td>\n",
       "      <td>0.692845</td>\n",
       "      <td>0.734186</td>\n",
       "      <td>0.687977</td>\n",
       "      <td>0.738133</td>\n",
       "      <td>0.687649</td>\n",
       "      <td>0.738306</td>\n",
       "      <td>0.684734</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.530905</td>\n",
       "      <td>0.726877</td>\n",
       "      <td>1.539623</td>\n",
       "      <td>0.723319</td>\n",
       "      <td>1.546359</td>\n",
       "      <td>0.717847</td>\n",
       "      <td>1.551087</td>\n",
       "      <td>0.711938</td>\n",
       "      <td>1.554165</td>\n",
       "      <td>0.704144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.662933</td>\n",
       "      <td>1.503392</td>\n",
       "      <td>0.655053</td>\n",
       "      <td>1.522959</td>\n",
       "      <td>0.626180</td>\n",
       "      <td>1.534933</td>\n",
       "      <td>0.627570</td>\n",
       "      <td>1.544035</td>\n",
       "      <td>0.634346</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.780776</td>\n",
       "      <td>0.937516</td>\n",
       "      <td>-0.777200</td>\n",
       "      <td>0.951399</td>\n",
       "      <td>-0.775548</td>\n",
       "      <td>0.962227</td>\n",
       "      <td>-0.764915</td>\n",
       "      <td>0.971558</td>\n",
       "      <td>-0.742790</td>\n",
       "      <td>0.970343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.942788</td>\n",
       "      <td>-0.608840</td>\n",
       "      <td>0.952457</td>\n",
       "      <td>-0.606729</td>\n",
       "      <td>0.941668</td>\n",
       "      <td>-0.611781</td>\n",
       "      <td>0.936083</td>\n",
       "      <td>-0.617873</td>\n",
       "      <td>0.935047</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.575719</td>\n",
       "      <td>0.859821</td>\n",
       "      <td>2.568802</td>\n",
       "      <td>0.850976</td>\n",
       "      <td>2.560511</td>\n",
       "      <td>0.841711</td>\n",
       "      <td>2.546162</td>\n",
       "      <td>0.831889</td>\n",
       "      <td>2.525103</td>\n",
       "      <td>0.829984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.883524</td>\n",
       "      <td>2.416099</td>\n",
       "      <td>0.901521</td>\n",
       "      <td>2.444698</td>\n",
       "      <td>0.879872</td>\n",
       "      <td>2.457544</td>\n",
       "      <td>0.876004</td>\n",
       "      <td>2.469427</td>\n",
       "      <td>0.873282</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.332992</td>\n",
       "      <td>0.492066</td>\n",
       "      <td>0.332676</td>\n",
       "      <td>0.502047</td>\n",
       "      <td>0.328673</td>\n",
       "      <td>0.506943</td>\n",
       "      <td>0.317754</td>\n",
       "      <td>0.514758</td>\n",
       "      <td>0.310805</td>\n",
       "      <td>0.521839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524652</td>\n",
       "      <td>0.359452</td>\n",
       "      <td>0.540035</td>\n",
       "      <td>0.356702</td>\n",
       "      <td>0.526980</td>\n",
       "      <td>0.355034</td>\n",
       "      <td>0.522026</td>\n",
       "      <td>0.354245</td>\n",
       "      <td>0.518942</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.064595</td>\n",
       "      <td>1.146429</td>\n",
       "      <td>1.054271</td>\n",
       "      <td>1.131820</td>\n",
       "      <td>1.044546</td>\n",
       "      <td>1.114547</td>\n",
       "      <td>1.043038</td>\n",
       "      <td>1.098081</td>\n",
       "      <td>1.036098</td>\n",
       "      <td>1.086502</td>\n",
       "      <td>...</td>\n",
       "      <td>1.137040</td>\n",
       "      <td>1.004239</td>\n",
       "      <td>1.139773</td>\n",
       "      <td>0.982086</td>\n",
       "      <td>1.154704</td>\n",
       "      <td>0.979553</td>\n",
       "      <td>1.150418</td>\n",
       "      <td>0.982190</td>\n",
       "      <td>1.144725</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.603767</td>\n",
       "      <td>0.545589</td>\n",
       "      <td>2.601548</td>\n",
       "      <td>0.556514</td>\n",
       "      <td>2.596773</td>\n",
       "      <td>0.565528</td>\n",
       "      <td>2.590543</td>\n",
       "      <td>0.567339</td>\n",
       "      <td>2.590298</td>\n",
       "      <td>0.567012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534529</td>\n",
       "      <td>2.529493</td>\n",
       "      <td>0.553180</td>\n",
       "      <td>2.567521</td>\n",
       "      <td>0.559505</td>\n",
       "      <td>2.581088</td>\n",
       "      <td>0.556125</td>\n",
       "      <td>2.591369</td>\n",
       "      <td>0.551450</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.795795</td>\n",
       "      <td>0.243442</td>\n",
       "      <td>0.806332</td>\n",
       "      <td>0.313115</td>\n",
       "      <td>0.814110</td>\n",
       "      <td>0.379731</td>\n",
       "      <td>0.817628</td>\n",
       "      <td>0.445745</td>\n",
       "      <td>0.828872</td>\n",
       "      <td>0.504092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409417</td>\n",
       "      <td>1.026473</td>\n",
       "      <td>0.431589</td>\n",
       "      <td>1.002369</td>\n",
       "      <td>0.463557</td>\n",
       "      <td>0.985220</td>\n",
       "      <td>0.470066</td>\n",
       "      <td>0.969654</td>\n",
       "      <td>0.466929</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.169374</td>\n",
       "      <td>0.274520</td>\n",
       "      <td>0.164265</td>\n",
       "      <td>0.301201</td>\n",
       "      <td>0.157893</td>\n",
       "      <td>0.326168</td>\n",
       "      <td>0.150741</td>\n",
       "      <td>0.353728</td>\n",
       "      <td>0.148982</td>\n",
       "      <td>0.378253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411064</td>\n",
       "      <td>0.242723</td>\n",
       "      <td>0.438162</td>\n",
       "      <td>0.237259</td>\n",
       "      <td>0.455426</td>\n",
       "      <td>0.230357</td>\n",
       "      <td>0.450581</td>\n",
       "      <td>0.224326</td>\n",
       "      <td>0.442548</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.215357</td>\n",
       "      <td>0.659541</td>\n",
       "      <td>1.208648</td>\n",
       "      <td>0.668852</td>\n",
       "      <td>1.202459</td>\n",
       "      <td>0.677675</td>\n",
       "      <td>1.192533</td>\n",
       "      <td>0.684004</td>\n",
       "      <td>1.179294</td>\n",
       "      <td>0.691238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.720550</td>\n",
       "      <td>1.156542</td>\n",
       "      <td>0.745425</td>\n",
       "      <td>1.171392</td>\n",
       "      <td>0.705865</td>\n",
       "      <td>1.175636</td>\n",
       "      <td>0.699015</td>\n",
       "      <td>1.180488</td>\n",
       "      <td>0.694486</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.203843</td>\n",
       "      <td>2.403359</td>\n",
       "      <td>-1.206415</td>\n",
       "      <td>2.370941</td>\n",
       "      <td>-1.208347</td>\n",
       "      <td>2.333102</td>\n",
       "      <td>-1.207560</td>\n",
       "      <td>2.292661</td>\n",
       "      <td>-1.204975</td>\n",
       "      <td>2.254549</td>\n",
       "      <td>...</td>\n",
       "      <td>2.322307</td>\n",
       "      <td>-1.158020</td>\n",
       "      <td>2.321174</td>\n",
       "      <td>-1.159997</td>\n",
       "      <td>2.271923</td>\n",
       "      <td>-1.160360</td>\n",
       "      <td>2.267559</td>\n",
       "      <td>-1.159204</td>\n",
       "      <td>2.267884</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.738529</td>\n",
       "      <td>2.356742</td>\n",
       "      <td>0.744348</td>\n",
       "      <td>2.307964</td>\n",
       "      <td>0.747435</td>\n",
       "      <td>2.252758</td>\n",
       "      <td>0.746384</td>\n",
       "      <td>2.195715</td>\n",
       "      <td>0.741557</td>\n",
       "      <td>2.138389</td>\n",
       "      <td>...</td>\n",
       "      <td>2.152748</td>\n",
       "      <td>0.664058</td>\n",
       "      <td>2.148647</td>\n",
       "      <td>0.691367</td>\n",
       "      <td>2.123936</td>\n",
       "      <td>0.699596</td>\n",
       "      <td>2.121421</td>\n",
       "      <td>0.707535</td>\n",
       "      <td>2.126473</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.335732</td>\n",
       "      <td>2.411991</td>\n",
       "      <td>1.336126</td>\n",
       "      <td>2.367537</td>\n",
       "      <td>1.333468</td>\n",
       "      <td>2.321385</td>\n",
       "      <td>1.326844</td>\n",
       "      <td>2.269657</td>\n",
       "      <td>1.315505</td>\n",
       "      <td>2.222282</td>\n",
       "      <td>...</td>\n",
       "      <td>2.261397</td>\n",
       "      <td>1.239919</td>\n",
       "      <td>2.253806</td>\n",
       "      <td>1.257030</td>\n",
       "      <td>2.245903</td>\n",
       "      <td>1.262910</td>\n",
       "      <td>2.243203</td>\n",
       "      <td>1.269380</td>\n",
       "      <td>2.245128</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.075878</td>\n",
       "      <td>2.028697</td>\n",
       "      <td>0.074212</td>\n",
       "      <td>1.986269</td>\n",
       "      <td>0.070164</td>\n",
       "      <td>1.939750</td>\n",
       "      <td>0.063146</td>\n",
       "      <td>1.893373</td>\n",
       "      <td>0.054683</td>\n",
       "      <td>1.846378</td>\n",
       "      <td>...</td>\n",
       "      <td>1.889355</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>1.887391</td>\n",
       "      <td>0.034431</td>\n",
       "      <td>1.863740</td>\n",
       "      <td>0.035407</td>\n",
       "      <td>1.858373</td>\n",
       "      <td>0.038563</td>\n",
       "      <td>1.858280</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.682605</td>\n",
       "      <td>2.306672</td>\n",
       "      <td>-0.690655</td>\n",
       "      <td>2.260305</td>\n",
       "      <td>-0.698346</td>\n",
       "      <td>2.209238</td>\n",
       "      <td>-0.705351</td>\n",
       "      <td>2.157922</td>\n",
       "      <td>-0.711357</td>\n",
       "      <td>2.109349</td>\n",
       "      <td>...</td>\n",
       "      <td>2.223535</td>\n",
       "      <td>-0.720010</td>\n",
       "      <td>2.222587</td>\n",
       "      <td>-0.711523</td>\n",
       "      <td>2.177602</td>\n",
       "      <td>-0.708122</td>\n",
       "      <td>2.171758</td>\n",
       "      <td>-0.707905</td>\n",
       "      <td>2.171985</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.180699</td>\n",
       "      <td>2.056322</td>\n",
       "      <td>2.181690</td>\n",
       "      <td>2.018609</td>\n",
       "      <td>2.180350</td>\n",
       "      <td>1.973227</td>\n",
       "      <td>2.173592</td>\n",
       "      <td>1.926236</td>\n",
       "      <td>2.160710</td>\n",
       "      <td>1.878644</td>\n",
       "      <td>...</td>\n",
       "      <td>1.904171</td>\n",
       "      <td>2.029227</td>\n",
       "      <td>1.905465</td>\n",
       "      <td>2.068340</td>\n",
       "      <td>1.863740</td>\n",
       "      <td>2.085779</td>\n",
       "      <td>1.859997</td>\n",
       "      <td>2.103601</td>\n",
       "      <td>1.863157</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.237966</td>\n",
       "      <td>0.923704</td>\n",
       "      <td>2.240166</td>\n",
       "      <td>0.947995</td>\n",
       "      <td>2.241176</td>\n",
       "      <td>0.968923</td>\n",
       "      <td>2.241332</td>\n",
       "      <td>0.986346</td>\n",
       "      <td>2.244532</td>\n",
       "      <td>0.991316</td>\n",
       "      <td>...</td>\n",
       "      <td>1.018513</td>\n",
       "      <td>2.271578</td>\n",
       "      <td>0.996822</td>\n",
       "      <td>2.248631</td>\n",
       "      <td>1.034363</td>\n",
       "      <td>2.239926</td>\n",
       "      <td>1.033508</td>\n",
       "      <td>2.231241</td>\n",
       "      <td>1.024445</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.843712</td>\n",
       "      <td>0.355668</td>\n",
       "      <td>0.846096</td>\n",
       "      <td>0.415241</td>\n",
       "      <td>0.849202</td>\n",
       "      <td>0.473466</td>\n",
       "      <td>0.852666</td>\n",
       "      <td>0.524617</td>\n",
       "      <td>0.864962</td>\n",
       "      <td>0.571852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575684</td>\n",
       "      <td>1.014244</td>\n",
       "      <td>0.602473</td>\n",
       "      <td>0.996735</td>\n",
       "      <td>0.595282</td>\n",
       "      <td>0.982953</td>\n",
       "      <td>0.591847</td>\n",
       "      <td>0.966235</td>\n",
       "      <td>0.582333</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>-1.212024</td>\n",
       "      <td>-1.251752</td>\n",
       "      <td>-1.213432</td>\n",
       "      <td>-1.268125</td>\n",
       "      <td>-1.216535</td>\n",
       "      <td>-1.279044</td>\n",
       "      <td>-1.220407</td>\n",
       "      <td>-1.287793</td>\n",
       "      <td>-1.224767</td>\n",
       "      <td>-1.293151</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.281235</td>\n",
       "      <td>-1.256962</td>\n",
       "      <td>-1.277252</td>\n",
       "      <td>-1.249016</td>\n",
       "      <td>-1.287890</td>\n",
       "      <td>-1.245367</td>\n",
       "      <td>-1.288457</td>\n",
       "      <td>-1.241259</td>\n",
       "      <td>-1.288514</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>-1.108010</td>\n",
       "      <td>-1.186143</td>\n",
       "      <td>-1.111683</td>\n",
       "      <td>-1.206850</td>\n",
       "      <td>-1.117108</td>\n",
       "      <td>-1.223807</td>\n",
       "      <td>-1.124637</td>\n",
       "      <td>-1.238498</td>\n",
       "      <td>-1.135124</td>\n",
       "      <td>-1.251204</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.236787</td>\n",
       "      <td>-1.213605</td>\n",
       "      <td>-1.236175</td>\n",
       "      <td>-1.199436</td>\n",
       "      <td>-1.252113</td>\n",
       "      <td>-1.194363</td>\n",
       "      <td>-1.254358</td>\n",
       "      <td>-1.188835</td>\n",
       "      <td>-1.252755</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>-1.152421</td>\n",
       "      <td>-1.039386</td>\n",
       "      <td>-1.151447</td>\n",
       "      <td>-1.031535</td>\n",
       "      <td>-1.151031</td>\n",
       "      <td>-1.022946</td>\n",
       "      <td>-1.149164</td>\n",
       "      <td>-1.013384</td>\n",
       "      <td>-1.147930</td>\n",
       "      <td>-1.004366</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.014550</td>\n",
       "      <td>-1.132451</td>\n",
       "      <td>-0.999566</td>\n",
       "      <td>-1.136334</td>\n",
       "      <td>-0.980533</td>\n",
       "      <td>-1.143359</td>\n",
       "      <td>-0.979944</td>\n",
       "      <td>-1.148947</td>\n",
       "      <td>-0.982937</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>-1.175795</td>\n",
       "      <td>-1.053198</td>\n",
       "      <td>-1.179516</td>\n",
       "      <td>-1.051960</td>\n",
       "      <td>-1.181443</td>\n",
       "      <td>-1.048054</td>\n",
       "      <td>-1.184201</td>\n",
       "      <td>-1.041318</td>\n",
       "      <td>-1.189841</td>\n",
       "      <td>-1.026952</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.001380</td>\n",
       "      <td>-1.185813</td>\n",
       "      <td>-0.989707</td>\n",
       "      <td>-1.190421</td>\n",
       "      <td>-1.000048</td>\n",
       "      <td>-1.194363</td>\n",
       "      <td>-1.002677</td>\n",
       "      <td>-1.195673</td>\n",
       "      <td>-1.004068</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>-0.613652</td>\n",
       "      <td>-0.904715</td>\n",
       "      <td>-0.627501</td>\n",
       "      <td>-0.832391</td>\n",
       "      <td>-0.638690</td>\n",
       "      <td>-0.755132</td>\n",
       "      <td>-0.638779</td>\n",
       "      <td>-0.681465</td>\n",
       "      <td>-0.625207</td>\n",
       "      <td>-0.618781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.514103</td>\n",
       "      <td>-0.440973</td>\n",
       "      <td>-0.509917</td>\n",
       "      <td>-0.444467</td>\n",
       "      <td>-0.526816</td>\n",
       "      <td>-0.447434</td>\n",
       "      <td>-0.530165</td>\n",
       "      <td>-0.450345</td>\n",
       "      <td>-0.535949</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>-1.022696</td>\n",
       "      <td>-1.051472</td>\n",
       "      <td>-1.022800</td>\n",
       "      <td>-1.072385</td>\n",
       "      <td>-1.024700</td>\n",
       "      <td>-1.091574</td>\n",
       "      <td>-1.031203</td>\n",
       "      <td>-1.107045</td>\n",
       "      <td>-1.041988</td>\n",
       "      <td>-1.118912</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.119907</td>\n",
       "      <td>-1.105770</td>\n",
       "      <td>-1.116227</td>\n",
       "      <td>-1.092388</td>\n",
       "      <td>-1.133399</td>\n",
       "      <td>-1.086687</td>\n",
       "      <td>-1.134201</td>\n",
       "      <td>-1.080569</td>\n",
       "      <td>-1.134100</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>-0.799475</td>\n",
       "      <td>-1.048018</td>\n",
       "      <td>-0.800590</td>\n",
       "      <td>-1.070683</td>\n",
       "      <td>-0.804791</td>\n",
       "      <td>-1.091574</td>\n",
       "      <td>-0.810464</td>\n",
       "      <td>-1.108688</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>-1.122138</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.124845</td>\n",
       "      <td>-0.901217</td>\n",
       "      <td>-1.119513</td>\n",
       "      <td>-0.882800</td>\n",
       "      <td>-1.138277</td>\n",
       "      <td>-0.874736</td>\n",
       "      <td>-1.140696</td>\n",
       "      <td>-0.866315</td>\n",
       "      <td>-1.140602</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>-1.172289</td>\n",
       "      <td>-1.046292</td>\n",
       "      <td>-1.174837</td>\n",
       "      <td>-1.040045</td>\n",
       "      <td>-1.176764</td>\n",
       "      <td>-1.031315</td>\n",
       "      <td>-1.178362</td>\n",
       "      <td>-1.019957</td>\n",
       "      <td>-1.184020</td>\n",
       "      <td>-1.002752</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.965163</td>\n",
       "      <td>-1.172472</td>\n",
       "      <td>-0.960131</td>\n",
       "      <td>-1.172392</td>\n",
       "      <td>-0.967523</td>\n",
       "      <td>-1.176228</td>\n",
       "      <td>-0.966954</td>\n",
       "      <td>-1.178578</td>\n",
       "      <td>-0.968309</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>-0.877777</td>\n",
       "      <td>-0.654365</td>\n",
       "      <td>-0.880118</td>\n",
       "      <td>-0.662182</td>\n",
       "      <td>-0.881993</td>\n",
       "      <td>-0.666418</td>\n",
       "      <td>-0.882876</td>\n",
       "      <td>-0.671606</td>\n",
       "      <td>-0.883658</td>\n",
       "      <td>-0.675248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.677078</td>\n",
       "      <td>-0.887877</td>\n",
       "      <td>-0.675872</td>\n",
       "      <td>-0.880546</td>\n",
       "      <td>-0.695943</td>\n",
       "      <td>-0.880403</td>\n",
       "      <td>-0.697412</td>\n",
       "      <td>-0.878851</td>\n",
       "      <td>-0.696865</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>-0.942056</td>\n",
       "      <td>-0.562857</td>\n",
       "      <td>-0.936255</td>\n",
       "      <td>-0.561758</td>\n",
       "      <td>-0.932292</td>\n",
       "      <td>-0.557619</td>\n",
       "      <td>-0.930761</td>\n",
       "      <td>-0.553298</td>\n",
       "      <td>-0.929061</td>\n",
       "      <td>-0.549408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.571721</td>\n",
       "      <td>-0.904552</td>\n",
       "      <td>-0.554281</td>\n",
       "      <td>-0.914351</td>\n",
       "      <td>-0.521937</td>\n",
       "      <td>-0.921207</td>\n",
       "      <td>-0.522046</td>\n",
       "      <td>-0.926716</td>\n",
       "      <td>-0.522946</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>-0.822849</td>\n",
       "      <td>-1.034206</td>\n",
       "      <td>-0.832168</td>\n",
       "      <td>-1.050258</td>\n",
       "      <td>-0.843392</td>\n",
       "      <td>-1.063118</td>\n",
       "      <td>-0.852510</td>\n",
       "      <td>-1.072538</td>\n",
       "      <td>-0.863866</td>\n",
       "      <td>-1.081805</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.042535</td>\n",
       "      <td>-0.921228</td>\n",
       "      <td>-1.042287</td>\n",
       "      <td>-0.899702</td>\n",
       "      <td>-1.048835</td>\n",
       "      <td>-0.891738</td>\n",
       "      <td>-1.051389</td>\n",
       "      <td>-0.883410</td>\n",
       "      <td>-1.052830</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>-0.963092</td>\n",
       "      <td>-1.020394</td>\n",
       "      <td>-0.970171</td>\n",
       "      <td>-1.043450</td>\n",
       "      <td>-0.977911</td>\n",
       "      <td>-1.061444</td>\n",
       "      <td>-0.985654</td>\n",
       "      <td>-1.072538</td>\n",
       "      <td>-0.997749</td>\n",
       "      <td>-1.080192</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.067228</td>\n",
       "      <td>-1.052409</td>\n",
       "      <td>-1.062004</td>\n",
       "      <td>-1.058583</td>\n",
       "      <td>-1.068350</td>\n",
       "      <td>-1.056085</td>\n",
       "      <td>-1.074122</td>\n",
       "      <td>-1.052077</td>\n",
       "      <td>-1.075586</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>-0.734028</td>\n",
       "      <td>-0.978956</td>\n",
       "      <td>-0.724572</td>\n",
       "      <td>-0.958345</td>\n",
       "      <td>-0.717062</td>\n",
       "      <td>-0.939254</td>\n",
       "      <td>-0.713526</td>\n",
       "      <td>-0.919724</td>\n",
       "      <td>-0.710193</td>\n",
       "      <td>-0.902726</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.897669</td>\n",
       "      <td>-0.685547</td>\n",
       "      <td>-0.905908</td>\n",
       "      <td>-0.684479</td>\n",
       "      <td>-0.915484</td>\n",
       "      <td>-0.685454</td>\n",
       "      <td>-0.913371</td>\n",
       "      <td>-0.686251</td>\n",
       "      <td>-0.913045</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>-0.928031</td>\n",
       "      <td>-0.997948</td>\n",
       "      <td>-0.924560</td>\n",
       "      <td>-1.002599</td>\n",
       "      <td>-0.922934</td>\n",
       "      <td>-1.007882</td>\n",
       "      <td>-0.920250</td>\n",
       "      <td>-1.011741</td>\n",
       "      <td>-0.919748</td>\n",
       "      <td>-1.014046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.971748</td>\n",
       "      <td>-0.949020</td>\n",
       "      <td>-0.986421</td>\n",
       "      <td>-0.945902</td>\n",
       "      <td>-0.990291</td>\n",
       "      <td>-0.941609</td>\n",
       "      <td>-0.988063</td>\n",
       "      <td>-0.938113</td>\n",
       "      <td>-0.987813</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>-0.809993</td>\n",
       "      <td>-0.585303</td>\n",
       "      <td>-0.812286</td>\n",
       "      <td>-0.607715</td>\n",
       "      <td>-0.814149</td>\n",
       "      <td>-0.627920</td>\n",
       "      <td>-0.817472</td>\n",
       "      <td>-0.646959</td>\n",
       "      <td>-0.825448</td>\n",
       "      <td>-0.663954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.649092</td>\n",
       "      <td>-0.892324</td>\n",
       "      <td>-0.649582</td>\n",
       "      <td>-0.876039</td>\n",
       "      <td>-0.668297</td>\n",
       "      <td>-0.869069</td>\n",
       "      <td>-0.669808</td>\n",
       "      <td>-0.860617</td>\n",
       "      <td>-0.667608</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>-0.928031</td>\n",
       "      <td>-0.997948</td>\n",
       "      <td>-0.924560</td>\n",
       "      <td>-1.002599</td>\n",
       "      <td>-0.922934</td>\n",
       "      <td>-1.007882</td>\n",
       "      <td>-0.920250</td>\n",
       "      <td>-1.011741</td>\n",
       "      <td>-0.919748</td>\n",
       "      <td>-1.014046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.971748</td>\n",
       "      <td>-0.949020</td>\n",
       "      <td>-0.986421</td>\n",
       "      <td>-0.945902</td>\n",
       "      <td>-0.990291</td>\n",
       "      <td>-0.941609</td>\n",
       "      <td>-0.988063</td>\n",
       "      <td>-0.938113</td>\n",
       "      <td>-0.987813</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>-1.057756</td>\n",
       "      <td>-1.136073</td>\n",
       "      <td>-1.049699</td>\n",
       "      <td>-1.138767</td>\n",
       "      <td>-1.042246</td>\n",
       "      <td>-1.140115</td>\n",
       "      <td>-1.033539</td>\n",
       "      <td>-1.143194</td>\n",
       "      <td>-1.024525</td>\n",
       "      <td>-1.144725</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.167647</td>\n",
       "      <td>-1.042403</td>\n",
       "      <td>-1.178665</td>\n",
       "      <td>-1.027032</td>\n",
       "      <td>-1.182185</td>\n",
       "      <td>-1.022082</td>\n",
       "      <td>-1.176418</td>\n",
       "      <td>-1.020167</td>\n",
       "      <td>-1.169859</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>-0.873103</td>\n",
       "      <td>-0.808028</td>\n",
       "      <td>-0.874270</td>\n",
       "      <td>-0.810263</td>\n",
       "      <td>-0.873805</td>\n",
       "      <td>-0.810369</td>\n",
       "      <td>-0.875868</td>\n",
       "      <td>-0.809632</td>\n",
       "      <td>-0.877837</td>\n",
       "      <td>-0.805927</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.782435</td>\n",
       "      <td>-0.875648</td>\n",
       "      <td>-0.787604</td>\n",
       "      <td>-0.881673</td>\n",
       "      <td>-0.800022</td>\n",
       "      <td>-0.883804</td>\n",
       "      <td>-0.799708</td>\n",
       "      <td>-0.885689</td>\n",
       "      <td>-0.800891</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>-0.273562</td>\n",
       "      <td>-0.963417</td>\n",
       "      <td>-0.274306</td>\n",
       "      <td>-0.999195</td>\n",
       "      <td>-0.277245</td>\n",
       "      <td>-1.029642</td>\n",
       "      <td>-0.280225</td>\n",
       "      <td>-1.054463</td>\n",
       "      <td>-0.287590</td>\n",
       "      <td>-1.075352</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.065582</td>\n",
       "      <td>-0.437638</td>\n",
       "      <td>-1.070220</td>\n",
       "      <td>-0.421931</td>\n",
       "      <td>-1.082986</td>\n",
       "      <td>-0.407764</td>\n",
       "      <td>-1.083865</td>\n",
       "      <td>-0.391083</td>\n",
       "      <td>-1.082087</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>-0.849729</td>\n",
       "      <td>-1.054925</td>\n",
       "      <td>-0.854388</td>\n",
       "      <td>-1.058768</td>\n",
       "      <td>-0.858599</td>\n",
       "      <td>-1.058097</td>\n",
       "      <td>-0.863021</td>\n",
       "      <td>-1.056106</td>\n",
       "      <td>-0.865031</td>\n",
       "      <td>-1.055992</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.012903</td>\n",
       "      <td>-0.887877</td>\n",
       "      <td>-1.022569</td>\n",
       "      <td>-0.868151</td>\n",
       "      <td>-1.034199</td>\n",
       "      <td>-0.862269</td>\n",
       "      <td>-1.035152</td>\n",
       "      <td>-0.857198</td>\n",
       "      <td>-1.034950</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>-0.039822</td>\n",
       "      <td>-1.051472</td>\n",
       "      <td>-0.036893</td>\n",
       "      <td>-1.074087</td>\n",
       "      <td>-0.035112</td>\n",
       "      <td>-1.094921</td>\n",
       "      <td>-0.037296</td>\n",
       "      <td>-1.113617</td>\n",
       "      <td>-0.045438</td>\n",
       "      <td>-1.125365</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.119907</td>\n",
       "      <td>-0.166383</td>\n",
       "      <td>-1.135944</td>\n",
       "      <td>-0.156001</td>\n",
       "      <td>-1.136651</td>\n",
       "      <td>-0.144808</td>\n",
       "      <td>-1.135825</td>\n",
       "      <td>-0.133523</td>\n",
       "      <td>-1.134100</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>-0.928031</td>\n",
       "      <td>-0.997948</td>\n",
       "      <td>-0.924560</td>\n",
       "      <td>-1.002599</td>\n",
       "      <td>-0.922934</td>\n",
       "      <td>-1.007882</td>\n",
       "      <td>-0.920250</td>\n",
       "      <td>-1.011741</td>\n",
       "      <td>-0.919748</td>\n",
       "      <td>-1.014046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.971748</td>\n",
       "      <td>-0.949020</td>\n",
       "      <td>-0.986421</td>\n",
       "      <td>-0.945902</td>\n",
       "      <td>-0.990291</td>\n",
       "      <td>-0.941609</td>\n",
       "      <td>-0.988063</td>\n",
       "      <td>-0.938113</td>\n",
       "      <td>-0.987813</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>-1.091649</td>\n",
       "      <td>-1.122260</td>\n",
       "      <td>-1.091801</td>\n",
       "      <td>-1.096214</td>\n",
       "      <td>-1.092544</td>\n",
       "      <td>-1.066466</td>\n",
       "      <td>-1.094271</td>\n",
       "      <td>-1.039675</td>\n",
       "      <td>-1.086228</td>\n",
       "      <td>-1.018885</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.080398</td>\n",
       "      <td>-1.019058</td>\n",
       "      <td>-1.043930</td>\n",
       "      <td>-1.004496</td>\n",
       "      <td>-1.029320</td>\n",
       "      <td>-1.006214</td>\n",
       "      <td>-1.030281</td>\n",
       "      <td>-1.011050</td>\n",
       "      <td>-1.030074</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>-1.076456</td>\n",
       "      <td>-0.973777</td>\n",
       "      <td>-1.073089</td>\n",
       "      <td>-0.905580</td>\n",
       "      <td>-1.069150</td>\n",
       "      <td>-0.838824</td>\n",
       "      <td>-1.059233</td>\n",
       "      <td>-0.778412</td>\n",
       "      <td>-1.043152</td>\n",
       "      <td>-0.725261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.667200</td>\n",
       "      <td>-0.870090</td>\n",
       "      <td>-0.667656</td>\n",
       "      <td>-0.890687</td>\n",
       "      <td>-0.702448</td>\n",
       "      <td>-0.900805</td>\n",
       "      <td>-0.703907</td>\n",
       "      <td>-0.909622</td>\n",
       "      <td>-0.706617</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>-1.013346</td>\n",
       "      <td>-0.814934</td>\n",
       "      <td>-1.011104</td>\n",
       "      <td>-0.830688</td>\n",
       "      <td>-1.011833</td>\n",
       "      <td>-0.843845</td>\n",
       "      <td>-1.011348</td>\n",
       "      <td>-0.855640</td>\n",
       "      <td>-1.014047</td>\n",
       "      <td>-0.867233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.884499</td>\n",
       "      <td>-1.045738</td>\n",
       "      <td>-0.900979</td>\n",
       "      <td>-1.046188</td>\n",
       "      <td>-0.895969</td>\n",
       "      <td>-1.043617</td>\n",
       "      <td>-0.890638</td>\n",
       "      <td>-1.041821</td>\n",
       "      <td>-0.885413</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>-0.908163</td>\n",
       "      <td>-0.122587</td>\n",
       "      <td>-0.908186</td>\n",
       "      <td>-0.115811</td>\n",
       "      <td>-0.905388</td>\n",
       "      <td>-0.104008</td>\n",
       "      <td>-0.907402</td>\n",
       "      <td>-0.094856</td>\n",
       "      <td>-0.910434</td>\n",
       "      <td>-0.076704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067982</td>\n",
       "      <td>-0.874536</td>\n",
       "      <td>-0.056416</td>\n",
       "      <td>-0.873785</td>\n",
       "      <td>-0.090986</td>\n",
       "      <td>-0.878137</td>\n",
       "      <td>-0.090128</td>\n",
       "      <td>-0.883410</td>\n",
       "      <td>-0.092212</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>-0.624170</td>\n",
       "      <td>-1.041112</td>\n",
       "      <td>-0.621654</td>\n",
       "      <td>-1.048556</td>\n",
       "      <td>-0.621144</td>\n",
       "      <td>-1.054749</td>\n",
       "      <td>-0.622428</td>\n",
       "      <td>-1.057750</td>\n",
       "      <td>-0.625207</td>\n",
       "      <td>-1.059219</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.077105</td>\n",
       "      <td>-0.645526</td>\n",
       "      <td>-1.083365</td>\n",
       "      <td>-0.643914</td>\n",
       "      <td>-1.084612</td>\n",
       "      <td>-0.641250</td>\n",
       "      <td>-1.083865</td>\n",
       "      <td>-0.639526</td>\n",
       "      <td>-1.083713</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>-1.076456</td>\n",
       "      <td>-0.973777</td>\n",
       "      <td>-1.073089</td>\n",
       "      <td>-0.905580</td>\n",
       "      <td>-1.069150</td>\n",
       "      <td>-0.838824</td>\n",
       "      <td>-1.059233</td>\n",
       "      <td>-0.778412</td>\n",
       "      <td>-1.043152</td>\n",
       "      <td>-0.725261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.667200</td>\n",
       "      <td>-0.870090</td>\n",
       "      <td>-0.667656</td>\n",
       "      <td>-0.890687</td>\n",
       "      <td>-0.702448</td>\n",
       "      <td>-0.900805</td>\n",
       "      <td>-0.703907</td>\n",
       "      <td>-0.909622</td>\n",
       "      <td>-0.706617</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>-0.745715</td>\n",
       "      <td>-1.068737</td>\n",
       "      <td>-0.739775</td>\n",
       "      <td>-1.062173</td>\n",
       "      <td>-0.734608</td>\n",
       "      <td>-1.049728</td>\n",
       "      <td>-0.731045</td>\n",
       "      <td>-1.033102</td>\n",
       "      <td>-0.727656</td>\n",
       "      <td>-1.015659</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.078752</td>\n",
       "      <td>-0.738909</td>\n",
       "      <td>-1.098153</td>\n",
       "      <td>-0.750962</td>\n",
       "      <td>-1.081359</td>\n",
       "      <td>-0.753459</td>\n",
       "      <td>-1.075746</td>\n",
       "      <td>-0.755770</td>\n",
       "      <td>-1.073960</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>-0.825186</td>\n",
       "      <td>-0.890902</td>\n",
       "      <td>-0.821642</td>\n",
       "      <td>-0.878347</td>\n",
       "      <td>-0.815319</td>\n",
       "      <td>-0.863931</td>\n",
       "      <td>-0.809296</td>\n",
       "      <td>-0.850711</td>\n",
       "      <td>-0.806821</td>\n",
       "      <td>-0.836580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.833467</td>\n",
       "      <td>-0.784489</td>\n",
       "      <td>-0.831968</td>\n",
       "      <td>-0.782513</td>\n",
       "      <td>-0.850435</td>\n",
       "      <td>-0.784062</td>\n",
       "      <td>-0.850044</td>\n",
       "      <td>-0.783121</td>\n",
       "      <td>-0.848028</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1185 rows Ã— 137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x1        y1        x2        y2        x3        y3        x4  \\\n",
       "0     1.361444  1.510731  1.347821  1.526705  1.338147  1.543050  1.331516   \n",
       "1     0.336498  1.483107  0.304608  1.521599  0.274866  1.553093  0.247679   \n",
       "2     2.507934  0.970321  2.511495  0.988845  2.513722  1.002400  2.511124   \n",
       "3     3.811030  1.151609  3.804986  1.167564  3.802761  1.186522  3.805188   \n",
       "4    -0.896476  0.954782 -0.889474  0.966718 -0.880824  0.978966 -0.872365   \n",
       "5    -0.057353  1.194772 -0.054436  1.186287 -0.047979  1.173131 -0.033792   \n",
       "6     1.920080  0.013811  1.918548  0.011846  1.917162  0.008139  1.918984   \n",
       "7     3.354070  0.188193  3.351212  0.176948  3.346567  0.168828  3.339185   \n",
       "8    -0.141499  0.162294 -0.139811  0.165034 -0.140387  0.168828 -0.140074   \n",
       "9     1.175621  0.212364  1.179410  0.192267  1.181404  0.168828  1.179685   \n",
       "10    0.414800  1.350162  0.415712  1.347986  0.421082  1.343863  0.422868   \n",
       "11   -0.188247  0.562854 -0.184252  0.571833 -0.181327  0.580592 -0.175111   \n",
       "12    0.684769  0.640549  0.689380  0.650129  0.690119  0.657589  0.687988   \n",
       "13    1.530905  0.726877  1.539623  0.723319  1.546359  0.717847  1.551087   \n",
       "14   -0.780776  0.937516 -0.777200  0.951399 -0.775548  0.962227 -0.764915   \n",
       "15    2.575719  0.859821  2.568802  0.850976  2.560511  0.841711  2.546162   \n",
       "16    0.332992  0.492066  0.332676  0.502047  0.328673  0.506943  0.317754   \n",
       "17    1.064595  1.146429  1.054271  1.131820  1.044546  1.114547  1.043038   \n",
       "18    2.603767  0.545589  2.601548  0.556514  2.596773  0.565528  2.590543   \n",
       "19    0.795795  0.243442  0.806332  0.313115  0.814110  0.379731  0.817628   \n",
       "20    0.169374  0.274520  0.164265  0.301201  0.157893  0.326168  0.150741   \n",
       "21    1.215357  0.659541  1.208648  0.668852  1.202459  0.677675  1.192533   \n",
       "22   -1.203843  2.403359 -1.206415  2.370941 -1.208347  2.333102 -1.207560   \n",
       "23    0.738529  2.356742  0.744348  2.307964  0.747435  2.252758  0.746384   \n",
       "24    1.335732  2.411991  1.336126  2.367537  1.333468  2.321385  1.326844   \n",
       "25    0.075878  2.028697  0.074212  1.986269  0.070164  1.939750  0.063146   \n",
       "26   -0.682605  2.306672 -0.690655  2.260305 -0.698346  2.209238 -0.705351   \n",
       "27    2.180699  2.056322  2.181690  2.018609  2.180350  1.973227  2.173592   \n",
       "28    2.237966  0.923704  2.240166  0.947995  2.241176  0.968923  2.241332   \n",
       "29    0.843712  0.355668  0.846096  0.415241  0.849202  0.473466  0.852666   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1155 -1.212024 -1.251752 -1.213432 -1.268125 -1.216535 -1.279044 -1.220407   \n",
       "1156 -1.108010 -1.186143 -1.111683 -1.206850 -1.117108 -1.223807 -1.124637   \n",
       "1157 -1.152421 -1.039386 -1.151447 -1.031535 -1.151031 -1.022946 -1.149164   \n",
       "1158 -1.175795 -1.053198 -1.179516 -1.051960 -1.181443 -1.048054 -1.184201   \n",
       "1159 -0.613652 -0.904715 -0.627501 -0.832391 -0.638690 -0.755132 -0.638779   \n",
       "1160 -1.022696 -1.051472 -1.022800 -1.072385 -1.024700 -1.091574 -1.031203   \n",
       "1161 -0.799475 -1.048018 -0.800590 -1.070683 -0.804791 -1.091574 -0.810464   \n",
       "1162 -1.172289 -1.046292 -1.174837 -1.040045 -1.176764 -1.031315 -1.178362   \n",
       "1163 -0.877777 -0.654365 -0.880118 -0.662182 -0.881993 -0.666418 -0.882876   \n",
       "1164 -0.942056 -0.562857 -0.936255 -0.561758 -0.932292 -0.557619 -0.930761   \n",
       "1165 -0.822849 -1.034206 -0.832168 -1.050258 -0.843392 -1.063118 -0.852510   \n",
       "1166 -0.963092 -1.020394 -0.970171 -1.043450 -0.977911 -1.061444 -0.985654   \n",
       "1167 -0.734028 -0.978956 -0.724572 -0.958345 -0.717062 -0.939254 -0.713526   \n",
       "1168 -0.928031 -0.997948 -0.924560 -1.002599 -0.922934 -1.007882 -0.920250   \n",
       "1169 -0.809993 -0.585303 -0.812286 -0.607715 -0.814149 -0.627920 -0.817472   \n",
       "1170 -0.928031 -0.997948 -0.924560 -1.002599 -0.922934 -1.007882 -0.920250   \n",
       "1171 -1.057756 -1.136073 -1.049699 -1.138767 -1.042246 -1.140115 -1.033539   \n",
       "1172 -0.873103 -0.808028 -0.874270 -0.810263 -0.873805 -0.810369 -0.875868   \n",
       "1173 -0.273562 -0.963417 -0.274306 -0.999195 -0.277245 -1.029642 -0.280225   \n",
       "1174 -0.849729 -1.054925 -0.854388 -1.058768 -0.858599 -1.058097 -0.863021   \n",
       "1175 -0.039822 -1.051472 -0.036893 -1.074087 -0.035112 -1.094921 -0.037296   \n",
       "1176 -0.928031 -0.997948 -0.924560 -1.002599 -0.922934 -1.007882 -0.920250   \n",
       "1177 -1.091649 -1.122260 -1.091801 -1.096214 -1.092544 -1.066466 -1.094271   \n",
       "1178 -1.076456 -0.973777 -1.073089 -0.905580 -1.069150 -0.838824 -1.059233   \n",
       "1179 -1.013346 -0.814934 -1.011104 -0.830688 -1.011833 -0.843845 -1.011348   \n",
       "1180 -0.908163 -0.122587 -0.908186 -0.115811 -0.905388 -0.104008 -0.907402   \n",
       "1181 -0.624170 -1.041112 -0.621654 -1.048556 -0.621144 -1.054749 -0.622428   \n",
       "1182 -1.076456 -0.973777 -1.073089 -0.905580 -1.069150 -0.838824 -1.059233   \n",
       "1183 -0.745715 -1.068737 -0.739775 -1.062173 -0.734608 -1.049728 -0.731045   \n",
       "1184 -0.825186 -0.890902 -0.821642 -0.878347 -0.815319 -0.863931 -0.809296   \n",
       "\n",
       "            y4        x5        y5  ...       y64       x65       y65  \\\n",
       "0     1.559810  1.321326  1.585019  ...  1.645717  1.414456  1.668856   \n",
       "1     1.587744  0.230476  1.625352  ...  1.874539  0.329436  1.892320   \n",
       "2     1.014280  2.508804  1.018743  ...  1.066253  2.486136  1.049401   \n",
       "3     1.196671  3.802224  1.202661  ...  1.248982  3.705673  1.261363   \n",
       "4     0.987989 -0.853389  0.996156  ...  0.975712 -0.686659  0.977104   \n",
       "5     1.157235 -0.018661  1.141355  ...  1.137040 -0.006298  1.095409   \n",
       "6     0.003734  1.909244 -0.000878  ... -0.035058  1.811334 -0.031770   \n",
       "7     0.164764  3.326068  0.162068  ...  0.164133  3.194290  0.158832   \n",
       "8     0.171337 -0.132753  0.170134  ...  0.201996 -0.095234  0.186765   \n",
       "9     0.145046  1.175802  0.120121  ...  0.172364  1.055377  0.155546   \n",
       "10    1.337983  0.424896  1.330114  ...  1.334584  0.459505  1.323802   \n",
       "11    0.585414 -0.163022  0.583145  ...  0.585561 -0.096346  0.581113   \n",
       "12    0.664286  0.689169  0.671878  ...  0.682687  0.712973  0.692845   \n",
       "13    0.711938  1.554165  0.704144  ...  0.662933  1.503392  0.655053   \n",
       "14    0.971558 -0.742790  0.970343  ...  0.942788 -0.608840  0.952457   \n",
       "15    0.831889  2.525103  0.829984  ...  0.883524  2.416099  0.901521   \n",
       "16    0.514758  0.310805  0.521839  ...  0.524652  0.359452  0.540035   \n",
       "17    1.098081  1.036098  1.086502  ...  1.137040  1.004239  1.139773   \n",
       "18    0.567339  2.590298  0.567012  ...  0.534529  2.529493  0.553180   \n",
       "19    0.445745  0.828872  0.504092  ...  0.409417  1.026473  0.431589   \n",
       "20    0.353728  0.148982  0.378253  ...  0.411064  0.242723  0.438162   \n",
       "21    0.684004  1.179294  0.691238  ...  0.720550  1.156542  0.745425   \n",
       "22    2.292661 -1.204975  2.254549  ...  2.322307 -1.158020  2.321174   \n",
       "23    2.195715  0.741557  2.138389  ...  2.152748  0.664058  2.148647   \n",
       "24    2.269657  1.315505  2.222282  ...  2.261397  1.239919  2.253806   \n",
       "25    1.893373  0.054683  1.846378  ...  1.889355  0.031500  1.887391   \n",
       "26    2.157922 -0.711357  2.109349  ...  2.223535 -0.720010  2.222587   \n",
       "27    1.926236  2.160710  1.878644  ...  1.904171  2.029227  1.905465   \n",
       "28    0.986346  2.244532  0.991316  ...  1.018513  2.271578  0.996822   \n",
       "29    0.524617  0.864962  0.571852  ...  0.575684  1.014244  0.602473   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1155 -1.287793 -1.224767 -1.293151  ... -1.281235 -1.256962 -1.277252   \n",
       "1156 -1.238498 -1.135124 -1.251204  ... -1.236787 -1.213605 -1.236175   \n",
       "1157 -1.013384 -1.147930 -1.004366  ... -1.014550 -1.132451 -0.999566   \n",
       "1158 -1.041318 -1.189841 -1.026952  ... -1.001380 -1.185813 -0.989707   \n",
       "1159 -0.681465 -0.625207 -0.618781  ... -0.514103 -0.440973 -0.509917   \n",
       "1160 -1.107045 -1.041988 -1.118912  ... -1.119907 -1.105770 -1.116227   \n",
       "1161 -1.108688 -0.821955 -1.122138  ... -1.124845 -0.901217 -1.119513   \n",
       "1162 -1.019957 -1.184020 -1.002752  ... -0.965163 -1.172472 -0.960131   \n",
       "1163 -0.671606 -0.883658 -0.675248  ... -0.677078 -0.887877 -0.675872   \n",
       "1164 -0.553298 -0.929061 -0.549408  ... -0.571721 -0.904552 -0.554281   \n",
       "1165 -1.072538 -0.863866 -1.081805  ... -1.042535 -0.921228 -1.042287   \n",
       "1166 -1.072538 -0.997749 -1.080192  ... -1.067228 -1.052409 -1.062004   \n",
       "1167 -0.919724 -0.710193 -0.902726  ... -0.897669 -0.685547 -0.905908   \n",
       "1168 -1.011741 -0.919748 -1.014046  ... -0.971748 -0.949020 -0.986421   \n",
       "1169 -0.646959 -0.825448 -0.663954  ... -0.649092 -0.892324 -0.649582   \n",
       "1170 -1.011741 -0.919748 -1.014046  ... -0.971748 -0.949020 -0.986421   \n",
       "1171 -1.143194 -1.024525 -1.144725  ... -1.167647 -1.042403 -1.178665   \n",
       "1172 -0.809632 -0.877837 -0.805927  ... -0.782435 -0.875648 -0.787604   \n",
       "1173 -1.054463 -0.287590 -1.075352  ... -1.065582 -0.437638 -1.070220   \n",
       "1174 -1.056106 -0.865031 -1.055992  ... -1.012903 -0.887877 -1.022569   \n",
       "1175 -1.113617 -0.045438 -1.125365  ... -1.119907 -0.166383 -1.135944   \n",
       "1176 -1.011741 -0.919748 -1.014046  ... -0.971748 -0.949020 -0.986421   \n",
       "1177 -1.039675 -1.086228 -1.018885  ... -1.080398 -1.019058 -1.043930   \n",
       "1178 -0.778412 -1.043152 -0.725261  ... -0.667200 -0.870090 -0.667656   \n",
       "1179 -0.855640 -1.014047 -0.867233  ... -0.884499 -1.045738 -0.900979   \n",
       "1180 -0.094856 -0.910434 -0.076704  ... -0.067982 -0.874536 -0.056416   \n",
       "1181 -1.057750 -0.625207 -1.059219  ... -1.077105 -0.645526 -1.083365   \n",
       "1182 -0.778412 -1.043152 -0.725261  ... -0.667200 -0.870090 -0.667656   \n",
       "1183 -1.033102 -0.727656 -1.015659  ... -1.078752 -0.738909 -1.098153   \n",
       "1184 -0.850711 -0.806821 -0.836580  ... -0.833467 -0.784489 -0.831968   \n",
       "\n",
       "           x66       y66       x67       y67       x68       y68    target  \n",
       "0     1.349429  1.694612  1.330916  1.686256  1.316106  1.672984 -0.688390  \n",
       "1     0.291346  1.836094  0.280228  1.817780  0.274470  1.798140 -0.688390  \n",
       "2     2.503292  1.024606  2.507415  1.027013  2.512734  1.026070 -0.688390  \n",
       "3     3.715750  1.281550  3.717917  1.280318  3.723037  1.271507 -0.688390  \n",
       "4    -0.704762  0.988829 -0.717190  0.991290 -0.735256  0.993562 -0.688390  \n",
       "5    -0.039939  1.136816 -0.048467  1.147170 -0.058307  1.154478 -0.688390  \n",
       "6     1.827201 -0.042200  1.838692 -0.039792  1.850600 -0.033697 -0.688390  \n",
       "7     3.218822  0.183846  3.233943  0.182662  3.250084  0.182482 -0.688390  \n",
       "8    -0.089519  0.188725 -0.089270  0.189157 -0.091356  0.188984 -0.688390  \n",
       "9     1.097022  0.136686  1.115564  0.135573  1.131483  0.138596 -0.688390  \n",
       "10    0.446847  1.348225  0.440041  1.351763  0.432881  1.349527 -0.688390  \n",
       "11   -0.081631  0.559505 -0.080203  0.557748 -0.082239  0.561203 -0.688390  \n",
       "12    0.734186  0.687977  0.738133  0.687649  0.738306  0.684734 -0.688390  \n",
       "13    1.522959  0.626180  1.534933  0.627570  1.544035  0.634346 -0.688390  \n",
       "14   -0.606729  0.941668 -0.611781  0.936083 -0.617873  0.935047 -0.688390  \n",
       "15    2.444698  0.879872  2.457544  0.876004  2.469427  0.873282 -0.688390  \n",
       "16    0.356702  0.526980  0.355034  0.522026  0.354245  0.518942 -0.688390  \n",
       "17    0.982086  1.154704  0.979553  1.150418  0.982190  1.144725 -0.688390  \n",
       "18    2.567521  0.559505  2.581088  0.556125  2.591369  0.551450 -0.688390  \n",
       "19    1.002369  0.463557  0.985220  0.470066  0.969654  0.466929 -0.688390  \n",
       "20    0.237259  0.455426  0.230357  0.450581  0.224326  0.442548 -0.688390  \n",
       "21    1.171392  0.705865  1.175636  0.699015  1.180488  0.694486 -0.688390  \n",
       "22   -1.159997  2.271923 -1.160360  2.267559 -1.159204  2.267884 -0.688390  \n",
       "23    0.691367  2.123936  0.699596  2.121421  0.707535  2.126473 -0.688390  \n",
       "24    1.257030  2.245903  1.262910  2.243203  1.269380  2.245128 -0.688390  \n",
       "25    0.034431  1.863740  0.035407  1.858373  0.038563  1.858280 -0.688390  \n",
       "26   -0.711523  2.177602 -0.708122  2.171758 -0.707905  2.171985 -0.688390  \n",
       "27    2.068340  1.863740  2.085779  1.859997  2.103601  1.863157 -0.688390  \n",
       "28    2.248631  1.034363  2.239926  1.033508  2.231241  1.024445 -0.688390  \n",
       "29    0.996735  0.595282  0.982953  0.591847  0.966235  0.582333 -0.688390  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1155 -1.249016 -1.287890 -1.245367 -1.288457 -1.241259 -1.288514  1.452665  \n",
       "1156 -1.199436 -1.252113 -1.194363 -1.254358 -1.188835 -1.252755  1.452665  \n",
       "1157 -1.136334 -0.980533 -1.143359 -0.979944 -1.148947 -0.982937  1.452665  \n",
       "1158 -1.190421 -1.000048 -1.194363 -1.002677 -1.195673 -1.004068  1.452665  \n",
       "1159 -0.444467 -0.526816 -0.447434 -0.530165 -0.450345 -0.535949  1.452665  \n",
       "1160 -1.092388 -1.133399 -1.086687 -1.134201 -1.080569 -1.134100  1.452665  \n",
       "1161 -0.882800 -1.138277 -0.874736 -1.140696 -0.866315 -1.140602  1.452665  \n",
       "1162 -1.172392 -0.967523 -1.176228 -0.966954 -1.178578 -0.968309  1.452665  \n",
       "1163 -0.880546 -0.695943 -0.880403 -0.697412 -0.878851 -0.696865  1.452665  \n",
       "1164 -0.914351 -0.521937 -0.921207 -0.522046 -0.926716 -0.522946  1.452665  \n",
       "1165 -0.899702 -1.048835 -0.891738 -1.051389 -0.883410 -1.052830  1.452665  \n",
       "1166 -1.058583 -1.068350 -1.056085 -1.074122 -1.052077 -1.075586  1.452665  \n",
       "1167 -0.684479 -0.915484 -0.685454 -0.913371 -0.686251 -0.913045  1.452665  \n",
       "1168 -0.945902 -0.990291 -0.941609 -0.988063 -0.938113 -0.987813  1.452665  \n",
       "1169 -0.876039 -0.668297 -0.869069 -0.669808 -0.860617 -0.667608  1.452665  \n",
       "1170 -0.945902 -0.990291 -0.941609 -0.988063 -0.938113 -0.987813  1.452665  \n",
       "1171 -1.027032 -1.182185 -1.022082 -1.176418 -1.020167 -1.169859  1.452665  \n",
       "1172 -0.881673 -0.800022 -0.883804 -0.799708 -0.885689 -0.800891  1.452665  \n",
       "1173 -0.421931 -1.082986 -0.407764 -1.083865 -0.391083 -1.082087  1.452665  \n",
       "1174 -0.868151 -1.034199 -0.862269 -1.035152 -0.857198 -1.034950  1.452665  \n",
       "1175 -0.156001 -1.136651 -0.144808 -1.135825 -0.133523 -1.134100  1.452665  \n",
       "1176 -0.945902 -0.990291 -0.941609 -0.988063 -0.938113 -0.987813  1.452665  \n",
       "1177 -1.004496 -1.029320 -1.006214 -1.030281 -1.011050 -1.030074  1.452665  \n",
       "1178 -0.890687 -0.702448 -0.900805 -0.703907 -0.909622 -0.706617  1.452665  \n",
       "1179 -1.046188 -0.895969 -1.043617 -0.890638 -1.041821 -0.885413  1.452665  \n",
       "1180 -0.873785 -0.090986 -0.878137 -0.090128 -0.883410 -0.092212  1.452665  \n",
       "1181 -0.643914 -1.084612 -0.641250 -1.083865 -0.639526 -1.083713  1.452665  \n",
       "1182 -0.890687 -0.702448 -0.900805 -0.703907 -0.909622 -0.706617  1.452665  \n",
       "1183 -0.750962 -1.081359 -0.753459 -1.075746 -0.755770 -1.073960  1.452665  \n",
       "1184 -0.782513 -0.850435 -0.784062 -0.850044 -0.783121 -0.848028  1.452665  \n",
       "\n",
       "[1185 rows x 137 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>y5</th>\n",
       "      <th>...</th>\n",
       "      <th>y62</th>\n",
       "      <th>x63</th>\n",
       "      <th>y63</th>\n",
       "      <th>x64</th>\n",
       "      <th>y64</th>\n",
       "      <th>x65</th>\n",
       "      <th>y65</th>\n",
       "      <th>x67</th>\n",
       "      <th>y67</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.361444</td>\n",
       "      <td>1.510731</td>\n",
       "      <td>1.347821</td>\n",
       "      <td>1.526705</td>\n",
       "      <td>1.338147</td>\n",
       "      <td>1.543050</td>\n",
       "      <td>1.331516</td>\n",
       "      <td>1.559810</td>\n",
       "      <td>1.321326</td>\n",
       "      <td>1.585019</td>\n",
       "      <td>...</td>\n",
       "      <td>1.628604</td>\n",
       "      <td>1.334749</td>\n",
       "      <td>1.638874</td>\n",
       "      <td>1.351820</td>\n",
       "      <td>1.645717</td>\n",
       "      <td>1.414456</td>\n",
       "      <td>1.668856</td>\n",
       "      <td>1.330916</td>\n",
       "      <td>1.686256</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.336498</td>\n",
       "      <td>1.483107</td>\n",
       "      <td>0.304608</td>\n",
       "      <td>1.521599</td>\n",
       "      <td>0.274866</td>\n",
       "      <td>1.553093</td>\n",
       "      <td>0.247679</td>\n",
       "      <td>1.587744</td>\n",
       "      <td>0.230476</td>\n",
       "      <td>1.625352</td>\n",
       "      <td>...</td>\n",
       "      <td>1.840849</td>\n",
       "      <td>0.289702</td>\n",
       "      <td>1.859211</td>\n",
       "      <td>0.300219</td>\n",
       "      <td>1.874539</td>\n",
       "      <td>0.329436</td>\n",
       "      <td>1.892320</td>\n",
       "      <td>0.280228</td>\n",
       "      <td>1.817780</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.507934</td>\n",
       "      <td>0.970321</td>\n",
       "      <td>2.511495</td>\n",
       "      <td>0.988845</td>\n",
       "      <td>2.513722</td>\n",
       "      <td>1.002400</td>\n",
       "      <td>2.511124</td>\n",
       "      <td>1.014280</td>\n",
       "      <td>2.508804</td>\n",
       "      <td>1.018743</td>\n",
       "      <td>...</td>\n",
       "      <td>1.064263</td>\n",
       "      <td>2.507876</td>\n",
       "      <td>1.066654</td>\n",
       "      <td>2.504862</td>\n",
       "      <td>1.066253</td>\n",
       "      <td>2.486136</td>\n",
       "      <td>1.049401</td>\n",
       "      <td>2.507415</td>\n",
       "      <td>1.027013</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.811030</td>\n",
       "      <td>1.151609</td>\n",
       "      <td>3.804986</td>\n",
       "      <td>1.167564</td>\n",
       "      <td>3.802761</td>\n",
       "      <td>1.186522</td>\n",
       "      <td>3.805188</td>\n",
       "      <td>1.196671</td>\n",
       "      <td>3.802224</td>\n",
       "      <td>1.202661</td>\n",
       "      <td>...</td>\n",
       "      <td>1.241956</td>\n",
       "      <td>3.716140</td>\n",
       "      <td>1.247528</td>\n",
       "      <td>3.715387</td>\n",
       "      <td>1.248982</td>\n",
       "      <td>3.705673</td>\n",
       "      <td>1.261363</td>\n",
       "      <td>3.717917</td>\n",
       "      <td>1.280318</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.896476</td>\n",
       "      <td>0.954782</td>\n",
       "      <td>-0.889474</td>\n",
       "      <td>0.966718</td>\n",
       "      <td>-0.880824</td>\n",
       "      <td>0.978966</td>\n",
       "      <td>-0.872365</td>\n",
       "      <td>0.987989</td>\n",
       "      <td>-0.853389</td>\n",
       "      <td>0.996156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977061</td>\n",
       "      <td>-0.717941</td>\n",
       "      <td>0.976217</td>\n",
       "      <td>-0.706298</td>\n",
       "      <td>0.975712</td>\n",
       "      <td>-0.686659</td>\n",
       "      <td>0.977104</td>\n",
       "      <td>-0.717190</td>\n",
       "      <td>0.991290</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.057353</td>\n",
       "      <td>1.194772</td>\n",
       "      <td>-0.054436</td>\n",
       "      <td>1.186287</td>\n",
       "      <td>-0.047979</td>\n",
       "      <td>1.173131</td>\n",
       "      <td>-0.033792</td>\n",
       "      <td>1.157235</td>\n",
       "      <td>-0.018661</td>\n",
       "      <td>1.141355</td>\n",
       "      <td>...</td>\n",
       "      <td>1.158046</td>\n",
       "      <td>-0.059403</td>\n",
       "      <td>1.148869</td>\n",
       "      <td>-0.050315</td>\n",
       "      <td>1.137040</td>\n",
       "      <td>-0.006298</td>\n",
       "      <td>1.095409</td>\n",
       "      <td>-0.048467</td>\n",
       "      <td>1.147170</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.920080</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>1.918548</td>\n",
       "      <td>0.011846</td>\n",
       "      <td>1.917162</td>\n",
       "      <td>0.008139</td>\n",
       "      <td>1.918984</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>1.909244</td>\n",
       "      <td>-0.000878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026578</td>\n",
       "      <td>1.836870</td>\n",
       "      <td>-0.031746</td>\n",
       "      <td>1.826337</td>\n",
       "      <td>-0.035058</td>\n",
       "      <td>1.811334</td>\n",
       "      <td>-0.031770</td>\n",
       "      <td>1.838692</td>\n",
       "      <td>-0.039792</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.354070</td>\n",
       "      <td>0.188193</td>\n",
       "      <td>3.351212</td>\n",
       "      <td>0.176948</td>\n",
       "      <td>3.346567</td>\n",
       "      <td>0.168828</td>\n",
       "      <td>3.339185</td>\n",
       "      <td>0.164764</td>\n",
       "      <td>3.326068</td>\n",
       "      <td>0.162068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164278</td>\n",
       "      <td>3.232154</td>\n",
       "      <td>0.165572</td>\n",
       "      <td>3.217200</td>\n",
       "      <td>0.164133</td>\n",
       "      <td>3.194290</td>\n",
       "      <td>0.158832</td>\n",
       "      <td>3.233943</td>\n",
       "      <td>0.182662</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.141499</td>\n",
       "      <td>0.162294</td>\n",
       "      <td>-0.139811</td>\n",
       "      <td>0.165034</td>\n",
       "      <td>-0.140387</td>\n",
       "      <td>0.168828</td>\n",
       "      <td>-0.140074</td>\n",
       "      <td>0.171337</td>\n",
       "      <td>-0.132753</td>\n",
       "      <td>0.170134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198830</td>\n",
       "      <td>-0.085472</td>\n",
       "      <td>0.201746</td>\n",
       "      <td>-0.085256</td>\n",
       "      <td>0.201996</td>\n",
       "      <td>-0.095234</td>\n",
       "      <td>0.186765</td>\n",
       "      <td>-0.089270</td>\n",
       "      <td>0.189157</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.175621</td>\n",
       "      <td>0.212364</td>\n",
       "      <td>1.179410</td>\n",
       "      <td>0.192267</td>\n",
       "      <td>1.181404</td>\n",
       "      <td>0.168828</td>\n",
       "      <td>1.179685</td>\n",
       "      <td>0.145046</td>\n",
       "      <td>1.175802</td>\n",
       "      <td>0.120121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174150</td>\n",
       "      <td>1.114858</td>\n",
       "      <td>0.172149</td>\n",
       "      <td>1.097091</td>\n",
       "      <td>0.172364</td>\n",
       "      <td>1.055377</td>\n",
       "      <td>0.155546</td>\n",
       "      <td>1.115564</td>\n",
       "      <td>0.135573</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.414800</td>\n",
       "      <td>1.350162</td>\n",
       "      <td>0.415712</td>\n",
       "      <td>1.347986</td>\n",
       "      <td>0.421082</td>\n",
       "      <td>1.343863</td>\n",
       "      <td>0.422868</td>\n",
       "      <td>1.337983</td>\n",
       "      <td>0.424896</td>\n",
       "      <td>1.330114</td>\n",
       "      <td>...</td>\n",
       "      <td>1.335739</td>\n",
       "      <td>0.438185</td>\n",
       "      <td>1.336320</td>\n",
       "      <td>0.445617</td>\n",
       "      <td>1.334584</td>\n",
       "      <td>0.459505</td>\n",
       "      <td>1.323802</td>\n",
       "      <td>0.440041</td>\n",
       "      <td>1.351763</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.188247</td>\n",
       "      <td>0.562854</td>\n",
       "      <td>-0.184252</td>\n",
       "      <td>0.571833</td>\n",
       "      <td>-0.181327</td>\n",
       "      <td>0.580592</td>\n",
       "      <td>-0.175111</td>\n",
       "      <td>0.585414</td>\n",
       "      <td>-0.163022</td>\n",
       "      <td>0.583145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587123</td>\n",
       "      <td>-0.082072</td>\n",
       "      <td>0.584871</td>\n",
       "      <td>-0.084129</td>\n",
       "      <td>0.585561</td>\n",
       "      <td>-0.096346</td>\n",
       "      <td>0.581113</td>\n",
       "      <td>-0.080203</td>\n",
       "      <td>0.557748</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.684769</td>\n",
       "      <td>0.640549</td>\n",
       "      <td>0.689380</td>\n",
       "      <td>0.650129</td>\n",
       "      <td>0.690119</td>\n",
       "      <td>0.657589</td>\n",
       "      <td>0.687988</td>\n",
       "      <td>0.664286</td>\n",
       "      <td>0.689169</td>\n",
       "      <td>0.671878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677615</td>\n",
       "      <td>0.740817</td>\n",
       "      <td>0.680241</td>\n",
       "      <td>0.736413</td>\n",
       "      <td>0.682687</td>\n",
       "      <td>0.712973</td>\n",
       "      <td>0.692845</td>\n",
       "      <td>0.738133</td>\n",
       "      <td>0.687649</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.530905</td>\n",
       "      <td>0.726877</td>\n",
       "      <td>1.539623</td>\n",
       "      <td>0.723319</td>\n",
       "      <td>1.546359</td>\n",
       "      <td>0.717847</td>\n",
       "      <td>1.551087</td>\n",
       "      <td>0.711938</td>\n",
       "      <td>1.554165</td>\n",
       "      <td>0.704144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672679</td>\n",
       "      <td>1.531970</td>\n",
       "      <td>0.667086</td>\n",
       "      <td>1.522015</td>\n",
       "      <td>0.662933</td>\n",
       "      <td>1.503392</td>\n",
       "      <td>0.655053</td>\n",
       "      <td>1.534933</td>\n",
       "      <td>0.627570</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.780776</td>\n",
       "      <td>0.937516</td>\n",
       "      <td>-0.777200</td>\n",
       "      <td>0.951399</td>\n",
       "      <td>-0.775548</td>\n",
       "      <td>0.962227</td>\n",
       "      <td>-0.764915</td>\n",
       "      <td>0.971558</td>\n",
       "      <td>-0.742790</td>\n",
       "      <td>0.970343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937574</td>\n",
       "      <td>-0.611396</td>\n",
       "      <td>0.938397</td>\n",
       "      <td>-0.607111</td>\n",
       "      <td>0.942788</td>\n",
       "      <td>-0.608840</td>\n",
       "      <td>0.952457</td>\n",
       "      <td>-0.611781</td>\n",
       "      <td>0.936083</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.575719</td>\n",
       "      <td>0.859821</td>\n",
       "      <td>2.568802</td>\n",
       "      <td>0.850976</td>\n",
       "      <td>2.560511</td>\n",
       "      <td>0.841711</td>\n",
       "      <td>2.546162</td>\n",
       "      <td>0.831889</td>\n",
       "      <td>2.525103</td>\n",
       "      <td>0.829984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.876698</td>\n",
       "      <td>2.461404</td>\n",
       "      <td>0.879202</td>\n",
       "      <td>2.448506</td>\n",
       "      <td>0.883524</td>\n",
       "      <td>2.416099</td>\n",
       "      <td>0.901521</td>\n",
       "      <td>2.457544</td>\n",
       "      <td>0.876004</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.332992</td>\n",
       "      <td>0.492066</td>\n",
       "      <td>0.332676</td>\n",
       "      <td>0.502047</td>\n",
       "      <td>0.328673</td>\n",
       "      <td>0.506943</td>\n",
       "      <td>0.317754</td>\n",
       "      <td>0.514758</td>\n",
       "      <td>0.310805</td>\n",
       "      <td>0.521839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518020</td>\n",
       "      <td>0.357709</td>\n",
       "      <td>0.519098</td>\n",
       "      <td>0.359956</td>\n",
       "      <td>0.524652</td>\n",
       "      <td>0.359452</td>\n",
       "      <td>0.540035</td>\n",
       "      <td>0.355034</td>\n",
       "      <td>0.522026</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.064595</td>\n",
       "      <td>1.146429</td>\n",
       "      <td>1.054271</td>\n",
       "      <td>1.131820</td>\n",
       "      <td>1.044546</td>\n",
       "      <td>1.114547</td>\n",
       "      <td>1.043038</td>\n",
       "      <td>1.098081</td>\n",
       "      <td>1.036098</td>\n",
       "      <td>1.086502</td>\n",
       "      <td>...</td>\n",
       "      <td>1.130075</td>\n",
       "      <td>0.982244</td>\n",
       "      <td>1.134070</td>\n",
       "      <td>0.983252</td>\n",
       "      <td>1.137040</td>\n",
       "      <td>1.004239</td>\n",
       "      <td>1.139773</td>\n",
       "      <td>0.979553</td>\n",
       "      <td>1.150418</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.603767</td>\n",
       "      <td>0.545589</td>\n",
       "      <td>2.601548</td>\n",
       "      <td>0.556514</td>\n",
       "      <td>2.596773</td>\n",
       "      <td>0.565528</td>\n",
       "      <td>2.590543</td>\n",
       "      <td>0.567339</td>\n",
       "      <td>2.590298</td>\n",
       "      <td>0.567012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522956</td>\n",
       "      <td>2.587218</td>\n",
       "      <td>0.530608</td>\n",
       "      <td>2.573616</td>\n",
       "      <td>0.534529</td>\n",
       "      <td>2.529493</td>\n",
       "      <td>0.553180</td>\n",
       "      <td>2.581088</td>\n",
       "      <td>0.556125</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.795795</td>\n",
       "      <td>0.243442</td>\n",
       "      <td>0.806332</td>\n",
       "      <td>0.313115</td>\n",
       "      <td>0.814110</td>\n",
       "      <td>0.379731</td>\n",
       "      <td>0.817628</td>\n",
       "      <td>0.445745</td>\n",
       "      <td>0.828872</td>\n",
       "      <td>0.504092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409429</td>\n",
       "      <td>0.987911</td>\n",
       "      <td>0.410574</td>\n",
       "      <td>1.004667</td>\n",
       "      <td>0.409417</td>\n",
       "      <td>1.026473</td>\n",
       "      <td>0.431589</td>\n",
       "      <td>0.985220</td>\n",
       "      <td>0.470066</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.169374</td>\n",
       "      <td>0.274520</td>\n",
       "      <td>0.164265</td>\n",
       "      <td>0.301201</td>\n",
       "      <td>0.157893</td>\n",
       "      <td>0.326168</td>\n",
       "      <td>0.150741</td>\n",
       "      <td>0.353728</td>\n",
       "      <td>0.148982</td>\n",
       "      <td>0.378253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394622</td>\n",
       "      <td>0.237563</td>\n",
       "      <td>0.402352</td>\n",
       "      <td>0.243863</td>\n",
       "      <td>0.411064</td>\n",
       "      <td>0.242723</td>\n",
       "      <td>0.438162</td>\n",
       "      <td>0.230357</td>\n",
       "      <td>0.450581</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.215357</td>\n",
       "      <td>0.659541</td>\n",
       "      <td>1.208648</td>\n",
       "      <td>0.668852</td>\n",
       "      <td>1.202459</td>\n",
       "      <td>0.677675</td>\n",
       "      <td>1.192533</td>\n",
       "      <td>0.684004</td>\n",
       "      <td>1.179294</td>\n",
       "      <td>0.691238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710521</td>\n",
       "      <td>1.179465</td>\n",
       "      <td>0.714771</td>\n",
       "      <td>1.174862</td>\n",
       "      <td>0.720550</td>\n",
       "      <td>1.156542</td>\n",
       "      <td>0.745425</td>\n",
       "      <td>1.175636</td>\n",
       "      <td>0.699015</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.203843</td>\n",
       "      <td>2.403359</td>\n",
       "      <td>-1.206415</td>\n",
       "      <td>2.370941</td>\n",
       "      <td>-1.208347</td>\n",
       "      <td>2.333102</td>\n",
       "      <td>-1.207560</td>\n",
       "      <td>2.292661</td>\n",
       "      <td>-1.204975</td>\n",
       "      <td>2.254549</td>\n",
       "      <td>...</td>\n",
       "      <td>2.317989</td>\n",
       "      <td>-1.161122</td>\n",
       "      <td>2.317974</td>\n",
       "      <td>-1.160526</td>\n",
       "      <td>2.322307</td>\n",
       "      <td>-1.158020</td>\n",
       "      <td>2.321174</td>\n",
       "      <td>-1.160360</td>\n",
       "      <td>2.267559</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.738529</td>\n",
       "      <td>2.356742</td>\n",
       "      <td>0.744348</td>\n",
       "      <td>2.307964</td>\n",
       "      <td>0.747435</td>\n",
       "      <td>2.252758</td>\n",
       "      <td>0.746384</td>\n",
       "      <td>2.195715</td>\n",
       "      <td>0.741557</td>\n",
       "      <td>2.138389</td>\n",
       "      <td>...</td>\n",
       "      <td>2.155103</td>\n",
       "      <td>0.697746</td>\n",
       "      <td>2.151898</td>\n",
       "      <td>0.689074</td>\n",
       "      <td>2.152748</td>\n",
       "      <td>0.664058</td>\n",
       "      <td>2.148647</td>\n",
       "      <td>0.699596</td>\n",
       "      <td>2.121421</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.335732</td>\n",
       "      <td>2.411991</td>\n",
       "      <td>1.336126</td>\n",
       "      <td>2.367537</td>\n",
       "      <td>1.333468</td>\n",
       "      <td>2.321385</td>\n",
       "      <td>1.326844</td>\n",
       "      <td>2.269657</td>\n",
       "      <td>1.315505</td>\n",
       "      <td>2.222282</td>\n",
       "      <td>...</td>\n",
       "      <td>2.258758</td>\n",
       "      <td>1.264474</td>\n",
       "      <td>2.260423</td>\n",
       "      <td>1.258269</td>\n",
       "      <td>2.261397</td>\n",
       "      <td>1.239919</td>\n",
       "      <td>2.253806</td>\n",
       "      <td>1.262910</td>\n",
       "      <td>2.243203</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.075878</td>\n",
       "      <td>2.028697</td>\n",
       "      <td>0.074212</td>\n",
       "      <td>1.986269</td>\n",
       "      <td>0.070164</td>\n",
       "      <td>1.939750</td>\n",
       "      <td>0.063146</td>\n",
       "      <td>1.893373</td>\n",
       "      <td>0.054683</td>\n",
       "      <td>1.846378</td>\n",
       "      <td>...</td>\n",
       "      <td>1.885273</td>\n",
       "      <td>0.038075</td>\n",
       "      <td>1.885520</td>\n",
       "      <td>0.036473</td>\n",
       "      <td>1.889355</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>1.887391</td>\n",
       "      <td>0.035407</td>\n",
       "      <td>1.858373</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.682605</td>\n",
       "      <td>2.306672</td>\n",
       "      <td>-0.690655</td>\n",
       "      <td>2.260305</td>\n",
       "      <td>-0.698346</td>\n",
       "      <td>2.209238</td>\n",
       "      <td>-0.705351</td>\n",
       "      <td>2.157922</td>\n",
       "      <td>-0.711357</td>\n",
       "      <td>2.109349</td>\n",
       "      <td>...</td>\n",
       "      <td>2.215980</td>\n",
       "      <td>-0.707740</td>\n",
       "      <td>2.216026</td>\n",
       "      <td>-0.709679</td>\n",
       "      <td>2.223535</td>\n",
       "      <td>-0.720010</td>\n",
       "      <td>2.222587</td>\n",
       "      <td>-0.708122</td>\n",
       "      <td>2.171758</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.180699</td>\n",
       "      <td>2.056322</td>\n",
       "      <td>2.181690</td>\n",
       "      <td>2.018609</td>\n",
       "      <td>2.180350</td>\n",
       "      <td>1.973227</td>\n",
       "      <td>2.173592</td>\n",
       "      <td>1.926236</td>\n",
       "      <td>2.160710</td>\n",
       "      <td>1.878644</td>\n",
       "      <td>...</td>\n",
       "      <td>1.903371</td>\n",
       "      <td>2.086230</td>\n",
       "      <td>1.900319</td>\n",
       "      <td>2.068667</td>\n",
       "      <td>1.904171</td>\n",
       "      <td>2.029227</td>\n",
       "      <td>1.905465</td>\n",
       "      <td>2.085779</td>\n",
       "      <td>1.859997</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.237966</td>\n",
       "      <td>0.923704</td>\n",
       "      <td>2.240166</td>\n",
       "      <td>0.947995</td>\n",
       "      <td>2.241176</td>\n",
       "      <td>0.968923</td>\n",
       "      <td>2.241332</td>\n",
       "      <td>0.986346</td>\n",
       "      <td>2.244532</td>\n",
       "      <td>0.991316</td>\n",
       "      <td>...</td>\n",
       "      <td>1.013258</td>\n",
       "      <td>2.238113</td>\n",
       "      <td>1.020613</td>\n",
       "      <td>2.247879</td>\n",
       "      <td>1.018513</td>\n",
       "      <td>2.271578</td>\n",
       "      <td>0.996822</td>\n",
       "      <td>2.239926</td>\n",
       "      <td>1.033508</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.843712</td>\n",
       "      <td>0.355668</td>\n",
       "      <td>0.846096</td>\n",
       "      <td>0.415241</td>\n",
       "      <td>0.849202</td>\n",
       "      <td>0.473466</td>\n",
       "      <td>0.852666</td>\n",
       "      <td>0.524617</td>\n",
       "      <td>0.864962</td>\n",
       "      <td>0.571852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560798</td>\n",
       "      <td>0.986777</td>\n",
       "      <td>0.568428</td>\n",
       "      <td>1.000159</td>\n",
       "      <td>0.575684</td>\n",
       "      <td>1.014244</td>\n",
       "      <td>0.602473</td>\n",
       "      <td>0.982953</td>\n",
       "      <td>0.591847</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>-1.212024</td>\n",
       "      <td>-1.251752</td>\n",
       "      <td>-1.213432</td>\n",
       "      <td>-1.268125</td>\n",
       "      <td>-1.216535</td>\n",
       "      <td>-1.279044</td>\n",
       "      <td>-1.220407</td>\n",
       "      <td>-1.287793</td>\n",
       "      <td>-1.224767</td>\n",
       "      <td>-1.293151</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.281949</td>\n",
       "      <td>-1.244998</td>\n",
       "      <td>-1.281421</td>\n",
       "      <td>-1.247315</td>\n",
       "      <td>-1.281235</td>\n",
       "      <td>-1.256962</td>\n",
       "      <td>-1.277252</td>\n",
       "      <td>-1.245367</td>\n",
       "      <td>-1.288457</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>-1.108010</td>\n",
       "      <td>-1.186143</td>\n",
       "      <td>-1.111683</td>\n",
       "      <td>-1.206850</td>\n",
       "      <td>-1.117108</td>\n",
       "      <td>-1.223807</td>\n",
       "      <td>-1.124637</td>\n",
       "      <td>-1.238498</td>\n",
       "      <td>-1.135124</td>\n",
       "      <td>-1.251204</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.237526</td>\n",
       "      <td>-1.193992</td>\n",
       "      <td>-1.238669</td>\n",
       "      <td>-1.198848</td>\n",
       "      <td>-1.236787</td>\n",
       "      <td>-1.213605</td>\n",
       "      <td>-1.236175</td>\n",
       "      <td>-1.194363</td>\n",
       "      <td>-1.254358</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>-1.152421</td>\n",
       "      <td>-1.039386</td>\n",
       "      <td>-1.151447</td>\n",
       "      <td>-1.031535</td>\n",
       "      <td>-1.151031</td>\n",
       "      <td>-1.022946</td>\n",
       "      <td>-1.149164</td>\n",
       "      <td>-1.013384</td>\n",
       "      <td>-1.147930</td>\n",
       "      <td>-1.004366</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.015409</td>\n",
       "      <td>-1.142987</td>\n",
       "      <td>-1.013399</td>\n",
       "      <td>-1.136857</td>\n",
       "      <td>-1.014550</td>\n",
       "      <td>-1.132451</td>\n",
       "      <td>-0.999566</td>\n",
       "      <td>-1.143359</td>\n",
       "      <td>-0.979944</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>-1.175795</td>\n",
       "      <td>-1.053198</td>\n",
       "      <td>-1.179516</td>\n",
       "      <td>-1.051960</td>\n",
       "      <td>-1.181443</td>\n",
       "      <td>-1.048054</td>\n",
       "      <td>-1.184201</td>\n",
       "      <td>-1.041318</td>\n",
       "      <td>-1.189841</td>\n",
       "      <td>-1.026952</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.003892</td>\n",
       "      <td>-1.191725</td>\n",
       "      <td>-1.003533</td>\n",
       "      <td>-1.188704</td>\n",
       "      <td>-1.001380</td>\n",
       "      <td>-1.185813</td>\n",
       "      <td>-0.989707</td>\n",
       "      <td>-1.194363</td>\n",
       "      <td>-1.002677</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>-0.613652</td>\n",
       "      <td>-0.904715</td>\n",
       "      <td>-0.627501</td>\n",
       "      <td>-0.832391</td>\n",
       "      <td>-0.638690</td>\n",
       "      <td>-0.755132</td>\n",
       "      <td>-0.638779</td>\n",
       "      <td>-0.681465</td>\n",
       "      <td>-0.625207</td>\n",
       "      <td>-0.618781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.528397</td>\n",
       "      <td>-0.442511</td>\n",
       "      <td>-0.520106</td>\n",
       "      <td>-0.439171</td>\n",
       "      <td>-0.514103</td>\n",
       "      <td>-0.440973</td>\n",
       "      <td>-0.509917</td>\n",
       "      <td>-0.447434</td>\n",
       "      <td>-0.530165</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>-1.022696</td>\n",
       "      <td>-1.051472</td>\n",
       "      <td>-1.022800</td>\n",
       "      <td>-1.072385</td>\n",
       "      <td>-1.024700</td>\n",
       "      <td>-1.091574</td>\n",
       "      <td>-1.031203</td>\n",
       "      <td>-1.107045</td>\n",
       "      <td>-1.041988</td>\n",
       "      <td>-1.118912</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.120709</td>\n",
       "      <td>-1.085181</td>\n",
       "      <td>-1.121923</td>\n",
       "      <td>-1.090645</td>\n",
       "      <td>-1.119907</td>\n",
       "      <td>-1.105770</td>\n",
       "      <td>-1.116227</td>\n",
       "      <td>-1.086687</td>\n",
       "      <td>-1.134201</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>-0.799475</td>\n",
       "      <td>-1.048018</td>\n",
       "      <td>-0.800590</td>\n",
       "      <td>-1.070683</td>\n",
       "      <td>-0.804791</td>\n",
       "      <td>-1.091574</td>\n",
       "      <td>-0.810464</td>\n",
       "      <td>-1.108688</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>-1.122138</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.127290</td>\n",
       "      <td>-0.874358</td>\n",
       "      <td>-1.126856</td>\n",
       "      <td>-0.882128</td>\n",
       "      <td>-1.124845</td>\n",
       "      <td>-0.901217</td>\n",
       "      <td>-1.119513</td>\n",
       "      <td>-0.874736</td>\n",
       "      <td>-1.140696</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>-1.172289</td>\n",
       "      <td>-1.046292</td>\n",
       "      <td>-1.174837</td>\n",
       "      <td>-1.040045</td>\n",
       "      <td>-1.176764</td>\n",
       "      <td>-1.031315</td>\n",
       "      <td>-1.178362</td>\n",
       "      <td>-1.019957</td>\n",
       "      <td>-1.184020</td>\n",
       "      <td>-1.002752</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.966050</td>\n",
       "      <td>-1.175857</td>\n",
       "      <td>-0.965714</td>\n",
       "      <td>-1.172925</td>\n",
       "      <td>-0.965163</td>\n",
       "      <td>-1.172472</td>\n",
       "      <td>-0.960131</td>\n",
       "      <td>-1.176228</td>\n",
       "      <td>-0.966954</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>-0.877777</td>\n",
       "      <td>-0.654365</td>\n",
       "      <td>-0.880118</td>\n",
       "      <td>-0.662182</td>\n",
       "      <td>-0.881993</td>\n",
       "      <td>-0.666418</td>\n",
       "      <td>-0.882876</td>\n",
       "      <td>-0.671606</td>\n",
       "      <td>-0.883658</td>\n",
       "      <td>-0.675248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678120</td>\n",
       "      <td>-0.880025</td>\n",
       "      <td>-0.679604</td>\n",
       "      <td>-0.879874</td>\n",
       "      <td>-0.677078</td>\n",
       "      <td>-0.887877</td>\n",
       "      <td>-0.675872</td>\n",
       "      <td>-0.880403</td>\n",
       "      <td>-0.697412</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>-0.942056</td>\n",
       "      <td>-0.562857</td>\n",
       "      <td>-0.936255</td>\n",
       "      <td>-0.561758</td>\n",
       "      <td>-0.932292</td>\n",
       "      <td>-0.557619</td>\n",
       "      <td>-0.930761</td>\n",
       "      <td>-0.553298</td>\n",
       "      <td>-0.929061</td>\n",
       "      <td>-0.549408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569530</td>\n",
       "      <td>-0.924230</td>\n",
       "      <td>-0.572724</td>\n",
       "      <td>-0.918196</td>\n",
       "      <td>-0.571721</td>\n",
       "      <td>-0.904552</td>\n",
       "      <td>-0.554281</td>\n",
       "      <td>-0.921207</td>\n",
       "      <td>-0.522046</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>-0.822849</td>\n",
       "      <td>-1.034206</td>\n",
       "      <td>-0.832168</td>\n",
       "      <td>-1.050258</td>\n",
       "      <td>-0.843392</td>\n",
       "      <td>-1.063118</td>\n",
       "      <td>-0.852510</td>\n",
       "      <td>-1.072538</td>\n",
       "      <td>-0.863866</td>\n",
       "      <td>-1.081805</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.048315</td>\n",
       "      <td>-0.890226</td>\n",
       "      <td>-1.046285</td>\n",
       "      <td>-0.897908</td>\n",
       "      <td>-1.042535</td>\n",
       "      <td>-0.921228</td>\n",
       "      <td>-1.042287</td>\n",
       "      <td>-0.891738</td>\n",
       "      <td>-1.051389</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>-0.963092</td>\n",
       "      <td>-1.020394</td>\n",
       "      <td>-0.970171</td>\n",
       "      <td>-1.043450</td>\n",
       "      <td>-0.977911</td>\n",
       "      <td>-1.061444</td>\n",
       "      <td>-0.985654</td>\n",
       "      <td>-1.072538</td>\n",
       "      <td>-0.997749</td>\n",
       "      <td>-1.080192</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.071350</td>\n",
       "      <td>-1.055711</td>\n",
       "      <td>-1.069305</td>\n",
       "      <td>-1.056832</td>\n",
       "      <td>-1.067228</td>\n",
       "      <td>-1.052409</td>\n",
       "      <td>-1.062004</td>\n",
       "      <td>-1.056085</td>\n",
       "      <td>-1.074122</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>-0.734028</td>\n",
       "      <td>-0.978956</td>\n",
       "      <td>-0.724572</td>\n",
       "      <td>-0.958345</td>\n",
       "      <td>-0.717062</td>\n",
       "      <td>-0.939254</td>\n",
       "      <td>-0.713526</td>\n",
       "      <td>-0.919724</td>\n",
       "      <td>-0.710193</td>\n",
       "      <td>-0.902726</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.895301</td>\n",
       "      <td>-0.686204</td>\n",
       "      <td>-0.895008</td>\n",
       "      <td>-0.684883</td>\n",
       "      <td>-0.897669</td>\n",
       "      <td>-0.685547</td>\n",
       "      <td>-0.905908</td>\n",
       "      <td>-0.685454</td>\n",
       "      <td>-0.913371</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>-0.928031</td>\n",
       "      <td>-0.997948</td>\n",
       "      <td>-0.924560</td>\n",
       "      <td>-1.002599</td>\n",
       "      <td>-0.922934</td>\n",
       "      <td>-1.007882</td>\n",
       "      <td>-0.920250</td>\n",
       "      <td>-1.011741</td>\n",
       "      <td>-0.919748</td>\n",
       "      <td>-1.014046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.970986</td>\n",
       "      <td>-0.941232</td>\n",
       "      <td>-0.970647</td>\n",
       "      <td>-0.945247</td>\n",
       "      <td>-0.971748</td>\n",
       "      <td>-0.949020</td>\n",
       "      <td>-0.986421</td>\n",
       "      <td>-0.941609</td>\n",
       "      <td>-0.988063</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>-0.809993</td>\n",
       "      <td>-0.585303</td>\n",
       "      <td>-0.812286</td>\n",
       "      <td>-0.607715</td>\n",
       "      <td>-0.814149</td>\n",
       "      <td>-0.627920</td>\n",
       "      <td>-0.817472</td>\n",
       "      <td>-0.646959</td>\n",
       "      <td>-0.825448</td>\n",
       "      <td>-0.663954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.650150</td>\n",
       "      <td>-0.868690</td>\n",
       "      <td>-0.650006</td>\n",
       "      <td>-0.875366</td>\n",
       "      <td>-0.649092</td>\n",
       "      <td>-0.892324</td>\n",
       "      <td>-0.649582</td>\n",
       "      <td>-0.869069</td>\n",
       "      <td>-0.669808</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>-0.928031</td>\n",
       "      <td>-0.997948</td>\n",
       "      <td>-0.924560</td>\n",
       "      <td>-1.002599</td>\n",
       "      <td>-0.922934</td>\n",
       "      <td>-1.007882</td>\n",
       "      <td>-0.920250</td>\n",
       "      <td>-1.011741</td>\n",
       "      <td>-0.919748</td>\n",
       "      <td>-1.014046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.970986</td>\n",
       "      <td>-0.941232</td>\n",
       "      <td>-0.970647</td>\n",
       "      <td>-0.945247</td>\n",
       "      <td>-0.971748</td>\n",
       "      <td>-0.949020</td>\n",
       "      <td>-0.986421</td>\n",
       "      <td>-0.941609</td>\n",
       "      <td>-0.988063</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>-1.057756</td>\n",
       "      <td>-1.136073</td>\n",
       "      <td>-1.049699</td>\n",
       "      <td>-1.138767</td>\n",
       "      <td>-1.042246</td>\n",
       "      <td>-1.140115</td>\n",
       "      <td>-1.033539</td>\n",
       "      <td>-1.143194</td>\n",
       "      <td>-1.024525</td>\n",
       "      <td>-1.144725</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.156906</td>\n",
       "      <td>-1.022840</td>\n",
       "      <td>-1.161387</td>\n",
       "      <td>-1.028654</td>\n",
       "      <td>-1.167647</td>\n",
       "      <td>-1.042403</td>\n",
       "      <td>-1.178665</td>\n",
       "      <td>-1.022082</td>\n",
       "      <td>-1.176418</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>-0.873103</td>\n",
       "      <td>-0.808028</td>\n",
       "      <td>-0.874270</td>\n",
       "      <td>-0.810263</td>\n",
       "      <td>-0.873805</td>\n",
       "      <td>-0.810369</td>\n",
       "      <td>-0.875868</td>\n",
       "      <td>-0.809632</td>\n",
       "      <td>-0.877837</td>\n",
       "      <td>-0.805927</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.781775</td>\n",
       "      <td>-0.884559</td>\n",
       "      <td>-0.781551</td>\n",
       "      <td>-0.882128</td>\n",
       "      <td>-0.782435</td>\n",
       "      <td>-0.875648</td>\n",
       "      <td>-0.787604</td>\n",
       "      <td>-0.883804</td>\n",
       "      <td>-0.799708</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>-0.273562</td>\n",
       "      <td>-0.963417</td>\n",
       "      <td>-0.274306</td>\n",
       "      <td>-0.999195</td>\n",
       "      <td>-0.277245</td>\n",
       "      <td>-1.029642</td>\n",
       "      <td>-0.280225</td>\n",
       "      <td>-1.054463</td>\n",
       "      <td>-0.287590</td>\n",
       "      <td>-1.075352</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.064768</td>\n",
       "      <td>-0.409641</td>\n",
       "      <td>-1.067661</td>\n",
       "      <td>-0.424518</td>\n",
       "      <td>-1.065582</td>\n",
       "      <td>-0.437638</td>\n",
       "      <td>-1.070220</td>\n",
       "      <td>-0.407764</td>\n",
       "      <td>-1.083865</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>-0.849729</td>\n",
       "      <td>-1.054925</td>\n",
       "      <td>-0.854388</td>\n",
       "      <td>-1.058768</td>\n",
       "      <td>-0.858599</td>\n",
       "      <td>-1.058097</td>\n",
       "      <td>-0.863021</td>\n",
       "      <td>-1.056106</td>\n",
       "      <td>-0.865031</td>\n",
       "      <td>-1.055992</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.015409</td>\n",
       "      <td>-0.859623</td>\n",
       "      <td>-1.013399</td>\n",
       "      <td>-0.866349</td>\n",
       "      <td>-1.012903</td>\n",
       "      <td>-0.887877</td>\n",
       "      <td>-1.022569</td>\n",
       "      <td>-0.862269</td>\n",
       "      <td>-1.035152</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>-0.039822</td>\n",
       "      <td>-1.051472</td>\n",
       "      <td>-0.036893</td>\n",
       "      <td>-1.074087</td>\n",
       "      <td>-0.035112</td>\n",
       "      <td>-1.094921</td>\n",
       "      <td>-0.037296</td>\n",
       "      <td>-1.113617</td>\n",
       "      <td>-0.045438</td>\n",
       "      <td>-1.125365</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.117418</td>\n",
       "      <td>-0.145545</td>\n",
       "      <td>-1.118635</td>\n",
       "      <td>-0.156264</td>\n",
       "      <td>-1.119907</td>\n",
       "      <td>-0.166383</td>\n",
       "      <td>-1.135944</td>\n",
       "      <td>-0.144808</td>\n",
       "      <td>-1.135825</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>-0.928031</td>\n",
       "      <td>-0.997948</td>\n",
       "      <td>-0.924560</td>\n",
       "      <td>-1.002599</td>\n",
       "      <td>-0.922934</td>\n",
       "      <td>-1.007882</td>\n",
       "      <td>-0.920250</td>\n",
       "      <td>-1.011741</td>\n",
       "      <td>-0.919748</td>\n",
       "      <td>-1.014046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.970986</td>\n",
       "      <td>-0.941232</td>\n",
       "      <td>-0.970647</td>\n",
       "      <td>-0.945247</td>\n",
       "      <td>-0.971748</td>\n",
       "      <td>-0.949020</td>\n",
       "      <td>-0.986421</td>\n",
       "      <td>-0.941609</td>\n",
       "      <td>-0.988063</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>-1.091649</td>\n",
       "      <td>-1.122260</td>\n",
       "      <td>-1.091801</td>\n",
       "      <td>-1.096214</td>\n",
       "      <td>-1.092544</td>\n",
       "      <td>-1.066466</td>\n",
       "      <td>-1.094271</td>\n",
       "      <td>-1.039675</td>\n",
       "      <td>-1.086228</td>\n",
       "      <td>-1.018885</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.084512</td>\n",
       "      <td>-1.004705</td>\n",
       "      <td>-1.084104</td>\n",
       "      <td>-1.002730</td>\n",
       "      <td>-1.080398</td>\n",
       "      <td>-1.019058</td>\n",
       "      <td>-1.043930</td>\n",
       "      <td>-1.006214</td>\n",
       "      <td>-1.030281</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>-1.076456</td>\n",
       "      <td>-0.973777</td>\n",
       "      <td>-1.073089</td>\n",
       "      <td>-0.905580</td>\n",
       "      <td>-1.069150</td>\n",
       "      <td>-0.838824</td>\n",
       "      <td>-1.059233</td>\n",
       "      <td>-0.778412</td>\n",
       "      <td>-1.043152</td>\n",
       "      <td>-0.725261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.673184</td>\n",
       "      <td>-0.899294</td>\n",
       "      <td>-0.666449</td>\n",
       "      <td>-0.887764</td>\n",
       "      <td>-0.667200</td>\n",
       "      <td>-0.870090</td>\n",
       "      <td>-0.667656</td>\n",
       "      <td>-0.900805</td>\n",
       "      <td>-0.703907</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>-1.013346</td>\n",
       "      <td>-0.814934</td>\n",
       "      <td>-1.011104</td>\n",
       "      <td>-0.830688</td>\n",
       "      <td>-1.011833</td>\n",
       "      <td>-0.843845</td>\n",
       "      <td>-1.011348</td>\n",
       "      <td>-0.855640</td>\n",
       "      <td>-1.014047</td>\n",
       "      <td>-0.867233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.875558</td>\n",
       "      <td>-1.046643</td>\n",
       "      <td>-0.880210</td>\n",
       "      <td>-1.048942</td>\n",
       "      <td>-0.884499</td>\n",
       "      <td>-1.045738</td>\n",
       "      <td>-0.900979</td>\n",
       "      <td>-1.043617</td>\n",
       "      <td>-0.890638</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>-0.908163</td>\n",
       "      <td>-0.122587</td>\n",
       "      <td>-0.908186</td>\n",
       "      <td>-0.115811</td>\n",
       "      <td>-0.905388</td>\n",
       "      <td>-0.104008</td>\n",
       "      <td>-0.907402</td>\n",
       "      <td>-0.094856</td>\n",
       "      <td>-0.910434</td>\n",
       "      <td>-0.076704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067710</td>\n",
       "      <td>-0.876625</td>\n",
       "      <td>-0.066276</td>\n",
       "      <td>-0.871984</td>\n",
       "      <td>-0.067982</td>\n",
       "      <td>-0.874536</td>\n",
       "      <td>-0.056416</td>\n",
       "      <td>-0.878137</td>\n",
       "      <td>-0.090128</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>-0.624170</td>\n",
       "      <td>-1.041112</td>\n",
       "      <td>-0.621654</td>\n",
       "      <td>-1.048556</td>\n",
       "      <td>-0.621144</td>\n",
       "      <td>-1.054749</td>\n",
       "      <td>-0.622428</td>\n",
       "      <td>-1.057750</td>\n",
       "      <td>-0.625207</td>\n",
       "      <td>-1.059219</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.074640</td>\n",
       "      <td>-0.643133</td>\n",
       "      <td>-1.075883</td>\n",
       "      <td>-0.645433</td>\n",
       "      <td>-1.077105</td>\n",
       "      <td>-0.645526</td>\n",
       "      <td>-1.083365</td>\n",
       "      <td>-0.641250</td>\n",
       "      <td>-1.083865</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>-1.076456</td>\n",
       "      <td>-0.973777</td>\n",
       "      <td>-1.073089</td>\n",
       "      <td>-0.905580</td>\n",
       "      <td>-1.069150</td>\n",
       "      <td>-0.838824</td>\n",
       "      <td>-1.059233</td>\n",
       "      <td>-0.778412</td>\n",
       "      <td>-1.043152</td>\n",
       "      <td>-0.725261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.673184</td>\n",
       "      <td>-0.899294</td>\n",
       "      <td>-0.666449</td>\n",
       "      <td>-0.887764</td>\n",
       "      <td>-0.667200</td>\n",
       "      <td>-0.870090</td>\n",
       "      <td>-0.667656</td>\n",
       "      <td>-0.900805</td>\n",
       "      <td>-0.703907</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>-0.745715</td>\n",
       "      <td>-1.068737</td>\n",
       "      <td>-0.739775</td>\n",
       "      <td>-1.062173</td>\n",
       "      <td>-0.734608</td>\n",
       "      <td>-1.049728</td>\n",
       "      <td>-0.731045</td>\n",
       "      <td>-1.033102</td>\n",
       "      <td>-0.727656</td>\n",
       "      <td>-1.015659</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.071350</td>\n",
       "      <td>-0.758745</td>\n",
       "      <td>-1.072594</td>\n",
       "      <td>-0.754764</td>\n",
       "      <td>-1.078752</td>\n",
       "      <td>-0.738909</td>\n",
       "      <td>-1.098153</td>\n",
       "      <td>-0.753459</td>\n",
       "      <td>-1.075746</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>-0.825186</td>\n",
       "      <td>-0.890902</td>\n",
       "      <td>-0.821642</td>\n",
       "      <td>-0.878347</td>\n",
       "      <td>-0.815319</td>\n",
       "      <td>-0.863931</td>\n",
       "      <td>-0.809296</td>\n",
       "      <td>-0.850711</td>\n",
       "      <td>-0.806821</td>\n",
       "      <td>-0.836580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.829489</td>\n",
       "      <td>-0.785948</td>\n",
       "      <td>-0.832525</td>\n",
       "      <td>-0.784069</td>\n",
       "      <td>-0.833467</td>\n",
       "      <td>-0.784489</td>\n",
       "      <td>-0.831968</td>\n",
       "      <td>-0.784062</td>\n",
       "      <td>-0.850044</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1185 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x1        y1        x2        y2        x3        y3        x4  \\\n",
       "0     1.361444  1.510731  1.347821  1.526705  1.338147  1.543050  1.331516   \n",
       "1     0.336498  1.483107  0.304608  1.521599  0.274866  1.553093  0.247679   \n",
       "2     2.507934  0.970321  2.511495  0.988845  2.513722  1.002400  2.511124   \n",
       "3     3.811030  1.151609  3.804986  1.167564  3.802761  1.186522  3.805188   \n",
       "4    -0.896476  0.954782 -0.889474  0.966718 -0.880824  0.978966 -0.872365   \n",
       "5    -0.057353  1.194772 -0.054436  1.186287 -0.047979  1.173131 -0.033792   \n",
       "6     1.920080  0.013811  1.918548  0.011846  1.917162  0.008139  1.918984   \n",
       "7     3.354070  0.188193  3.351212  0.176948  3.346567  0.168828  3.339185   \n",
       "8    -0.141499  0.162294 -0.139811  0.165034 -0.140387  0.168828 -0.140074   \n",
       "9     1.175621  0.212364  1.179410  0.192267  1.181404  0.168828  1.179685   \n",
       "10    0.414800  1.350162  0.415712  1.347986  0.421082  1.343863  0.422868   \n",
       "11   -0.188247  0.562854 -0.184252  0.571833 -0.181327  0.580592 -0.175111   \n",
       "12    0.684769  0.640549  0.689380  0.650129  0.690119  0.657589  0.687988   \n",
       "13    1.530905  0.726877  1.539623  0.723319  1.546359  0.717847  1.551087   \n",
       "14   -0.780776  0.937516 -0.777200  0.951399 -0.775548  0.962227 -0.764915   \n",
       "15    2.575719  0.859821  2.568802  0.850976  2.560511  0.841711  2.546162   \n",
       "16    0.332992  0.492066  0.332676  0.502047  0.328673  0.506943  0.317754   \n",
       "17    1.064595  1.146429  1.054271  1.131820  1.044546  1.114547  1.043038   \n",
       "18    2.603767  0.545589  2.601548  0.556514  2.596773  0.565528  2.590543   \n",
       "19    0.795795  0.243442  0.806332  0.313115  0.814110  0.379731  0.817628   \n",
       "20    0.169374  0.274520  0.164265  0.301201  0.157893  0.326168  0.150741   \n",
       "21    1.215357  0.659541  1.208648  0.668852  1.202459  0.677675  1.192533   \n",
       "22   -1.203843  2.403359 -1.206415  2.370941 -1.208347  2.333102 -1.207560   \n",
       "23    0.738529  2.356742  0.744348  2.307964  0.747435  2.252758  0.746384   \n",
       "24    1.335732  2.411991  1.336126  2.367537  1.333468  2.321385  1.326844   \n",
       "25    0.075878  2.028697  0.074212  1.986269  0.070164  1.939750  0.063146   \n",
       "26   -0.682605  2.306672 -0.690655  2.260305 -0.698346  2.209238 -0.705351   \n",
       "27    2.180699  2.056322  2.181690  2.018609  2.180350  1.973227  2.173592   \n",
       "28    2.237966  0.923704  2.240166  0.947995  2.241176  0.968923  2.241332   \n",
       "29    0.843712  0.355668  0.846096  0.415241  0.849202  0.473466  0.852666   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1155 -1.212024 -1.251752 -1.213432 -1.268125 -1.216535 -1.279044 -1.220407   \n",
       "1156 -1.108010 -1.186143 -1.111683 -1.206850 -1.117108 -1.223807 -1.124637   \n",
       "1157 -1.152421 -1.039386 -1.151447 -1.031535 -1.151031 -1.022946 -1.149164   \n",
       "1158 -1.175795 -1.053198 -1.179516 -1.051960 -1.181443 -1.048054 -1.184201   \n",
       "1159 -0.613652 -0.904715 -0.627501 -0.832391 -0.638690 -0.755132 -0.638779   \n",
       "1160 -1.022696 -1.051472 -1.022800 -1.072385 -1.024700 -1.091574 -1.031203   \n",
       "1161 -0.799475 -1.048018 -0.800590 -1.070683 -0.804791 -1.091574 -0.810464   \n",
       "1162 -1.172289 -1.046292 -1.174837 -1.040045 -1.176764 -1.031315 -1.178362   \n",
       "1163 -0.877777 -0.654365 -0.880118 -0.662182 -0.881993 -0.666418 -0.882876   \n",
       "1164 -0.942056 -0.562857 -0.936255 -0.561758 -0.932292 -0.557619 -0.930761   \n",
       "1165 -0.822849 -1.034206 -0.832168 -1.050258 -0.843392 -1.063118 -0.852510   \n",
       "1166 -0.963092 -1.020394 -0.970171 -1.043450 -0.977911 -1.061444 -0.985654   \n",
       "1167 -0.734028 -0.978956 -0.724572 -0.958345 -0.717062 -0.939254 -0.713526   \n",
       "1168 -0.928031 -0.997948 -0.924560 -1.002599 -0.922934 -1.007882 -0.920250   \n",
       "1169 -0.809993 -0.585303 -0.812286 -0.607715 -0.814149 -0.627920 -0.817472   \n",
       "1170 -0.928031 -0.997948 -0.924560 -1.002599 -0.922934 -1.007882 -0.920250   \n",
       "1171 -1.057756 -1.136073 -1.049699 -1.138767 -1.042246 -1.140115 -1.033539   \n",
       "1172 -0.873103 -0.808028 -0.874270 -0.810263 -0.873805 -0.810369 -0.875868   \n",
       "1173 -0.273562 -0.963417 -0.274306 -0.999195 -0.277245 -1.029642 -0.280225   \n",
       "1174 -0.849729 -1.054925 -0.854388 -1.058768 -0.858599 -1.058097 -0.863021   \n",
       "1175 -0.039822 -1.051472 -0.036893 -1.074087 -0.035112 -1.094921 -0.037296   \n",
       "1176 -0.928031 -0.997948 -0.924560 -1.002599 -0.922934 -1.007882 -0.920250   \n",
       "1177 -1.091649 -1.122260 -1.091801 -1.096214 -1.092544 -1.066466 -1.094271   \n",
       "1178 -1.076456 -0.973777 -1.073089 -0.905580 -1.069150 -0.838824 -1.059233   \n",
       "1179 -1.013346 -0.814934 -1.011104 -0.830688 -1.011833 -0.843845 -1.011348   \n",
       "1180 -0.908163 -0.122587 -0.908186 -0.115811 -0.905388 -0.104008 -0.907402   \n",
       "1181 -0.624170 -1.041112 -0.621654 -1.048556 -0.621144 -1.054749 -0.622428   \n",
       "1182 -1.076456 -0.973777 -1.073089 -0.905580 -1.069150 -0.838824 -1.059233   \n",
       "1183 -0.745715 -1.068737 -0.739775 -1.062173 -0.734608 -1.049728 -0.731045   \n",
       "1184 -0.825186 -0.890902 -0.821642 -0.878347 -0.815319 -0.863931 -0.809296   \n",
       "\n",
       "            y4        x5        y5  ...       y62       x63       y63  \\\n",
       "0     1.559810  1.321326  1.585019  ...  1.628604  1.334749  1.638874   \n",
       "1     1.587744  0.230476  1.625352  ...  1.840849  0.289702  1.859211   \n",
       "2     1.014280  2.508804  1.018743  ...  1.064263  2.507876  1.066654   \n",
       "3     1.196671  3.802224  1.202661  ...  1.241956  3.716140  1.247528   \n",
       "4     0.987989 -0.853389  0.996156  ...  0.977061 -0.717941  0.976217   \n",
       "5     1.157235 -0.018661  1.141355  ...  1.158046 -0.059403  1.148869   \n",
       "6     0.003734  1.909244 -0.000878  ... -0.026578  1.836870 -0.031746   \n",
       "7     0.164764  3.326068  0.162068  ...  0.164278  3.232154  0.165572   \n",
       "8     0.171337 -0.132753  0.170134  ...  0.198830 -0.085472  0.201746   \n",
       "9     0.145046  1.175802  0.120121  ...  0.174150  1.114858  0.172149   \n",
       "10    1.337983  0.424896  1.330114  ...  1.335739  0.438185  1.336320   \n",
       "11    0.585414 -0.163022  0.583145  ...  0.587123 -0.082072  0.584871   \n",
       "12    0.664286  0.689169  0.671878  ...  0.677615  0.740817  0.680241   \n",
       "13    0.711938  1.554165  0.704144  ...  0.672679  1.531970  0.667086   \n",
       "14    0.971558 -0.742790  0.970343  ...  0.937574 -0.611396  0.938397   \n",
       "15    0.831889  2.525103  0.829984  ...  0.876698  2.461404  0.879202   \n",
       "16    0.514758  0.310805  0.521839  ...  0.518020  0.357709  0.519098   \n",
       "17    1.098081  1.036098  1.086502  ...  1.130075  0.982244  1.134070   \n",
       "18    0.567339  2.590298  0.567012  ...  0.522956  2.587218  0.530608   \n",
       "19    0.445745  0.828872  0.504092  ...  0.409429  0.987911  0.410574   \n",
       "20    0.353728  0.148982  0.378253  ...  0.394622  0.237563  0.402352   \n",
       "21    0.684004  1.179294  0.691238  ...  0.710521  1.179465  0.714771   \n",
       "22    2.292661 -1.204975  2.254549  ...  2.317989 -1.161122  2.317974   \n",
       "23    2.195715  0.741557  2.138389  ...  2.155103  0.697746  2.151898   \n",
       "24    2.269657  1.315505  2.222282  ...  2.258758  1.264474  2.260423   \n",
       "25    1.893373  0.054683  1.846378  ...  1.885273  0.038075  1.885520   \n",
       "26    2.157922 -0.711357  2.109349  ...  2.215980 -0.707740  2.216026   \n",
       "27    1.926236  2.160710  1.878644  ...  1.903371  2.086230  1.900319   \n",
       "28    0.986346  2.244532  0.991316  ...  1.013258  2.238113  1.020613   \n",
       "29    0.524617  0.864962  0.571852  ...  0.560798  0.986777  0.568428   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1155 -1.287793 -1.224767 -1.293151  ... -1.281949 -1.244998 -1.281421   \n",
       "1156 -1.238498 -1.135124 -1.251204  ... -1.237526 -1.193992 -1.238669   \n",
       "1157 -1.013384 -1.147930 -1.004366  ... -1.015409 -1.142987 -1.013399   \n",
       "1158 -1.041318 -1.189841 -1.026952  ... -1.003892 -1.191725 -1.003533   \n",
       "1159 -0.681465 -0.625207 -0.618781  ... -0.528397 -0.442511 -0.520106   \n",
       "1160 -1.107045 -1.041988 -1.118912  ... -1.120709 -1.085181 -1.121923   \n",
       "1161 -1.108688 -0.821955 -1.122138  ... -1.127290 -0.874358 -1.126856   \n",
       "1162 -1.019957 -1.184020 -1.002752  ... -0.966050 -1.175857 -0.965714   \n",
       "1163 -0.671606 -0.883658 -0.675248  ... -0.678120 -0.880025 -0.679604   \n",
       "1164 -0.553298 -0.929061 -0.549408  ... -0.569530 -0.924230 -0.572724   \n",
       "1165 -1.072538 -0.863866 -1.081805  ... -1.048315 -0.890226 -1.046285   \n",
       "1166 -1.072538 -0.997749 -1.080192  ... -1.071350 -1.055711 -1.069305   \n",
       "1167 -0.919724 -0.710193 -0.902726  ... -0.895301 -0.686204 -0.895008   \n",
       "1168 -1.011741 -0.919748 -1.014046  ... -0.970986 -0.941232 -0.970647   \n",
       "1169 -0.646959 -0.825448 -0.663954  ... -0.650150 -0.868690 -0.650006   \n",
       "1170 -1.011741 -0.919748 -1.014046  ... -0.970986 -0.941232 -0.970647   \n",
       "1171 -1.143194 -1.024525 -1.144725  ... -1.156906 -1.022840 -1.161387   \n",
       "1172 -0.809632 -0.877837 -0.805927  ... -0.781775 -0.884559 -0.781551   \n",
       "1173 -1.054463 -0.287590 -1.075352  ... -1.064768 -0.409641 -1.067661   \n",
       "1174 -1.056106 -0.865031 -1.055992  ... -1.015409 -0.859623 -1.013399   \n",
       "1175 -1.113617 -0.045438 -1.125365  ... -1.117418 -0.145545 -1.118635   \n",
       "1176 -1.011741 -0.919748 -1.014046  ... -0.970986 -0.941232 -0.970647   \n",
       "1177 -1.039675 -1.086228 -1.018885  ... -1.084512 -1.004705 -1.084104   \n",
       "1178 -0.778412 -1.043152 -0.725261  ... -0.673184 -0.899294 -0.666449   \n",
       "1179 -0.855640 -1.014047 -0.867233  ... -0.875558 -1.046643 -0.880210   \n",
       "1180 -0.094856 -0.910434 -0.076704  ... -0.067710 -0.876625 -0.066276   \n",
       "1181 -1.057750 -0.625207 -1.059219  ... -1.074640 -0.643133 -1.075883   \n",
       "1182 -0.778412 -1.043152 -0.725261  ... -0.673184 -0.899294 -0.666449   \n",
       "1183 -1.033102 -0.727656 -1.015659  ... -1.071350 -0.758745 -1.072594   \n",
       "1184 -0.850711 -0.806821 -0.836580  ... -0.829489 -0.785948 -0.832525   \n",
       "\n",
       "           x64       y64       x65       y65       x67       y67    target  \n",
       "0     1.351820  1.645717  1.414456  1.668856  1.330916  1.686256 -0.688390  \n",
       "1     0.300219  1.874539  0.329436  1.892320  0.280228  1.817780 -0.688390  \n",
       "2     2.504862  1.066253  2.486136  1.049401  2.507415  1.027013 -0.688390  \n",
       "3     3.715387  1.248982  3.705673  1.261363  3.717917  1.280318 -0.688390  \n",
       "4    -0.706298  0.975712 -0.686659  0.977104 -0.717190  0.991290 -0.688390  \n",
       "5    -0.050315  1.137040 -0.006298  1.095409 -0.048467  1.147170 -0.688390  \n",
       "6     1.826337 -0.035058  1.811334 -0.031770  1.838692 -0.039792 -0.688390  \n",
       "7     3.217200  0.164133  3.194290  0.158832  3.233943  0.182662 -0.688390  \n",
       "8    -0.085256  0.201996 -0.095234  0.186765 -0.089270  0.189157 -0.688390  \n",
       "9     1.097091  0.172364  1.055377  0.155546  1.115564  0.135573 -0.688390  \n",
       "10    0.445617  1.334584  0.459505  1.323802  0.440041  1.351763 -0.688390  \n",
       "11   -0.084129  0.585561 -0.096346  0.581113 -0.080203  0.557748 -0.688390  \n",
       "12    0.736413  0.682687  0.712973  0.692845  0.738133  0.687649 -0.688390  \n",
       "13    1.522015  0.662933  1.503392  0.655053  1.534933  0.627570 -0.688390  \n",
       "14   -0.607111  0.942788 -0.608840  0.952457 -0.611781  0.936083 -0.688390  \n",
       "15    2.448506  0.883524  2.416099  0.901521  2.457544  0.876004 -0.688390  \n",
       "16    0.359956  0.524652  0.359452  0.540035  0.355034  0.522026 -0.688390  \n",
       "17    0.983252  1.137040  1.004239  1.139773  0.979553  1.150418 -0.688390  \n",
       "18    2.573616  0.534529  2.529493  0.553180  2.581088  0.556125 -0.688390  \n",
       "19    1.004667  0.409417  1.026473  0.431589  0.985220  0.470066 -0.688390  \n",
       "20    0.243863  0.411064  0.242723  0.438162  0.230357  0.450581 -0.688390  \n",
       "21    1.174862  0.720550  1.156542  0.745425  1.175636  0.699015 -0.688390  \n",
       "22   -1.160526  2.322307 -1.158020  2.321174 -1.160360  2.267559 -0.688390  \n",
       "23    0.689074  2.152748  0.664058  2.148647  0.699596  2.121421 -0.688390  \n",
       "24    1.258269  2.261397  1.239919  2.253806  1.262910  2.243203 -0.688390  \n",
       "25    0.036473  1.889355  0.031500  1.887391  0.035407  1.858373 -0.688390  \n",
       "26   -0.709679  2.223535 -0.720010  2.222587 -0.708122  2.171758 -0.688390  \n",
       "27    2.068667  1.904171  2.029227  1.905465  2.085779  1.859997 -0.688390  \n",
       "28    2.247879  1.018513  2.271578  0.996822  2.239926  1.033508 -0.688390  \n",
       "29    1.000159  0.575684  1.014244  0.602473  0.982953  0.591847 -0.688390  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1155 -1.247315 -1.281235 -1.256962 -1.277252 -1.245367 -1.288457  1.452665  \n",
       "1156 -1.198848 -1.236787 -1.213605 -1.236175 -1.194363 -1.254358  1.452665  \n",
       "1157 -1.136857 -1.014550 -1.132451 -0.999566 -1.143359 -0.979944  1.452665  \n",
       "1158 -1.188704 -1.001380 -1.185813 -0.989707 -1.194363 -1.002677  1.452665  \n",
       "1159 -0.439171 -0.514103 -0.440973 -0.509917 -0.447434 -0.530165  1.452665  \n",
       "1160 -1.090645 -1.119907 -1.105770 -1.116227 -1.086687 -1.134201  1.452665  \n",
       "1161 -0.882128 -1.124845 -0.901217 -1.119513 -0.874736 -1.140696  1.452665  \n",
       "1162 -1.172925 -0.965163 -1.172472 -0.960131 -1.176228 -0.966954  1.452665  \n",
       "1163 -0.879874 -0.677078 -0.887877 -0.675872 -0.880403 -0.697412  1.452665  \n",
       "1164 -0.918196 -0.571721 -0.904552 -0.554281 -0.921207 -0.522046  1.452665  \n",
       "1165 -0.897908 -1.042535 -0.921228 -1.042287 -0.891738 -1.051389  1.452665  \n",
       "1166 -1.056832 -1.067228 -1.052409 -1.062004 -1.056085 -1.074122  1.452665  \n",
       "1167 -0.684883 -0.897669 -0.685547 -0.905908 -0.685454 -0.913371  1.452665  \n",
       "1168 -0.945247 -0.971748 -0.949020 -0.986421 -0.941609 -0.988063  1.452665  \n",
       "1169 -0.875366 -0.649092 -0.892324 -0.649582 -0.869069 -0.669808  1.452665  \n",
       "1170 -0.945247 -0.971748 -0.949020 -0.986421 -0.941609 -0.988063  1.452665  \n",
       "1171 -1.028654 -1.167647 -1.042403 -1.178665 -1.022082 -1.176418  1.452665  \n",
       "1172 -0.882128 -0.782435 -0.875648 -0.787604 -0.883804 -0.799708  1.452665  \n",
       "1173 -0.424518 -1.065582 -0.437638 -1.070220 -0.407764 -1.083865  1.452665  \n",
       "1174 -0.866349 -1.012903 -0.887877 -1.022569 -0.862269 -1.035152  1.452665  \n",
       "1175 -0.156264 -1.119907 -0.166383 -1.135944 -0.144808 -1.135825  1.452665  \n",
       "1176 -0.945247 -0.971748 -0.949020 -0.986421 -0.941609 -0.988063  1.452665  \n",
       "1177 -1.002730 -1.080398 -1.019058 -1.043930 -1.006214 -1.030281  1.452665  \n",
       "1178 -0.887764 -0.667200 -0.870090 -0.667656 -0.900805 -0.703907  1.452665  \n",
       "1179 -1.048942 -0.884499 -1.045738 -0.900979 -1.043617 -0.890638  1.452665  \n",
       "1180 -0.871984 -0.067982 -0.874536 -0.056416 -0.878137 -0.090128  1.452665  \n",
       "1181 -0.645433 -1.077105 -0.645526 -1.083365 -0.641250 -1.083865  1.452665  \n",
       "1182 -0.887764 -0.667200 -0.870090 -0.667656 -0.900805 -0.703907  1.452665  \n",
       "1183 -0.754764 -1.078752 -0.738909 -1.098153 -0.753459 -1.075746  1.452665  \n",
       "1184 -0.784069 -0.833467 -0.784489 -0.831968 -0.784062 -0.850044  1.452665  \n",
       "\n",
       "[1185 rows x 101 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_streamed_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required ML packages\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression\n",
    "from sklearn import svm #support vector Machine\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
    "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
    "from sklearn.naive_bayes import GaussianNB #Naive bayes\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:136], y, test_size=0.25, random_state=27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainb, X_testb, y_trainb, y_testb = train_test_split(df_streamed_ft.iloc[:,:100], y, test_size=0.25, random_state=27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for rbf SVM is  0.835016835016835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model1=svm.SVC(kernel='rbf',C=1,gamma=0.1)\n",
    "model1.fit(X_train,y_train)\n",
    "prediction1= model1.predict(X_test)\n",
    "print('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction1,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for rbf SVM is  0.8282828282828283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model1b=svm.SVC(kernel='rbf',C=1,gamma=0.1)\n",
    "model1b.fit(X_trainb,y_trainb)\n",
    "prediction1b=model1b.predict(X_testb)\n",
    "print('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction1b,y_testb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Logistic Regression is 0.835016835016835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train,y_train)\n",
    "prediction2=model2.predict(X_test)\n",
    "print('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction2,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Logistic Regression is 0.8282828282828283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model2b = LogisticRegression()\n",
    "model2b.fit(X_trainb,y_trainb)\n",
    "prediction2b = model2b.predict(X_testb)\n",
    "print('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction2b,y_testb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Decision Tree is 0.7946127946127947\n"
     ]
    }
   ],
   "source": [
    "model3=DecisionTreeClassifier()\n",
    "model3.fit(X_train,y_train)\n",
    "prediction3=model3.predict(X_test)\n",
    "print('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction3,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Decision Tree is 0.7744107744107744\n"
     ]
    }
   ],
   "source": [
    "model3b=DecisionTreeClassifier()\n",
    "model3b.fit(X_trainb,y_trainb)\n",
    "prediction3b=model3b.predict(X_testb)\n",
    "print('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction3b,y_testb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the KNN is 0.8080808080808081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model4=KNeighborsClassifier() \n",
    "model4.fit(X_train,y_train)\n",
    "prediction4=model4.predict(X_test)\n",
    "print('The accuracy of the KNN is',metrics.accuracy_score(prediction4,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the KNN is 0.8114478114478114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model4b=KNeighborsClassifier() \n",
    "model4b.fit(X_trainb,y_trainb)\n",
    "prediction4b = model4b.predict(X_testb)\n",
    "print('The accuracy of the KNN is',metrics.accuracy_score(prediction4b,y_testb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Naive Bayes is 0.8114478114478114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model7 = GaussianNB() \n",
    "model7.fit(X_train,y_train)\n",
    "prediction7 = model7.predict(X_test)\n",
    "print('The accuracy of the Naive Bayes is',metrics.accuracy_score(prediction7,y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Naive Bayes is 0.8080808080808081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model7b = GaussianNB() \n",
    "model7b.fit(X_trainb,y_trainb)\n",
    "prediction7b = model7b.predict(X_testb)\n",
    "print('The accuracy of the Naive Bayes is',metrics.accuracy_score(prediction7b,y_testb)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the XGboost is 0.8114478114478114\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model5 = XGBClassifier()\n",
    "model5.fit(X_train,y_train)\n",
    "prediciton5 = model5.predict(X_test)\n",
    "print('The accuracy of the XGboost is',metrics.accuracy_score(prediciton5,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the XGboost is 0.8047138047138047\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model5b = XGBClassifier()\n",
    "model5b.fit(X_trainb,y_trainb)\n",
    "prediciton5b = model5b.predict(X_testb)\n",
    "print('The accuracy of the XGboost is',metrics.accuracy_score(prediciton5b,y_testb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.640213\n",
      "Will train until validation_0-logloss hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-logloss:0.597801\n",
      "[2]\tvalidation_0-logloss:0.562773\n",
      "[3]\tvalidation_0-logloss:0.534449\n",
      "[4]\tvalidation_0-logloss:0.511387\n",
      "[5]\tvalidation_0-logloss:0.492102\n",
      "[6]\tvalidation_0-logloss:0.476113\n",
      "[7]\tvalidation_0-logloss:0.464301\n",
      "[8]\tvalidation_0-logloss:0.452637\n",
      "[9]\tvalidation_0-logloss:0.444271\n",
      "[10]\tvalidation_0-logloss:0.436633\n",
      "[11]\tvalidation_0-logloss:0.4293\n",
      "[12]\tvalidation_0-logloss:0.424387\n",
      "[13]\tvalidation_0-logloss:0.418741\n",
      "[14]\tvalidation_0-logloss:0.414489\n",
      "[15]\tvalidation_0-logloss:0.410087\n",
      "[16]\tvalidation_0-logloss:0.406642\n",
      "[17]\tvalidation_0-logloss:0.4037\n",
      "[18]\tvalidation_0-logloss:0.400618\n",
      "[19]\tvalidation_0-logloss:0.397354\n",
      "[20]\tvalidation_0-logloss:0.39657\n",
      "[21]\tvalidation_0-logloss:0.393926\n",
      "[22]\tvalidation_0-logloss:0.391482\n",
      "[23]\tvalidation_0-logloss:0.390668\n",
      "[24]\tvalidation_0-logloss:0.389957\n",
      "[25]\tvalidation_0-logloss:0.388988\n",
      "[26]\tvalidation_0-logloss:0.387029\n",
      "[27]\tvalidation_0-logloss:0.386733\n",
      "[28]\tvalidation_0-logloss:0.386069\n",
      "[29]\tvalidation_0-logloss:0.385068\n",
      "[30]\tvalidation_0-logloss:0.385632\n",
      "[31]\tvalidation_0-logloss:0.385581\n",
      "[32]\tvalidation_0-logloss:0.385616\n",
      "[33]\tvalidation_0-logloss:0.385791\n",
      "[34]\tvalidation_0-logloss:0.386494\n",
      "[35]\tvalidation_0-logloss:0.386805\n",
      "[36]\tvalidation_0-logloss:0.387213\n",
      "[37]\tvalidation_0-logloss:0.387828\n",
      "[38]\tvalidation_0-logloss:0.388322\n",
      "[39]\tvalidation_0-logloss:0.388876\n",
      "Stopping. Best iteration:\n",
      "[29]\tvalidation_0-logloss:0.385068\n",
      "\n",
      "The accuracy of the XGBoost is 0.8249158249158249\n"
     ]
    }
   ],
   "source": [
    "# fit model on training data\n",
    "model6 = XGBClassifier()\n",
    "eval_set = [(X_test,y_test)]\n",
    "model6.fit(X_train,y_train, early_stopping_rounds=10, eval_metric=\"logloss\",\n",
    "eval_set=eval_set, verbose=True)\n",
    "prediciton6 = model6.predict(X_test)\n",
    "print('The accuracy of the XGBoost is',metrics.accuracy_score(prediciton6,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.640213\n",
      "Will train until validation_0-logloss hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-logloss:0.597763\n",
      "[2]\tvalidation_0-logloss:0.562734\n",
      "[3]\tvalidation_0-logloss:0.534402\n",
      "[4]\tvalidation_0-logloss:0.511728\n",
      "[5]\tvalidation_0-logloss:0.492292\n",
      "[6]\tvalidation_0-logloss:0.476072\n",
      "[7]\tvalidation_0-logloss:0.464249\n",
      "[8]\tvalidation_0-logloss:0.453967\n",
      "[9]\tvalidation_0-logloss:0.443664\n",
      "[10]\tvalidation_0-logloss:0.436056\n",
      "[11]\tvalidation_0-logloss:0.428675\n",
      "[12]\tvalidation_0-logloss:0.423898\n",
      "[13]\tvalidation_0-logloss:0.41931\n",
      "[14]\tvalidation_0-logloss:0.415246\n",
      "[15]\tvalidation_0-logloss:0.409983\n",
      "[16]\tvalidation_0-logloss:0.407494\n",
      "[17]\tvalidation_0-logloss:0.405578\n",
      "[18]\tvalidation_0-logloss:0.402158\n",
      "[19]\tvalidation_0-logloss:0.399584\n",
      "[20]\tvalidation_0-logloss:0.398819\n",
      "[21]\tvalidation_0-logloss:0.397023\n",
      "[22]\tvalidation_0-logloss:0.395548\n",
      "[23]\tvalidation_0-logloss:0.393958\n",
      "[24]\tvalidation_0-logloss:0.393005\n",
      "[25]\tvalidation_0-logloss:0.390919\n",
      "[26]\tvalidation_0-logloss:0.389907\n",
      "[27]\tvalidation_0-logloss:0.389541\n",
      "[28]\tvalidation_0-logloss:0.38848\n",
      "[29]\tvalidation_0-logloss:0.388113\n",
      "[30]\tvalidation_0-logloss:0.388269\n",
      "[31]\tvalidation_0-logloss:0.388961\n",
      "[32]\tvalidation_0-logloss:0.38898\n",
      "[33]\tvalidation_0-logloss:0.389126\n",
      "[34]\tvalidation_0-logloss:0.38953\n",
      "[35]\tvalidation_0-logloss:0.390527\n",
      "[36]\tvalidation_0-logloss:0.39088\n",
      "[37]\tvalidation_0-logloss:0.391058\n",
      "[38]\tvalidation_0-logloss:0.391741\n",
      "[39]\tvalidation_0-logloss:0.391836\n",
      "Stopping. Best iteration:\n",
      "[29]\tvalidation_0-logloss:0.388113\n",
      "\n",
      "The accuracy of the XGBoost is 0.8249158249158249\n"
     ]
    }
   ],
   "source": [
    "# fit model on training data\n",
    "model6b = XGBClassifier()\n",
    "eval_set = [(X_testb,y_testb)]\n",
    "model6b.fit(X_trainb,y_trainb, early_stopping_rounds=10, eval_metric=\"logloss\",\n",
    "eval_set = eval_set, verbose=True)\n",
    "prediciton6b = model6b.predict(X_testb)\n",
    "print('The accuracy of the XGBoost is',metrics.accuracy_score(prediciton6,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear Svm</th>\n",
       "      <td>0.866050</td>\n",
       "      <td>0.040509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Radial Svm</th>\n",
       "      <td>0.855886</td>\n",
       "      <td>0.023824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.859282</td>\n",
       "      <td>0.038899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.850319</td>\n",
       "      <td>0.044860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.819765</td>\n",
       "      <td>0.035121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.836747</td>\n",
       "      <td>0.024006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.862692</td>\n",
       "      <td>0.035303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      CV Mean       Std\n",
       "Linear Svm           0.866050  0.040509\n",
       "Radial Svm           0.855886  0.023824\n",
       "Logistic Regression  0.859282  0.038899\n",
       "KNN                  0.850319  0.044860\n",
       "Decision Tree        0.819765  0.035121\n",
       "Naive Bayes          0.836747  0.024006\n",
       "Random Forest        0.862692  0.035303"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "xyz=[]\n",
    "accuracy=[]\n",
    "std=[]\n",
    "classifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\n",
    "models=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\n",
    "for i in models:\n",
    "    model = i\n",
    "    cv_result = cross_val_score(model,X_train,y_train, cv = kfold,scoring = \"accuracy\")\n",
    "    cv_result=cv_result\n",
    "    xyz.append(cv_result.mean())\n",
    "    std.append(cv_result.std())\n",
    "    accuracy.append(cv_result)\n",
    "new_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \n",
    "new_models_dataframe2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 28 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f,ax=plt.subplots(7,2, figsize=(12,10))\n",
    "\n",
    "y_pred = cross_val_predict(svm.SVC(kernel='rbf'),X_train,y_train,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')\n",
    "ax[0,0].set_title('Matrix for rbf-SVM1')\n",
    "\n",
    "y_predb = cross_val_predict(svm.SVC(kernel='rbf'),X_trainb,y_trainb,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_trainb,y_predb),ax=ax[0,1],annot=True,fmt='2.0f')\n",
    "ax[0,1].set_title('Matrix for rbf-SVM1  - b')\n",
    "\n",
    "\n",
    "y_pred = cross_val_predict(svm.SVC(kernel='linear'),X_train,y_train,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')\n",
    "ax[1,0].set_title('Matrix for Linear-SVM1')\n",
    "\n",
    "y_predb = cross_val_predict(svm.SVC(kernel='linear'),X_trainb,y_trainb,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_trainb,y_predb),ax=ax[1,1],annot=True,fmt='2.0f')\n",
    "ax[1,1].set_title('Matrix for Linear-SVM1  - b')\n",
    "\n",
    "y_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X_train,y_train,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')\n",
    "ax[2,0].set_title('Matrix for KNN1')\n",
    "\n",
    "y_predb = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X_trainb,y_trainb,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_trainb,y_predb),ax=ax[2,1],annot=True,fmt='2.0f')\n",
    "ax[2,1].set_title('Matrix for KNN1  - b')\n",
    "\n",
    "y_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X_train,y_train,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[3,0],annot=True,fmt='2.0f')\n",
    "ax[3,0].set_title('Matrix for Random-Forests1')\n",
    "\n",
    "y_predb = cross_val_predict(RandomForestClassifier(n_estimators=100),X_trainb,y_trainb,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_trainb,y_predb),ax=ax[3,1],annot=True,fmt='2.0f')\n",
    "ax[3,1].set_title('Matrix for Random-Forests1  - b')\n",
    "\n",
    "y_pred = cross_val_predict(LogisticRegression(),X_train,y_train,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[4,0],annot=True,fmt='2.0f')\n",
    "ax[4,0].set_title('Matrix for Logistic Regression1')\n",
    "\n",
    "y_predb = cross_val_predict(LogisticRegression(),X_trainb,y_trainb,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_trainb,y_predb),ax=ax[4,1],annot=True,fmt='2.0f')\n",
    "ax[4,1].set_title('Matrix for Logistic Regression1  - b')\n",
    "\n",
    "y_pred = cross_val_predict(DecisionTreeClassifier(),X_train,y_train,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[5,0],annot=True,fmt='2.0f')\n",
    "ax[5,0].set_title('Matrix for Decision Tree1')\n",
    "\n",
    "y_predb = cross_val_predict(DecisionTreeClassifier(),X_train,y_train,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_trainb,y_predb),ax=ax[5,1],annot=True,fmt='2.0f')\n",
    "ax[5,1].set_title('Matrix for Decision Tree1  - b')\n",
    "\n",
    "y_pred = cross_val_predict(GaussianNB(),X_train,y_train,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[6,0],annot=True,fmt='2.0f')\n",
    "ax[6,0].set_title('Matrix for Naive Bayes1')\n",
    "\n",
    "y_predb = cross_val_predict(GaussianNB(),X_trainb,y_trainb,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_trainb,y_predb),ax=ax[6,1],annot=True,fmt='2.0f')\n",
    "ax[6,1].set_title('Matrix for Naive Bayes1 - b')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.2,wspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(path):\n",
    "    image = face_recognition.load_image_file(path)\n",
    "    pic = cv2.imread(str(path))\n",
    "    \n",
    "    \n",
    "    gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "    gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    faces = detector(gray_image)\n",
    "    landmark_listed = []\n",
    "\n",
    "    faceFound = False\n",
    "    \n",
    "    for face in faces:\n",
    "        \n",
    "        faceFound = True\n",
    "\n",
    "        landmarks = predictor(gray_image, face)\n",
    "        for n in range(0, 68):\n",
    "            x = landmarks.part(n).x\n",
    "            y = landmarks.part(n).y\n",
    "            cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "            landmark_listed.append(x)\n",
    "            landmark_listed.append(y)\n",
    "        break\n",
    "    \n",
    "    if faceFound != True:\n",
    "        print(\"Face found in the picture\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    X = np.asarray(landmark_listed, dtype=np.float32)\n",
    "    \n",
    "    X = scaler.fit_transform(X.reshape(-1,1))\n",
    "    \n",
    "    print(X.shape)\n",
    "        \n",
    "    X_b = np.delete(X.reshape(1,-1), [10,11,22,23,26,27,28,29,44,45,56,57,64,65,68,69,72,73,76,77,86,87,100,101,106,107,110,111,112,113,116,117,130,131,134,135], 1)\n",
    "    print(X_b.shape)\n",
    "    \n",
    "    #prediction\n",
    "    prediction = []\n",
    "    \n",
    "    pred = int(model2.predict(X.reshape(1,-1)))\n",
    "    prediction.append(pred)\n",
    "    \n",
    "    print(pred)\n",
    "    \n",
    "    pred = int(model2b.predict(X_b.reshape(1,-1)))\n",
    "    prediction.append(pred)\n",
    "    \n",
    "    print(pred)\n",
    "    \n",
    "    pred = int(model3.predict(X.reshape(1,-1)))\n",
    "    prediction.append(pred)\n",
    "    \n",
    "    print(pred)\n",
    "\n",
    "    if mode(prediction) == 0:\n",
    "        print(\"happy face\")\n",
    "    else:\n",
    "        print(\"sad face\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136, 1)\n",
      "(1, 100)\n",
      "0\n",
      "0\n",
      "0\n",
      "happy face\n"
     ]
    }
   ],
   "source": [
    "predict('out.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import numpy\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import face_recognition\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics \n",
    "from statistics import mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks():\n",
    "    csvfile = open(\"facial_landmarks.csv\",'w', newline='')\n",
    "    obj = csv.writer(csvfile)\n",
    "    headers=[('x1','y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4', 'x5','y5', 'x6', 'y6', 'x7', 'y7', 'x8', 'y8','x9','y9', 'x10', 'y10', 'x11', 'y11', 'x12', 'y12','x13','y13', 'x14', 'y14', 'x15', 'y15', 'x16', 'y16','x17','y17', 'x18', 'y18', 'x19', 'y19', 'x20', 'y20', 'x21', 'y21', 'x22', 'y22','x23','y23', 'x24', 'y24', 'x25', 'y25', 'x26', 'y26','x27','y27', 'x28', 'y28', 'x29', 'y29', 'x30', 'y30','x31','y31', 'x32', 'y32', 'x33', 'y33', 'x34', 'y34','x35','y35', 'x36', 'y36', 'x37', 'y37', 'x38', 'y38','x39','y39', 'x40', 'y40', 'x41', 'y41', 'x42', 'y42', 'x43', 'y43','x44', 'y44', 'x45', 'y45', 'x46', 'y46', 'x47', 'y47', 'x48', 'y48','x49','y49', 'x50', 'y50', 'x51', 'y51', 'x52', 'y52', 'x53', 'y53','x54','y54', 'x55', 'y55', 'x56', 'y56', 'x57', 'y57','x58','y58', 'x59', 'y59', 'x60', 'y60', 'x61', 'y61','x62','y62', 'x63', 'y63', 'x64', 'y64', 'x65', 'y65','x66','y66', 'x67', 'y67', 'x68', 'y68', 'target' )]\n",
    "    obj.writerows(headers)\n",
    "\n",
    "\n",
    "    pathlist = Path(\"./happy\").glob('**/*.JPG')\n",
    "    countFaces = 0\n",
    "    countImages = 0 \n",
    "    #iterate through all the images in the folder\n",
    "    for path in pathlist:\n",
    "        path_in_str = str(path)\n",
    "        image = face_recognition.load_image_file(path_in_str)\n",
    "        pic = cv2.imread(path_in_str)\n",
    "\n",
    "        gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "        gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        faces = detector(gray_image)\n",
    "        for face in faces:\n",
    "            countFaces += 1\n",
    "\n",
    "            landmarks = predictor(gray_image, face)\n",
    "            landmark_list = []\n",
    "            for n in range(0, 68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "                landmark_list.append(x)\n",
    "                landmark_list.append(y)\n",
    "\n",
    "\n",
    "            landmark_list.append(\"happy\")\n",
    "            obj.writerow(landmark_list)\n",
    "        countImages += 1\n",
    "        \n",
    "    pathlist = Path(\"./happy\").glob('**/*.jpg')\n",
    "\n",
    "    #iterate through all the images in the folder\n",
    "    for path in pathlist:\n",
    "        path_in_str = str(path)\n",
    "        image = face_recognition.load_image_file(path_in_str)\n",
    "        pic = cv2.imread(path_in_str)\n",
    "\n",
    "        gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "        gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        faces = detector(gray_image)\n",
    "        for face in faces:\n",
    "            countFaces += 1\n",
    "            landmarks = predictor(gray_image, face)\n",
    "            landmark_list = []\n",
    "            for n in range(0, 68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "                landmark_list.append(x)\n",
    "                landmark_list.append(y)\n",
    "\n",
    "\n",
    "            landmark_list.append(\"happy\")\n",
    "            obj.writerow(landmark_list)\n",
    "        countImages += 1\n",
    "    pathlist = Path(\"./happy\").glob('**/*.png')\n",
    "\n",
    "\n",
    "    #iterate through all the images in the folder\n",
    "    for path in pathlist:\n",
    "        path_in_str = str(path)\n",
    "        image = face_recognition.load_image_file(path_in_str)\n",
    "        pic = cv2.imread(path_in_str)\n",
    "\n",
    "        gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "        gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        faces = detector(gray_image)\n",
    "        for face in faces:\n",
    "            countFaces += 1\n",
    "\n",
    "            landmarks = predictor(gray_image, face)\n",
    "            landmark_list = []\n",
    "            for n in range(0, 68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "                landmark_list.append(x)\n",
    "                landmark_list.append(y)\n",
    "\n",
    "\n",
    "            landmark_list.append(\"happy\")\n",
    "            obj.writerow(landmark_list)\n",
    "        countImages += 1\n",
    "\n",
    "    pathlist = Path(\"./happy\").glob('**/*.jpeg')\n",
    "    #iterate through all the images in the folder\n",
    "    for path in pathlist:\n",
    "        path_in_str = str(path)\n",
    "        image = face_recognition.load_image_file(path_in_str)\n",
    "        pic = cv2.imread(path_in_str)\n",
    "\n",
    "        gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "        gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        faces = detector(gray_image)\n",
    "        for face in faces:\n",
    "            countFaces += 1\n",
    "\n",
    "            landmarks = predictor(gray_image, face)\n",
    "            landmark_list = []\n",
    "            for n in range(0, 68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "                landmark_list.append(x)\n",
    "                landmark_list.append(y)\n",
    "\n",
    "\n",
    "            landmark_list.append(\"happy\")\n",
    "            obj.writerow(landmark_list)\n",
    "        countImages += 1\n",
    "        \n",
    "    pathlist = Path(\"./sad\").glob('**/*.JPG')\n",
    "    countFacesSad = 0\n",
    "    countImagesSad = 0\n",
    "    #iterate through all the images in the folder\n",
    "    for path in pathlist:\n",
    "        path_in_str = str(path)\n",
    "        image = face_recognition.load_image_file(path_in_str)\n",
    "        pic = cv2.imread(path_in_str)\n",
    "\n",
    "        gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "        gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        faces = detector(gray_image)\n",
    "        for face in faces:\n",
    "            countFacesSad += 1\n",
    "\n",
    "            landmarks = predictor(gray_image, face)\n",
    "            landmark_list = []\n",
    "            for n in range(0, 68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "                landmark_list.append(x)\n",
    "                landmark_list.append(y)\n",
    "\n",
    "\n",
    "            landmark_list.append(\"sad\")\n",
    "            obj.writerow(landmark_list)\n",
    "        countImagesSad += 1\n",
    "    \n",
    "    pathlist = Path(\"./sad\").glob('**/*.jpg')\n",
    "    #iterate through all the images in the folder\n",
    "    for path in pathlist:\n",
    "        path_in_str = str(path)\n",
    "        image = face_recognition.load_image_file(path_in_str)\n",
    "        pic = cv2.imread(path_in_str)\n",
    "\n",
    "        gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "        gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        faces = detector(gray_image)\n",
    "        for face in faces:\n",
    "            countFacesSad += 1\n",
    "\n",
    "            landmarks = predictor(gray_image, face)\n",
    "            landmark_list = []\n",
    "            for n in range(0, 68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "                landmark_list.append(x)\n",
    "                landmark_list.append(y)\n",
    "\n",
    "\n",
    "            landmark_list.append(\"sad\")\n",
    "            obj.writerow(landmark_list)\n",
    "        countImagesSad += 1\n",
    "    \n",
    "    pathlist = Path(\"./sad\").glob('**/*.png')\n",
    "\n",
    "    #iterate through all the images in the folder\n",
    "    for path in pathlist:\n",
    "        path_in_str = str(path)\n",
    "        image = face_recognition.load_image_file(path_in_str)\n",
    "        pic = cv2.imread(path_in_str)\n",
    "\n",
    "        gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "        gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        faces = detector(gray_image)\n",
    "        for face in faces:\n",
    "            countFacesSad += 1\n",
    "\n",
    "            landmarks = predictor(gray_image, face)\n",
    "            landmark_list = []\n",
    "            for n in range(0, 68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "                landmark_list.append(x)\n",
    "                landmark_list.append(y)\n",
    "\n",
    "\n",
    "            landmark_list.append(\"sad\")\n",
    "            obj.writerow(landmark_list)\n",
    "        countImagesSad += 1\n",
    "    \n",
    "    pathlist = Path(\"./sad\").glob('**/*.jpeg')\n",
    "    #iterate through all the images in the folder\n",
    "    for path in pathlist:\n",
    "        path_in_str = str(path)\n",
    "        image = face_recognition.load_image_file(path_in_str)\n",
    "        pic = cv2.imread(path_in_str)\n",
    "\n",
    "        gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "        gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        faces = detector(gray_image)\n",
    "        for face in faces:\n",
    "            countFacesSad += 1\n",
    "\n",
    "            landmarks = predictor(gray_image, face)\n",
    "            landmark_list = []\n",
    "            for n in range(0, 68):\n",
    "                x = landmarks.part(n).x\n",
    "                y = landmarks.part(n).y\n",
    "                cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "                landmark_list.append(x)\n",
    "                landmark_list.append(y)\n",
    "\n",
    "\n",
    "            landmark_list.append(\"sad\")\n",
    "            obj.writerow(landmark_list)\n",
    "        countImagesSad += 1\n",
    "        \n",
    "    csvfile.close()\n",
    "    print(f\"Total happy images: {countImages}, Total happy faces: {countFaces}, Total sad images: {countImagesSad}, Total sad Faces: {countFacesSad}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total happy images: 480, Total happy faces: 804, Total sad images: 508, Total sad Faces: 381\n"
     ]
    }
   ],
   "source": [
    "get_landmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>y5</th>\n",
       "      <th>...</th>\n",
       "      <th>y64</th>\n",
       "      <th>x65</th>\n",
       "      <th>y65</th>\n",
       "      <th>x66</th>\n",
       "      <th>y66</th>\n",
       "      <th>x67</th>\n",
       "      <th>y67</th>\n",
       "      <th>x68</th>\n",
       "      <th>y68</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2198</td>\n",
       "      <td>1666</td>\n",
       "      <td>2187</td>\n",
       "      <td>1727</td>\n",
       "      <td>2183</td>\n",
       "      <td>1791</td>\n",
       "      <td>2188</td>\n",
       "      <td>1857</td>\n",
       "      <td>2199</td>\n",
       "      <td>1926</td>\n",
       "      <td>...</td>\n",
       "      <td>1916</td>\n",
       "      <td>2512</td>\n",
       "      <td>1935</td>\n",
       "      <td>2400</td>\n",
       "      <td>1976</td>\n",
       "      <td>2359</td>\n",
       "      <td>1975</td>\n",
       "      <td>2322</td>\n",
       "      <td>1964</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1321</td>\n",
       "      <td>1650</td>\n",
       "      <td>1295</td>\n",
       "      <td>1724</td>\n",
       "      <td>1274</td>\n",
       "      <td>1797</td>\n",
       "      <td>1260</td>\n",
       "      <td>1874</td>\n",
       "      <td>1262</td>\n",
       "      <td>1951</td>\n",
       "      <td>...</td>\n",
       "      <td>2055</td>\n",
       "      <td>1536</td>\n",
       "      <td>2071</td>\n",
       "      <td>1461</td>\n",
       "      <td>2063</td>\n",
       "      <td>1432</td>\n",
       "      <td>2056</td>\n",
       "      <td>1408</td>\n",
       "      <td>2041</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3179</td>\n",
       "      <td>1353</td>\n",
       "      <td>3182</td>\n",
       "      <td>1411</td>\n",
       "      <td>3188</td>\n",
       "      <td>1468</td>\n",
       "      <td>3198</td>\n",
       "      <td>1525</td>\n",
       "      <td>3219</td>\n",
       "      <td>1575</td>\n",
       "      <td>...</td>\n",
       "      <td>1564</td>\n",
       "      <td>3476</td>\n",
       "      <td>1558</td>\n",
       "      <td>3424</td>\n",
       "      <td>1564</td>\n",
       "      <td>3397</td>\n",
       "      <td>1569</td>\n",
       "      <td>3372</td>\n",
       "      <td>1566</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4294</td>\n",
       "      <td>1458</td>\n",
       "      <td>4288</td>\n",
       "      <td>1516</td>\n",
       "      <td>4290</td>\n",
       "      <td>1578</td>\n",
       "      <td>4306</td>\n",
       "      <td>1636</td>\n",
       "      <td>4330</td>\n",
       "      <td>1689</td>\n",
       "      <td>...</td>\n",
       "      <td>1675</td>\n",
       "      <td>4573</td>\n",
       "      <td>1687</td>\n",
       "      <td>4500</td>\n",
       "      <td>1722</td>\n",
       "      <td>4465</td>\n",
       "      <td>1725</td>\n",
       "      <td>4434</td>\n",
       "      <td>1717</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>266</td>\n",
       "      <td>1344</td>\n",
       "      <td>274</td>\n",
       "      <td>1398</td>\n",
       "      <td>286</td>\n",
       "      <td>1454</td>\n",
       "      <td>301</td>\n",
       "      <td>1509</td>\n",
       "      <td>331</td>\n",
       "      <td>1561</td>\n",
       "      <td>...</td>\n",
       "      <td>1509</td>\n",
       "      <td>622</td>\n",
       "      <td>1514</td>\n",
       "      <td>577</td>\n",
       "      <td>1542</td>\n",
       "      <td>552</td>\n",
       "      <td>1547</td>\n",
       "      <td>522</td>\n",
       "      <td>1546</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     x1    y1    x2    y2    x3    y3    x4    y4    x5    y5  ...   y64  \\\n",
       "0  2198  1666  2187  1727  2183  1791  2188  1857  2199  1926  ...  1916   \n",
       "1  1321  1650  1295  1724  1274  1797  1260  1874  1262  1951  ...  2055   \n",
       "2  3179  1353  3182  1411  3188  1468  3198  1525  3219  1575  ...  1564   \n",
       "3  4294  1458  4288  1516  4290  1578  4306  1636  4330  1689  ...  1675   \n",
       "4   266  1344   274  1398   286  1454   301  1509   331  1561  ...  1509   \n",
       "\n",
       "    x65   y65   x66   y66   x67   y67   x68   y68  target  \n",
       "0  2512  1935  2400  1976  2359  1975  2322  1964   happy  \n",
       "1  1536  2071  1461  2063  1432  2056  1408  2041   happy  \n",
       "2  3476  1558  3424  1564  3397  1569  3372  1566   happy  \n",
       "3  4573  1687  4500  1722  4465  1725  4434  1717   happy  \n",
       "4   622  1514   577  1542   552  1547   522  1546   happy  \n",
       "\n",
       "[5 rows x 137 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('facial_landmarks.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "encoder = LabelEncoder()\n",
    "df['target'] = encoder.fit_transform(df['target'].values.reshape(-1,1))\n",
    "y = df['target']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['x1', 'y1','x2', 'y2', 'x3', 'y3', 'x4', 'y4', 'x5','y5', 'x6', 'y6', 'x7', 'y7', 'x8', 'y8','x9','y9', 'x10', 'y10', 'x11', 'y11', 'x12', 'y12','x13','y13', 'x14', 'y14', 'x15', 'y15', 'x16', 'y16','x17','y17', 'x18', 'y18', 'x19', 'y19', 'x20', 'y20', 'x21', 'y21', 'x22', 'y22','x23','y23', 'x24', 'y24', 'x25', 'y25', 'x26', 'y26','x27','y27', 'x28', 'y28', 'x29', 'y29', 'x30', 'y30','x31','y31', 'x32', 'y32', 'x33', 'y33', 'x34', 'y34','x35','y35', 'x36', 'y36', 'x37', 'y37', 'x38', 'y38','x39','y39', 'x40', 'y40', 'x41', 'y41', 'x42', 'y42', 'x43', 'y43','x44', 'y44', 'x45', 'y45', 'x46', 'y46', 'x47', 'y47', 'x48', 'y48','x49','y49', 'x50', 'y50', 'x51', 'y51', 'x52', 'y52', 'x53', 'y53','x54','y54', 'x55', 'y55', 'x56', 'y56', 'x57', 'y57','x58','y58', 'x59', 'y59', 'x60', 'y60', 'x61', 'y61','x62','y62', 'x63', 'y63', 'x64', 'y64', 'x65', 'y65','x66','y66', 'x67', 'y67', 'x68', 'y68']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00947685 0.00392099 0.00504184 0.00242473 0.00659777 0.00342529\n",
      " 0.04439001 0.00404121 0.00654222 0.00383551 0.00554181 0.00547109\n",
      " 0.00467005 0.0041119  0.00412045 0.00227458 0.00504994 0.00313229\n",
      " 0.00476836 0.00328798 0.0033182  0.00212358 0.00367299 0.00287957\n",
      " 0.00620953 0.00352168 0.01252129 0.0034223  0.00350899 0.00286594\n",
      " 0.03292186 0.00394411 0.00499489 0.00386531 0.02201138 0.00707895\n",
      " 0.00399175 0.00460549 0.00638744 0.01457452 0.03921778 0.00442103\n",
      " 0.01334864 0.00400961 0.01628498 0.00426243 0.00293423 0.00443447\n",
      " 0.00217187 0.00467634 0.00542445 0.00463182 0.0034564  0.00257881\n",
      " 0.0036301  0.00205581 0.00479169 0.00227316 0.02614124 0.00438824\n",
      " 0.00266935 0.00435701 0.00573587 0.00300849 0.00446409 0.00325368\n",
      " 0.00469752 0.01262165 0.0045345  0.00420131 0.00202569 0.01504017\n",
      " 0.00539205 0.02933881 0.00685917 0.00262875 0.00406963 0.00602945\n",
      " 0.00313122 0.00243887 0.00572101 0.00323276 0.00388662 0.00404109\n",
      " 0.00572596 0.0035965  0.00540572 0.0192907  0.00508282 0.00567885\n",
      " 0.01024749 0.00538257 0.00601884 0.00522348 0.00347206 0.00415164\n",
      " 0.01147901 0.00229272 0.00554362 0.00430966 0.00214369 0.0314033\n",
      " 0.04537877 0.00232892 0.02136063 0.00370438 0.00584831 0.0043246\n",
      " 0.00314747 0.00342001 0.00227744 0.00469575 0.00360986 0.00539124\n",
      " 0.00275592 0.00234515 0.00433387 0.00281232 0.007206   0.00175539\n",
      " 0.00357726 0.00571579 0.00256112 0.01668287 0.00317787 0.0028423\n",
      " 0.00522846 0.00308104 0.00586701 0.00313115 0.01477371 0.00315241\n",
      " 0.04465353 0.00465366 0.03535429 0.00334634]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102    0.045379\n",
      "132    0.044654\n",
      "6      0.044390\n",
      "40     0.039218\n",
      "134    0.035354\n",
      "30     0.032922\n",
      "101    0.031403\n",
      "73     0.029339\n",
      "58     0.026141\n",
      "34     0.022011\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119    0.001755\n",
      "70     0.002026\n",
      "55     0.002056\n",
      "21     0.002124\n",
      "100    0.002144\n",
      "48     0.002172\n",
      "57     0.002273\n",
      "15     0.002275\n",
      "110    0.002277\n",
      "97     0.002293\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(df[features], df['target'])\n",
    "print(model.feature_importances_) \n",
    "#use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()\n",
    "print(feat_importances.nlargest(10))\n",
    "\n",
    "\n",
    "feat_importances = pd.Series(model.feature_importances_)\n",
    "feat_importances.nsmallest(10).plot(kind='barh')\n",
    "plt.show()\n",
    "print(feat_importances.nsmallest(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = ['x6','y6','x12','y12','x14','y14','x15', 'y15','x23','y23','x29','y29', 'x33','y33','x35','y35','x37','y37','x39','y39','x44','y44', 'x51','y51', 'x54','y54', 'x56','y56','x57','y57','x59','y59','x66','y66', 'x68','y68']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "for columns in df:\n",
    "    df[columns] = scaler.fit_transform(np.array(df[columns]).reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_streamed_ft = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_streamed_ft = df_streamed_ft.drop(drop, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>y5</th>\n",
       "      <th>...</th>\n",
       "      <th>y64</th>\n",
       "      <th>x65</th>\n",
       "      <th>y65</th>\n",
       "      <th>x66</th>\n",
       "      <th>y66</th>\n",
       "      <th>x67</th>\n",
       "      <th>y67</th>\n",
       "      <th>x68</th>\n",
       "      <th>y68</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.361444</td>\n",
       "      <td>1.510731</td>\n",
       "      <td>1.347821</td>\n",
       "      <td>1.526705</td>\n",
       "      <td>1.338147</td>\n",
       "      <td>1.543050</td>\n",
       "      <td>1.331516</td>\n",
       "      <td>1.559810</td>\n",
       "      <td>1.321326</td>\n",
       "      <td>1.585019</td>\n",
       "      <td>...</td>\n",
       "      <td>1.645717</td>\n",
       "      <td>1.414456</td>\n",
       "      <td>1.668856</td>\n",
       "      <td>1.349429</td>\n",
       "      <td>1.694612</td>\n",
       "      <td>1.330916</td>\n",
       "      <td>1.686256</td>\n",
       "      <td>1.316106</td>\n",
       "      <td>1.672984</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.336498</td>\n",
       "      <td>1.483107</td>\n",
       "      <td>0.304608</td>\n",
       "      <td>1.521599</td>\n",
       "      <td>0.274866</td>\n",
       "      <td>1.553093</td>\n",
       "      <td>0.247679</td>\n",
       "      <td>1.587744</td>\n",
       "      <td>0.230476</td>\n",
       "      <td>1.625352</td>\n",
       "      <td>...</td>\n",
       "      <td>1.874539</td>\n",
       "      <td>0.329436</td>\n",
       "      <td>1.892320</td>\n",
       "      <td>0.291346</td>\n",
       "      <td>1.836094</td>\n",
       "      <td>0.280228</td>\n",
       "      <td>1.817780</td>\n",
       "      <td>0.274470</td>\n",
       "      <td>1.798140</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.507934</td>\n",
       "      <td>0.970321</td>\n",
       "      <td>2.511495</td>\n",
       "      <td>0.988845</td>\n",
       "      <td>2.513722</td>\n",
       "      <td>1.002400</td>\n",
       "      <td>2.511124</td>\n",
       "      <td>1.014280</td>\n",
       "      <td>2.508804</td>\n",
       "      <td>1.018743</td>\n",
       "      <td>...</td>\n",
       "      <td>1.066253</td>\n",
       "      <td>2.486136</td>\n",
       "      <td>1.049401</td>\n",
       "      <td>2.503292</td>\n",
       "      <td>1.024606</td>\n",
       "      <td>2.507415</td>\n",
       "      <td>1.027013</td>\n",
       "      <td>2.512734</td>\n",
       "      <td>1.026070</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.811030</td>\n",
       "      <td>1.151609</td>\n",
       "      <td>3.804986</td>\n",
       "      <td>1.167564</td>\n",
       "      <td>3.802761</td>\n",
       "      <td>1.186522</td>\n",
       "      <td>3.805188</td>\n",
       "      <td>1.196671</td>\n",
       "      <td>3.802224</td>\n",
       "      <td>1.202661</td>\n",
       "      <td>...</td>\n",
       "      <td>1.248982</td>\n",
       "      <td>3.705673</td>\n",
       "      <td>1.261363</td>\n",
       "      <td>3.715750</td>\n",
       "      <td>1.281550</td>\n",
       "      <td>3.717917</td>\n",
       "      <td>1.280318</td>\n",
       "      <td>3.723037</td>\n",
       "      <td>1.271507</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.896476</td>\n",
       "      <td>0.954782</td>\n",
       "      <td>-0.889474</td>\n",
       "      <td>0.966718</td>\n",
       "      <td>-0.880824</td>\n",
       "      <td>0.978966</td>\n",
       "      <td>-0.872365</td>\n",
       "      <td>0.987989</td>\n",
       "      <td>-0.853389</td>\n",
       "      <td>0.996156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975712</td>\n",
       "      <td>-0.686659</td>\n",
       "      <td>0.977104</td>\n",
       "      <td>-0.704762</td>\n",
       "      <td>0.988829</td>\n",
       "      <td>-0.717190</td>\n",
       "      <td>0.991290</td>\n",
       "      <td>-0.735256</td>\n",
       "      <td>0.993562</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.057353</td>\n",
       "      <td>1.194772</td>\n",
       "      <td>-0.054436</td>\n",
       "      <td>1.186287</td>\n",
       "      <td>-0.047979</td>\n",
       "      <td>1.173131</td>\n",
       "      <td>-0.033792</td>\n",
       "      <td>1.157235</td>\n",
       "      <td>-0.018661</td>\n",
       "      <td>1.141355</td>\n",
       "      <td>...</td>\n",
       "      <td>1.137040</td>\n",
       "      <td>-0.006298</td>\n",
       "      <td>1.095409</td>\n",
       "      <td>-0.039939</td>\n",
       "      <td>1.136816</td>\n",
       "      <td>-0.048467</td>\n",
       "      <td>1.147170</td>\n",
       "      <td>-0.058307</td>\n",
       "      <td>1.154478</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.920080</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>1.918548</td>\n",
       "      <td>0.011846</td>\n",
       "      <td>1.917162</td>\n",
       "      <td>0.008139</td>\n",
       "      <td>1.918984</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>1.909244</td>\n",
       "      <td>-0.000878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035058</td>\n",
       "      <td>1.811334</td>\n",
       "      <td>-0.031770</td>\n",
       "      <td>1.827201</td>\n",
       "      <td>-0.042200</td>\n",
       "      <td>1.838692</td>\n",
       "      <td>-0.039792</td>\n",
       "      <td>1.850600</td>\n",
       "      <td>-0.033697</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.354070</td>\n",
       "      <td>0.188193</td>\n",
       "      <td>3.351212</td>\n",
       "      <td>0.176948</td>\n",
       "      <td>3.346567</td>\n",
       "      <td>0.168828</td>\n",
       "      <td>3.339185</td>\n",
       "      <td>0.164764</td>\n",
       "      <td>3.326068</td>\n",
       "      <td>0.162068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164133</td>\n",
       "      <td>3.194290</td>\n",
       "      <td>0.158832</td>\n",
       "      <td>3.218822</td>\n",
       "      <td>0.183846</td>\n",
       "      <td>3.233943</td>\n",
       "      <td>0.182662</td>\n",
       "      <td>3.250084</td>\n",
       "      <td>0.182482</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.141499</td>\n",
       "      <td>0.162294</td>\n",
       "      <td>-0.139811</td>\n",
       "      <td>0.165034</td>\n",
       "      <td>-0.140387</td>\n",
       "      <td>0.168828</td>\n",
       "      <td>-0.140074</td>\n",
       "      <td>0.171337</td>\n",
       "      <td>-0.132753</td>\n",
       "      <td>0.170134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201996</td>\n",
       "      <td>-0.095234</td>\n",
       "      <td>0.186765</td>\n",
       "      <td>-0.089519</td>\n",
       "      <td>0.188725</td>\n",
       "      <td>-0.089270</td>\n",
       "      <td>0.189157</td>\n",
       "      <td>-0.091356</td>\n",
       "      <td>0.188984</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.175621</td>\n",
       "      <td>0.212364</td>\n",
       "      <td>1.179410</td>\n",
       "      <td>0.192267</td>\n",
       "      <td>1.181404</td>\n",
       "      <td>0.168828</td>\n",
       "      <td>1.179685</td>\n",
       "      <td>0.145046</td>\n",
       "      <td>1.175802</td>\n",
       "      <td>0.120121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172364</td>\n",
       "      <td>1.055377</td>\n",
       "      <td>0.155546</td>\n",
       "      <td>1.097022</td>\n",
       "      <td>0.136686</td>\n",
       "      <td>1.115564</td>\n",
       "      <td>0.135573</td>\n",
       "      <td>1.131483</td>\n",
       "      <td>0.138596</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.414800</td>\n",
       "      <td>1.350162</td>\n",
       "      <td>0.415712</td>\n",
       "      <td>1.347986</td>\n",
       "      <td>0.421082</td>\n",
       "      <td>1.343863</td>\n",
       "      <td>0.422868</td>\n",
       "      <td>1.337983</td>\n",
       "      <td>0.424896</td>\n",
       "      <td>1.330114</td>\n",
       "      <td>...</td>\n",
       "      <td>1.334584</td>\n",
       "      <td>0.459505</td>\n",
       "      <td>1.323802</td>\n",
       "      <td>0.446847</td>\n",
       "      <td>1.348225</td>\n",
       "      <td>0.440041</td>\n",
       "      <td>1.351763</td>\n",
       "      <td>0.432881</td>\n",
       "      <td>1.349527</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.188247</td>\n",
       "      <td>0.562854</td>\n",
       "      <td>-0.184252</td>\n",
       "      <td>0.571833</td>\n",
       "      <td>-0.181327</td>\n",
       "      <td>0.580592</td>\n",
       "      <td>-0.175111</td>\n",
       "      <td>0.585414</td>\n",
       "      <td>-0.163022</td>\n",
       "      <td>0.583145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.585561</td>\n",
       "      <td>-0.096346</td>\n",
       "      <td>0.581113</td>\n",
       "      <td>-0.081631</td>\n",
       "      <td>0.559505</td>\n",
       "      <td>-0.080203</td>\n",
       "      <td>0.557748</td>\n",
       "      <td>-0.082239</td>\n",
       "      <td>0.561203</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.684769</td>\n",
       "      <td>0.640549</td>\n",
       "      <td>0.689380</td>\n",
       "      <td>0.650129</td>\n",
       "      <td>0.690119</td>\n",
       "      <td>0.657589</td>\n",
       "      <td>0.687988</td>\n",
       "      <td>0.664286</td>\n",
       "      <td>0.689169</td>\n",
       "      <td>0.671878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.682687</td>\n",
       "      <td>0.712973</td>\n",
       "      <td>0.692845</td>\n",
       "      <td>0.734186</td>\n",
       "      <td>0.687977</td>\n",
       "      <td>0.738133</td>\n",
       "      <td>0.687649</td>\n",
       "      <td>0.738306</td>\n",
       "      <td>0.684734</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.530905</td>\n",
       "      <td>0.726877</td>\n",
       "      <td>1.539623</td>\n",
       "      <td>0.723319</td>\n",
       "      <td>1.546359</td>\n",
       "      <td>0.717847</td>\n",
       "      <td>1.551087</td>\n",
       "      <td>0.711938</td>\n",
       "      <td>1.554165</td>\n",
       "      <td>0.704144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.662933</td>\n",
       "      <td>1.503392</td>\n",
       "      <td>0.655053</td>\n",
       "      <td>1.522959</td>\n",
       "      <td>0.626180</td>\n",
       "      <td>1.534933</td>\n",
       "      <td>0.627570</td>\n",
       "      <td>1.544035</td>\n",
       "      <td>0.634346</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.780776</td>\n",
       "      <td>0.937516</td>\n",
       "      <td>-0.777200</td>\n",
       "      <td>0.951399</td>\n",
       "      <td>-0.775548</td>\n",
       "      <td>0.962227</td>\n",
       "      <td>-0.764915</td>\n",
       "      <td>0.971558</td>\n",
       "      <td>-0.742790</td>\n",
       "      <td>0.970343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.942788</td>\n",
       "      <td>-0.608840</td>\n",
       "      <td>0.952457</td>\n",
       "      <td>-0.606729</td>\n",
       "      <td>0.941668</td>\n",
       "      <td>-0.611781</td>\n",
       "      <td>0.936083</td>\n",
       "      <td>-0.617873</td>\n",
       "      <td>0.935047</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.575719</td>\n",
       "      <td>0.859821</td>\n",
       "      <td>2.568802</td>\n",
       "      <td>0.850976</td>\n",
       "      <td>2.560511</td>\n",
       "      <td>0.841711</td>\n",
       "      <td>2.546162</td>\n",
       "      <td>0.831889</td>\n",
       "      <td>2.525103</td>\n",
       "      <td>0.829984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.883524</td>\n",
       "      <td>2.416099</td>\n",
       "      <td>0.901521</td>\n",
       "      <td>2.444698</td>\n",
       "      <td>0.879872</td>\n",
       "      <td>2.457544</td>\n",
       "      <td>0.876004</td>\n",
       "      <td>2.469427</td>\n",
       "      <td>0.873282</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.332992</td>\n",
       "      <td>0.492066</td>\n",
       "      <td>0.332676</td>\n",
       "      <td>0.502047</td>\n",
       "      <td>0.328673</td>\n",
       "      <td>0.506943</td>\n",
       "      <td>0.317754</td>\n",
       "      <td>0.514758</td>\n",
       "      <td>0.310805</td>\n",
       "      <td>0.521839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524652</td>\n",
       "      <td>0.359452</td>\n",
       "      <td>0.540035</td>\n",
       "      <td>0.356702</td>\n",
       "      <td>0.526980</td>\n",
       "      <td>0.355034</td>\n",
       "      <td>0.522026</td>\n",
       "      <td>0.354245</td>\n",
       "      <td>0.518942</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.064595</td>\n",
       "      <td>1.146429</td>\n",
       "      <td>1.054271</td>\n",
       "      <td>1.131820</td>\n",
       "      <td>1.044546</td>\n",
       "      <td>1.114547</td>\n",
       "      <td>1.043038</td>\n",
       "      <td>1.098081</td>\n",
       "      <td>1.036098</td>\n",
       "      <td>1.086502</td>\n",
       "      <td>...</td>\n",
       "      <td>1.137040</td>\n",
       "      <td>1.004239</td>\n",
       "      <td>1.139773</td>\n",
       "      <td>0.982086</td>\n",
       "      <td>1.154704</td>\n",
       "      <td>0.979553</td>\n",
       "      <td>1.150418</td>\n",
       "      <td>0.982190</td>\n",
       "      <td>1.144725</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.603767</td>\n",
       "      <td>0.545589</td>\n",
       "      <td>2.601548</td>\n",
       "      <td>0.556514</td>\n",
       "      <td>2.596773</td>\n",
       "      <td>0.565528</td>\n",
       "      <td>2.590543</td>\n",
       "      <td>0.567339</td>\n",
       "      <td>2.590298</td>\n",
       "      <td>0.567012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534529</td>\n",
       "      <td>2.529493</td>\n",
       "      <td>0.553180</td>\n",
       "      <td>2.567521</td>\n",
       "      <td>0.559505</td>\n",
       "      <td>2.581088</td>\n",
       "      <td>0.556125</td>\n",
       "      <td>2.591369</td>\n",
       "      <td>0.551450</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.795795</td>\n",
       "      <td>0.243442</td>\n",
       "      <td>0.806332</td>\n",
       "      <td>0.313115</td>\n",
       "      <td>0.814110</td>\n",
       "      <td>0.379731</td>\n",
       "      <td>0.817628</td>\n",
       "      <td>0.445745</td>\n",
       "      <td>0.828872</td>\n",
       "      <td>0.504092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409417</td>\n",
       "      <td>1.026473</td>\n",
       "      <td>0.431589</td>\n",
       "      <td>1.002369</td>\n",
       "      <td>0.463557</td>\n",
       "      <td>0.985220</td>\n",
       "      <td>0.470066</td>\n",
       "      <td>0.969654</td>\n",
       "      <td>0.466929</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.169374</td>\n",
       "      <td>0.274520</td>\n",
       "      <td>0.164265</td>\n",
       "      <td>0.301201</td>\n",
       "      <td>0.157893</td>\n",
       "      <td>0.326168</td>\n",
       "      <td>0.150741</td>\n",
       "      <td>0.353728</td>\n",
       "      <td>0.148982</td>\n",
       "      <td>0.378253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411064</td>\n",
       "      <td>0.242723</td>\n",
       "      <td>0.438162</td>\n",
       "      <td>0.237259</td>\n",
       "      <td>0.455426</td>\n",
       "      <td>0.230357</td>\n",
       "      <td>0.450581</td>\n",
       "      <td>0.224326</td>\n",
       "      <td>0.442548</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.215357</td>\n",
       "      <td>0.659541</td>\n",
       "      <td>1.208648</td>\n",
       "      <td>0.668852</td>\n",
       "      <td>1.202459</td>\n",
       "      <td>0.677675</td>\n",
       "      <td>1.192533</td>\n",
       "      <td>0.684004</td>\n",
       "      <td>1.179294</td>\n",
       "      <td>0.691238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.720550</td>\n",
       "      <td>1.156542</td>\n",
       "      <td>0.745425</td>\n",
       "      <td>1.171392</td>\n",
       "      <td>0.705865</td>\n",
       "      <td>1.175636</td>\n",
       "      <td>0.699015</td>\n",
       "      <td>1.180488</td>\n",
       "      <td>0.694486</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.203843</td>\n",
       "      <td>2.403359</td>\n",
       "      <td>-1.206415</td>\n",
       "      <td>2.370941</td>\n",
       "      <td>-1.208347</td>\n",
       "      <td>2.333102</td>\n",
       "      <td>-1.207560</td>\n",
       "      <td>2.292661</td>\n",
       "      <td>-1.204975</td>\n",
       "      <td>2.254549</td>\n",
       "      <td>...</td>\n",
       "      <td>2.322307</td>\n",
       "      <td>-1.158020</td>\n",
       "      <td>2.321174</td>\n",
       "      <td>-1.159997</td>\n",
       "      <td>2.271923</td>\n",
       "      <td>-1.160360</td>\n",
       "      <td>2.267559</td>\n",
       "      <td>-1.159204</td>\n",
       "      <td>2.267884</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.738529</td>\n",
       "      <td>2.356742</td>\n",
       "      <td>0.744348</td>\n",
       "      <td>2.307964</td>\n",
       "      <td>0.747435</td>\n",
       "      <td>2.252758</td>\n",
       "      <td>0.746384</td>\n",
       "      <td>2.195715</td>\n",
       "      <td>0.741557</td>\n",
       "      <td>2.138389</td>\n",
       "      <td>...</td>\n",
       "      <td>2.152748</td>\n",
       "      <td>0.664058</td>\n",
       "      <td>2.148647</td>\n",
       "      <td>0.691367</td>\n",
       "      <td>2.123936</td>\n",
       "      <td>0.699596</td>\n",
       "      <td>2.121421</td>\n",
       "      <td>0.707535</td>\n",
       "      <td>2.126473</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.335732</td>\n",
       "      <td>2.411991</td>\n",
       "      <td>1.336126</td>\n",
       "      <td>2.367537</td>\n",
       "      <td>1.333468</td>\n",
       "      <td>2.321385</td>\n",
       "      <td>1.326844</td>\n",
       "      <td>2.269657</td>\n",
       "      <td>1.315505</td>\n",
       "      <td>2.222282</td>\n",
       "      <td>...</td>\n",
       "      <td>2.261397</td>\n",
       "      <td>1.239919</td>\n",
       "      <td>2.253806</td>\n",
       "      <td>1.257030</td>\n",
       "      <td>2.245903</td>\n",
       "      <td>1.262910</td>\n",
       "      <td>2.243203</td>\n",
       "      <td>1.269380</td>\n",
       "      <td>2.245128</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.075878</td>\n",
       "      <td>2.028697</td>\n",
       "      <td>0.074212</td>\n",
       "      <td>1.986269</td>\n",
       "      <td>0.070164</td>\n",
       "      <td>1.939750</td>\n",
       "      <td>0.063146</td>\n",
       "      <td>1.893373</td>\n",
       "      <td>0.054683</td>\n",
       "      <td>1.846378</td>\n",
       "      <td>...</td>\n",
       "      <td>1.889355</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>1.887391</td>\n",
       "      <td>0.034431</td>\n",
       "      <td>1.863740</td>\n",
       "      <td>0.035407</td>\n",
       "      <td>1.858373</td>\n",
       "      <td>0.038563</td>\n",
       "      <td>1.858280</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.682605</td>\n",
       "      <td>2.306672</td>\n",
       "      <td>-0.690655</td>\n",
       "      <td>2.260305</td>\n",
       "      <td>-0.698346</td>\n",
       "      <td>2.209238</td>\n",
       "      <td>-0.705351</td>\n",
       "      <td>2.157922</td>\n",
       "      <td>-0.711357</td>\n",
       "      <td>2.109349</td>\n",
       "      <td>...</td>\n",
       "      <td>2.223535</td>\n",
       "      <td>-0.720010</td>\n",
       "      <td>2.222587</td>\n",
       "      <td>-0.711523</td>\n",
       "      <td>2.177602</td>\n",
       "      <td>-0.708122</td>\n",
       "      <td>2.171758</td>\n",
       "      <td>-0.707905</td>\n",
       "      <td>2.171985</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.180699</td>\n",
       "      <td>2.056322</td>\n",
       "      <td>2.181690</td>\n",
       "      <td>2.018609</td>\n",
       "      <td>2.180350</td>\n",
       "      <td>1.973227</td>\n",
       "      <td>2.173592</td>\n",
       "      <td>1.926236</td>\n",
       "      <td>2.160710</td>\n",
       "      <td>1.878644</td>\n",
       "      <td>...</td>\n",
       "      <td>1.904171</td>\n",
       "      <td>2.029227</td>\n",
       "      <td>1.905465</td>\n",
       "      <td>2.068340</td>\n",
       "      <td>1.863740</td>\n",
       "      <td>2.085779</td>\n",
       "      <td>1.859997</td>\n",
       "      <td>2.103601</td>\n",
       "      <td>1.863157</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.237966</td>\n",
       "      <td>0.923704</td>\n",
       "      <td>2.240166</td>\n",
       "      <td>0.947995</td>\n",
       "      <td>2.241176</td>\n",
       "      <td>0.968923</td>\n",
       "      <td>2.241332</td>\n",
       "      <td>0.986346</td>\n",
       "      <td>2.244532</td>\n",
       "      <td>0.991316</td>\n",
       "      <td>...</td>\n",
       "      <td>1.018513</td>\n",
       "      <td>2.271578</td>\n",
       "      <td>0.996822</td>\n",
       "      <td>2.248631</td>\n",
       "      <td>1.034363</td>\n",
       "      <td>2.239926</td>\n",
       "      <td>1.033508</td>\n",
       "      <td>2.231241</td>\n",
       "      <td>1.024445</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.843712</td>\n",
       "      <td>0.355668</td>\n",
       "      <td>0.846096</td>\n",
       "      <td>0.415241</td>\n",
       "      <td>0.849202</td>\n",
       "      <td>0.473466</td>\n",
       "      <td>0.852666</td>\n",
       "      <td>0.524617</td>\n",
       "      <td>0.864962</td>\n",
       "      <td>0.571852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575684</td>\n",
       "      <td>1.014244</td>\n",
       "      <td>0.602473</td>\n",
       "      <td>0.996735</td>\n",
       "      <td>0.595282</td>\n",
       "      <td>0.982953</td>\n",
       "      <td>0.591847</td>\n",
       "      <td>0.966235</td>\n",
       "      <td>0.582333</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>-1.212024</td>\n",
       "      <td>-1.251752</td>\n",
       "      <td>-1.213432</td>\n",
       "      <td>-1.268125</td>\n",
       "      <td>-1.216535</td>\n",
       "      <td>-1.279044</td>\n",
       "      <td>-1.220407</td>\n",
       "      <td>-1.287793</td>\n",
       "      <td>-1.224767</td>\n",
       "      <td>-1.293151</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.281235</td>\n",
       "      <td>-1.256962</td>\n",
       "      <td>-1.277252</td>\n",
       "      <td>-1.249016</td>\n",
       "      <td>-1.287890</td>\n",
       "      <td>-1.245367</td>\n",
       "      <td>-1.288457</td>\n",
       "      <td>-1.241259</td>\n",
       "      <td>-1.288514</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>-1.108010</td>\n",
       "      <td>-1.186143</td>\n",
       "      <td>-1.111683</td>\n",
       "      <td>-1.206850</td>\n",
       "      <td>-1.117108</td>\n",
       "      <td>-1.223807</td>\n",
       "      <td>-1.124637</td>\n",
       "      <td>-1.238498</td>\n",
       "      <td>-1.135124</td>\n",
       "      <td>-1.251204</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.236787</td>\n",
       "      <td>-1.213605</td>\n",
       "      <td>-1.236175</td>\n",
       "      <td>-1.199436</td>\n",
       "      <td>-1.252113</td>\n",
       "      <td>-1.194363</td>\n",
       "      <td>-1.254358</td>\n",
       "      <td>-1.188835</td>\n",
       "      <td>-1.252755</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>-1.152421</td>\n",
       "      <td>-1.039386</td>\n",
       "      <td>-1.151447</td>\n",
       "      <td>-1.031535</td>\n",
       "      <td>-1.151031</td>\n",
       "      <td>-1.022946</td>\n",
       "      <td>-1.149164</td>\n",
       "      <td>-1.013384</td>\n",
       "      <td>-1.147930</td>\n",
       "      <td>-1.004366</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.014550</td>\n",
       "      <td>-1.132451</td>\n",
       "      <td>-0.999566</td>\n",
       "      <td>-1.136334</td>\n",
       "      <td>-0.980533</td>\n",
       "      <td>-1.143359</td>\n",
       "      <td>-0.979944</td>\n",
       "      <td>-1.148947</td>\n",
       "      <td>-0.982937</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>-1.175795</td>\n",
       "      <td>-1.053198</td>\n",
       "      <td>-1.179516</td>\n",
       "      <td>-1.051960</td>\n",
       "      <td>-1.181443</td>\n",
       "      <td>-1.048054</td>\n",
       "      <td>-1.184201</td>\n",
       "      <td>-1.041318</td>\n",
       "      <td>-1.189841</td>\n",
       "      <td>-1.026952</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.001380</td>\n",
       "      <td>-1.185813</td>\n",
       "      <td>-0.989707</td>\n",
       "      <td>-1.190421</td>\n",
       "      <td>-1.000048</td>\n",
       "      <td>-1.194363</td>\n",
       "      <td>-1.002677</td>\n",
       "      <td>-1.195673</td>\n",
       "      <td>-1.004068</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>-0.613652</td>\n",
       "      <td>-0.904715</td>\n",
       "      <td>-0.627501</td>\n",
       "      <td>-0.832391</td>\n",
       "      <td>-0.638690</td>\n",
       "      <td>-0.755132</td>\n",
       "      <td>-0.638779</td>\n",
       "      <td>-0.681465</td>\n",
       "      <td>-0.625207</td>\n",
       "      <td>-0.618781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.514103</td>\n",
       "      <td>-0.440973</td>\n",
       "      <td>-0.509917</td>\n",
       "      <td>-0.444467</td>\n",
       "      <td>-0.526816</td>\n",
       "      <td>-0.447434</td>\n",
       "      <td>-0.530165</td>\n",
       "      <td>-0.450345</td>\n",
       "      <td>-0.535949</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>-1.022696</td>\n",
       "      <td>-1.051472</td>\n",
       "      <td>-1.022800</td>\n",
       "      <td>-1.072385</td>\n",
       "      <td>-1.024700</td>\n",
       "      <td>-1.091574</td>\n",
       "      <td>-1.031203</td>\n",
       "      <td>-1.107045</td>\n",
       "      <td>-1.041988</td>\n",
       "      <td>-1.118912</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.119907</td>\n",
       "      <td>-1.105770</td>\n",
       "      <td>-1.116227</td>\n",
       "      <td>-1.092388</td>\n",
       "      <td>-1.133399</td>\n",
       "      <td>-1.086687</td>\n",
       "      <td>-1.134201</td>\n",
       "      <td>-1.080569</td>\n",
       "      <td>-1.134100</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>-0.799475</td>\n",
       "      <td>-1.048018</td>\n",
       "      <td>-0.800590</td>\n",
       "      <td>-1.070683</td>\n",
       "      <td>-0.804791</td>\n",
       "      <td>-1.091574</td>\n",
       "      <td>-0.810464</td>\n",
       "      <td>-1.108688</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>-1.122138</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.124845</td>\n",
       "      <td>-0.901217</td>\n",
       "      <td>-1.119513</td>\n",
       "      <td>-0.882800</td>\n",
       "      <td>-1.138277</td>\n",
       "      <td>-0.874736</td>\n",
       "      <td>-1.140696</td>\n",
       "      <td>-0.866315</td>\n",
       "      <td>-1.140602</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>-1.172289</td>\n",
       "      <td>-1.046292</td>\n",
       "      <td>-1.174837</td>\n",
       "      <td>-1.040045</td>\n",
       "      <td>-1.176764</td>\n",
       "      <td>-1.031315</td>\n",
       "      <td>-1.178362</td>\n",
       "      <td>-1.019957</td>\n",
       "      <td>-1.184020</td>\n",
       "      <td>-1.002752</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.965163</td>\n",
       "      <td>-1.172472</td>\n",
       "      <td>-0.960131</td>\n",
       "      <td>-1.172392</td>\n",
       "      <td>-0.967523</td>\n",
       "      <td>-1.176228</td>\n",
       "      <td>-0.966954</td>\n",
       "      <td>-1.178578</td>\n",
       "      <td>-0.968309</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>-0.877777</td>\n",
       "      <td>-0.654365</td>\n",
       "      <td>-0.880118</td>\n",
       "      <td>-0.662182</td>\n",
       "      <td>-0.881993</td>\n",
       "      <td>-0.666418</td>\n",
       "      <td>-0.882876</td>\n",
       "      <td>-0.671606</td>\n",
       "      <td>-0.883658</td>\n",
       "      <td>-0.675248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.677078</td>\n",
       "      <td>-0.887877</td>\n",
       "      <td>-0.675872</td>\n",
       "      <td>-0.880546</td>\n",
       "      <td>-0.695943</td>\n",
       "      <td>-0.880403</td>\n",
       "      <td>-0.697412</td>\n",
       "      <td>-0.878851</td>\n",
       "      <td>-0.696865</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>-0.942056</td>\n",
       "      <td>-0.562857</td>\n",
       "      <td>-0.936255</td>\n",
       "      <td>-0.561758</td>\n",
       "      <td>-0.932292</td>\n",
       "      <td>-0.557619</td>\n",
       "      <td>-0.930761</td>\n",
       "      <td>-0.553298</td>\n",
       "      <td>-0.929061</td>\n",
       "      <td>-0.549408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.571721</td>\n",
       "      <td>-0.904552</td>\n",
       "      <td>-0.554281</td>\n",
       "      <td>-0.914351</td>\n",
       "      <td>-0.521937</td>\n",
       "      <td>-0.921207</td>\n",
       "      <td>-0.522046</td>\n",
       "      <td>-0.926716</td>\n",
       "      <td>-0.522946</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>-0.822849</td>\n",
       "      <td>-1.034206</td>\n",
       "      <td>-0.832168</td>\n",
       "      <td>-1.050258</td>\n",
       "      <td>-0.843392</td>\n",
       "      <td>-1.063118</td>\n",
       "      <td>-0.852510</td>\n",
       "      <td>-1.072538</td>\n",
       "      <td>-0.863866</td>\n",
       "      <td>-1.081805</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.042535</td>\n",
       "      <td>-0.921228</td>\n",
       "      <td>-1.042287</td>\n",
       "      <td>-0.899702</td>\n",
       "      <td>-1.048835</td>\n",
       "      <td>-0.891738</td>\n",
       "      <td>-1.051389</td>\n",
       "      <td>-0.883410</td>\n",
       "      <td>-1.052830</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>-0.963092</td>\n",
       "      <td>-1.020394</td>\n",
       "      <td>-0.970171</td>\n",
       "      <td>-1.043450</td>\n",
       "      <td>-0.977911</td>\n",
       "      <td>-1.061444</td>\n",
       "      <td>-0.985654</td>\n",
       "      <td>-1.072538</td>\n",
       "      <td>-0.997749</td>\n",
       "      <td>-1.080192</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.067228</td>\n",
       "      <td>-1.052409</td>\n",
       "      <td>-1.062004</td>\n",
       "      <td>-1.058583</td>\n",
       "      <td>-1.068350</td>\n",
       "      <td>-1.056085</td>\n",
       "      <td>-1.074122</td>\n",
       "      <td>-1.052077</td>\n",
       "      <td>-1.075586</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>-0.734028</td>\n",
       "      <td>-0.978956</td>\n",
       "      <td>-0.724572</td>\n",
       "      <td>-0.958345</td>\n",
       "      <td>-0.717062</td>\n",
       "      <td>-0.939254</td>\n",
       "      <td>-0.713526</td>\n",
       "      <td>-0.919724</td>\n",
       "      <td>-0.710193</td>\n",
       "      <td>-0.902726</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.897669</td>\n",
       "      <td>-0.685547</td>\n",
       "      <td>-0.905908</td>\n",
       "      <td>-0.684479</td>\n",
       "      <td>-0.915484</td>\n",
       "      <td>-0.685454</td>\n",
       "      <td>-0.913371</td>\n",
       "      <td>-0.686251</td>\n",
       "      <td>-0.913045</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>-0.928031</td>\n",
       "      <td>-0.997948</td>\n",
       "      <td>-0.924560</td>\n",
       "      <td>-1.002599</td>\n",
       "      <td>-0.922934</td>\n",
       "      <td>-1.007882</td>\n",
       "      <td>-0.920250</td>\n",
       "      <td>-1.011741</td>\n",
       "      <td>-0.919748</td>\n",
       "      <td>-1.014046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.971748</td>\n",
       "      <td>-0.949020</td>\n",
       "      <td>-0.986421</td>\n",
       "      <td>-0.945902</td>\n",
       "      <td>-0.990291</td>\n",
       "      <td>-0.941609</td>\n",
       "      <td>-0.988063</td>\n",
       "      <td>-0.938113</td>\n",
       "      <td>-0.987813</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>-0.809993</td>\n",
       "      <td>-0.585303</td>\n",
       "      <td>-0.812286</td>\n",
       "      <td>-0.607715</td>\n",
       "      <td>-0.814149</td>\n",
       "      <td>-0.627920</td>\n",
       "      <td>-0.817472</td>\n",
       "      <td>-0.646959</td>\n",
       "      <td>-0.825448</td>\n",
       "      <td>-0.663954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.649092</td>\n",
       "      <td>-0.892324</td>\n",
       "      <td>-0.649582</td>\n",
       "      <td>-0.876039</td>\n",
       "      <td>-0.668297</td>\n",
       "      <td>-0.869069</td>\n",
       "      <td>-0.669808</td>\n",
       "      <td>-0.860617</td>\n",
       "      <td>-0.667608</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>-0.928031</td>\n",
       "      <td>-0.997948</td>\n",
       "      <td>-0.924560</td>\n",
       "      <td>-1.002599</td>\n",
       "      <td>-0.922934</td>\n",
       "      <td>-1.007882</td>\n",
       "      <td>-0.920250</td>\n",
       "      <td>-1.011741</td>\n",
       "      <td>-0.919748</td>\n",
       "      <td>-1.014046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.971748</td>\n",
       "      <td>-0.949020</td>\n",
       "      <td>-0.986421</td>\n",
       "      <td>-0.945902</td>\n",
       "      <td>-0.990291</td>\n",
       "      <td>-0.941609</td>\n",
       "      <td>-0.988063</td>\n",
       "      <td>-0.938113</td>\n",
       "      <td>-0.987813</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>-1.057756</td>\n",
       "      <td>-1.136073</td>\n",
       "      <td>-1.049699</td>\n",
       "      <td>-1.138767</td>\n",
       "      <td>-1.042246</td>\n",
       "      <td>-1.140115</td>\n",
       "      <td>-1.033539</td>\n",
       "      <td>-1.143194</td>\n",
       "      <td>-1.024525</td>\n",
       "      <td>-1.144725</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.167647</td>\n",
       "      <td>-1.042403</td>\n",
       "      <td>-1.178665</td>\n",
       "      <td>-1.027032</td>\n",
       "      <td>-1.182185</td>\n",
       "      <td>-1.022082</td>\n",
       "      <td>-1.176418</td>\n",
       "      <td>-1.020167</td>\n",
       "      <td>-1.169859</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>-0.873103</td>\n",
       "      <td>-0.808028</td>\n",
       "      <td>-0.874270</td>\n",
       "      <td>-0.810263</td>\n",
       "      <td>-0.873805</td>\n",
       "      <td>-0.810369</td>\n",
       "      <td>-0.875868</td>\n",
       "      <td>-0.809632</td>\n",
       "      <td>-0.877837</td>\n",
       "      <td>-0.805927</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.782435</td>\n",
       "      <td>-0.875648</td>\n",
       "      <td>-0.787604</td>\n",
       "      <td>-0.881673</td>\n",
       "      <td>-0.800022</td>\n",
       "      <td>-0.883804</td>\n",
       "      <td>-0.799708</td>\n",
       "      <td>-0.885689</td>\n",
       "      <td>-0.800891</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>-0.273562</td>\n",
       "      <td>-0.963417</td>\n",
       "      <td>-0.274306</td>\n",
       "      <td>-0.999195</td>\n",
       "      <td>-0.277245</td>\n",
       "      <td>-1.029642</td>\n",
       "      <td>-0.280225</td>\n",
       "      <td>-1.054463</td>\n",
       "      <td>-0.287590</td>\n",
       "      <td>-1.075352</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.065582</td>\n",
       "      <td>-0.437638</td>\n",
       "      <td>-1.070220</td>\n",
       "      <td>-0.421931</td>\n",
       "      <td>-1.082986</td>\n",
       "      <td>-0.407764</td>\n",
       "      <td>-1.083865</td>\n",
       "      <td>-0.391083</td>\n",
       "      <td>-1.082087</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>-0.849729</td>\n",
       "      <td>-1.054925</td>\n",
       "      <td>-0.854388</td>\n",
       "      <td>-1.058768</td>\n",
       "      <td>-0.858599</td>\n",
       "      <td>-1.058097</td>\n",
       "      <td>-0.863021</td>\n",
       "      <td>-1.056106</td>\n",
       "      <td>-0.865031</td>\n",
       "      <td>-1.055992</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.012903</td>\n",
       "      <td>-0.887877</td>\n",
       "      <td>-1.022569</td>\n",
       "      <td>-0.868151</td>\n",
       "      <td>-1.034199</td>\n",
       "      <td>-0.862269</td>\n",
       "      <td>-1.035152</td>\n",
       "      <td>-0.857198</td>\n",
       "      <td>-1.034950</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>-0.039822</td>\n",
       "      <td>-1.051472</td>\n",
       "      <td>-0.036893</td>\n",
       "      <td>-1.074087</td>\n",
       "      <td>-0.035112</td>\n",
       "      <td>-1.094921</td>\n",
       "      <td>-0.037296</td>\n",
       "      <td>-1.113617</td>\n",
       "      <td>-0.045438</td>\n",
       "      <td>-1.125365</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.119907</td>\n",
       "      <td>-0.166383</td>\n",
       "      <td>-1.135944</td>\n",
       "      <td>-0.156001</td>\n",
       "      <td>-1.136651</td>\n",
       "      <td>-0.144808</td>\n",
       "      <td>-1.135825</td>\n",
       "      <td>-0.133523</td>\n",
       "      <td>-1.134100</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>-0.928031</td>\n",
       "      <td>-0.997948</td>\n",
       "      <td>-0.924560</td>\n",
       "      <td>-1.002599</td>\n",
       "      <td>-0.922934</td>\n",
       "      <td>-1.007882</td>\n",
       "      <td>-0.920250</td>\n",
       "      <td>-1.011741</td>\n",
       "      <td>-0.919748</td>\n",
       "      <td>-1.014046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.971748</td>\n",
       "      <td>-0.949020</td>\n",
       "      <td>-0.986421</td>\n",
       "      <td>-0.945902</td>\n",
       "      <td>-0.990291</td>\n",
       "      <td>-0.941609</td>\n",
       "      <td>-0.988063</td>\n",
       "      <td>-0.938113</td>\n",
       "      <td>-0.987813</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>-1.091649</td>\n",
       "      <td>-1.122260</td>\n",
       "      <td>-1.091801</td>\n",
       "      <td>-1.096214</td>\n",
       "      <td>-1.092544</td>\n",
       "      <td>-1.066466</td>\n",
       "      <td>-1.094271</td>\n",
       "      <td>-1.039675</td>\n",
       "      <td>-1.086228</td>\n",
       "      <td>-1.018885</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.080398</td>\n",
       "      <td>-1.019058</td>\n",
       "      <td>-1.043930</td>\n",
       "      <td>-1.004496</td>\n",
       "      <td>-1.029320</td>\n",
       "      <td>-1.006214</td>\n",
       "      <td>-1.030281</td>\n",
       "      <td>-1.011050</td>\n",
       "      <td>-1.030074</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>-1.076456</td>\n",
       "      <td>-0.973777</td>\n",
       "      <td>-1.073089</td>\n",
       "      <td>-0.905580</td>\n",
       "      <td>-1.069150</td>\n",
       "      <td>-0.838824</td>\n",
       "      <td>-1.059233</td>\n",
       "      <td>-0.778412</td>\n",
       "      <td>-1.043152</td>\n",
       "      <td>-0.725261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.667200</td>\n",
       "      <td>-0.870090</td>\n",
       "      <td>-0.667656</td>\n",
       "      <td>-0.890687</td>\n",
       "      <td>-0.702448</td>\n",
       "      <td>-0.900805</td>\n",
       "      <td>-0.703907</td>\n",
       "      <td>-0.909622</td>\n",
       "      <td>-0.706617</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>-1.013346</td>\n",
       "      <td>-0.814934</td>\n",
       "      <td>-1.011104</td>\n",
       "      <td>-0.830688</td>\n",
       "      <td>-1.011833</td>\n",
       "      <td>-0.843845</td>\n",
       "      <td>-1.011348</td>\n",
       "      <td>-0.855640</td>\n",
       "      <td>-1.014047</td>\n",
       "      <td>-0.867233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.884499</td>\n",
       "      <td>-1.045738</td>\n",
       "      <td>-0.900979</td>\n",
       "      <td>-1.046188</td>\n",
       "      <td>-0.895969</td>\n",
       "      <td>-1.043617</td>\n",
       "      <td>-0.890638</td>\n",
       "      <td>-1.041821</td>\n",
       "      <td>-0.885413</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>-0.908163</td>\n",
       "      <td>-0.122587</td>\n",
       "      <td>-0.908186</td>\n",
       "      <td>-0.115811</td>\n",
       "      <td>-0.905388</td>\n",
       "      <td>-0.104008</td>\n",
       "      <td>-0.907402</td>\n",
       "      <td>-0.094856</td>\n",
       "      <td>-0.910434</td>\n",
       "      <td>-0.076704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067982</td>\n",
       "      <td>-0.874536</td>\n",
       "      <td>-0.056416</td>\n",
       "      <td>-0.873785</td>\n",
       "      <td>-0.090986</td>\n",
       "      <td>-0.878137</td>\n",
       "      <td>-0.090128</td>\n",
       "      <td>-0.883410</td>\n",
       "      <td>-0.092212</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>-0.624170</td>\n",
       "      <td>-1.041112</td>\n",
       "      <td>-0.621654</td>\n",
       "      <td>-1.048556</td>\n",
       "      <td>-0.621144</td>\n",
       "      <td>-1.054749</td>\n",
       "      <td>-0.622428</td>\n",
       "      <td>-1.057750</td>\n",
       "      <td>-0.625207</td>\n",
       "      <td>-1.059219</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.077105</td>\n",
       "      <td>-0.645526</td>\n",
       "      <td>-1.083365</td>\n",
       "      <td>-0.643914</td>\n",
       "      <td>-1.084612</td>\n",
       "      <td>-0.641250</td>\n",
       "      <td>-1.083865</td>\n",
       "      <td>-0.639526</td>\n",
       "      <td>-1.083713</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>-1.076456</td>\n",
       "      <td>-0.973777</td>\n",
       "      <td>-1.073089</td>\n",
       "      <td>-0.905580</td>\n",
       "      <td>-1.069150</td>\n",
       "      <td>-0.838824</td>\n",
       "      <td>-1.059233</td>\n",
       "      <td>-0.778412</td>\n",
       "      <td>-1.043152</td>\n",
       "      <td>-0.725261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.667200</td>\n",
       "      <td>-0.870090</td>\n",
       "      <td>-0.667656</td>\n",
       "      <td>-0.890687</td>\n",
       "      <td>-0.702448</td>\n",
       "      <td>-0.900805</td>\n",
       "      <td>-0.703907</td>\n",
       "      <td>-0.909622</td>\n",
       "      <td>-0.706617</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>-0.745715</td>\n",
       "      <td>-1.068737</td>\n",
       "      <td>-0.739775</td>\n",
       "      <td>-1.062173</td>\n",
       "      <td>-0.734608</td>\n",
       "      <td>-1.049728</td>\n",
       "      <td>-0.731045</td>\n",
       "      <td>-1.033102</td>\n",
       "      <td>-0.727656</td>\n",
       "      <td>-1.015659</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.078752</td>\n",
       "      <td>-0.738909</td>\n",
       "      <td>-1.098153</td>\n",
       "      <td>-0.750962</td>\n",
       "      <td>-1.081359</td>\n",
       "      <td>-0.753459</td>\n",
       "      <td>-1.075746</td>\n",
       "      <td>-0.755770</td>\n",
       "      <td>-1.073960</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>-0.825186</td>\n",
       "      <td>-0.890902</td>\n",
       "      <td>-0.821642</td>\n",
       "      <td>-0.878347</td>\n",
       "      <td>-0.815319</td>\n",
       "      <td>-0.863931</td>\n",
       "      <td>-0.809296</td>\n",
       "      <td>-0.850711</td>\n",
       "      <td>-0.806821</td>\n",
       "      <td>-0.836580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.833467</td>\n",
       "      <td>-0.784489</td>\n",
       "      <td>-0.831968</td>\n",
       "      <td>-0.782513</td>\n",
       "      <td>-0.850435</td>\n",
       "      <td>-0.784062</td>\n",
       "      <td>-0.850044</td>\n",
       "      <td>-0.783121</td>\n",
       "      <td>-0.848028</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1185 rows × 137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x1        y1        x2        y2        x3        y3        x4  \\\n",
       "0     1.361444  1.510731  1.347821  1.526705  1.338147  1.543050  1.331516   \n",
       "1     0.336498  1.483107  0.304608  1.521599  0.274866  1.553093  0.247679   \n",
       "2     2.507934  0.970321  2.511495  0.988845  2.513722  1.002400  2.511124   \n",
       "3     3.811030  1.151609  3.804986  1.167564  3.802761  1.186522  3.805188   \n",
       "4    -0.896476  0.954782 -0.889474  0.966718 -0.880824  0.978966 -0.872365   \n",
       "5    -0.057353  1.194772 -0.054436  1.186287 -0.047979  1.173131 -0.033792   \n",
       "6     1.920080  0.013811  1.918548  0.011846  1.917162  0.008139  1.918984   \n",
       "7     3.354070  0.188193  3.351212  0.176948  3.346567  0.168828  3.339185   \n",
       "8    -0.141499  0.162294 -0.139811  0.165034 -0.140387  0.168828 -0.140074   \n",
       "9     1.175621  0.212364  1.179410  0.192267  1.181404  0.168828  1.179685   \n",
       "10    0.414800  1.350162  0.415712  1.347986  0.421082  1.343863  0.422868   \n",
       "11   -0.188247  0.562854 -0.184252  0.571833 -0.181327  0.580592 -0.175111   \n",
       "12    0.684769  0.640549  0.689380  0.650129  0.690119  0.657589  0.687988   \n",
       "13    1.530905  0.726877  1.539623  0.723319  1.546359  0.717847  1.551087   \n",
       "14   -0.780776  0.937516 -0.777200  0.951399 -0.775548  0.962227 -0.764915   \n",
       "15    2.575719  0.859821  2.568802  0.850976  2.560511  0.841711  2.546162   \n",
       "16    0.332992  0.492066  0.332676  0.502047  0.328673  0.506943  0.317754   \n",
       "17    1.064595  1.146429  1.054271  1.131820  1.044546  1.114547  1.043038   \n",
       "18    2.603767  0.545589  2.601548  0.556514  2.596773  0.565528  2.590543   \n",
       "19    0.795795  0.243442  0.806332  0.313115  0.814110  0.379731  0.817628   \n",
       "20    0.169374  0.274520  0.164265  0.301201  0.157893  0.326168  0.150741   \n",
       "21    1.215357  0.659541  1.208648  0.668852  1.202459  0.677675  1.192533   \n",
       "22   -1.203843  2.403359 -1.206415  2.370941 -1.208347  2.333102 -1.207560   \n",
       "23    0.738529  2.356742  0.744348  2.307964  0.747435  2.252758  0.746384   \n",
       "24    1.335732  2.411991  1.336126  2.367537  1.333468  2.321385  1.326844   \n",
       "25    0.075878  2.028697  0.074212  1.986269  0.070164  1.939750  0.063146   \n",
       "26   -0.682605  2.306672 -0.690655  2.260305 -0.698346  2.209238 -0.705351   \n",
       "27    2.180699  2.056322  2.181690  2.018609  2.180350  1.973227  2.173592   \n",
       "28    2.237966  0.923704  2.240166  0.947995  2.241176  0.968923  2.241332   \n",
       "29    0.843712  0.355668  0.846096  0.415241  0.849202  0.473466  0.852666   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1155 -1.212024 -1.251752 -1.213432 -1.268125 -1.216535 -1.279044 -1.220407   \n",
       "1156 -1.108010 -1.186143 -1.111683 -1.206850 -1.117108 -1.223807 -1.124637   \n",
       "1157 -1.152421 -1.039386 -1.151447 -1.031535 -1.151031 -1.022946 -1.149164   \n",
       "1158 -1.175795 -1.053198 -1.179516 -1.051960 -1.181443 -1.048054 -1.184201   \n",
       "1159 -0.613652 -0.904715 -0.627501 -0.832391 -0.638690 -0.755132 -0.638779   \n",
       "1160 -1.022696 -1.051472 -1.022800 -1.072385 -1.024700 -1.091574 -1.031203   \n",
       "1161 -0.799475 -1.048018 -0.800590 -1.070683 -0.804791 -1.091574 -0.810464   \n",
       "1162 -1.172289 -1.046292 -1.174837 -1.040045 -1.176764 -1.031315 -1.178362   \n",
       "1163 -0.877777 -0.654365 -0.880118 -0.662182 -0.881993 -0.666418 -0.882876   \n",
       "1164 -0.942056 -0.562857 -0.936255 -0.561758 -0.932292 -0.557619 -0.930761   \n",
       "1165 -0.822849 -1.034206 -0.832168 -1.050258 -0.843392 -1.063118 -0.852510   \n",
       "1166 -0.963092 -1.020394 -0.970171 -1.043450 -0.977911 -1.061444 -0.985654   \n",
       "1167 -0.734028 -0.978956 -0.724572 -0.958345 -0.717062 -0.939254 -0.713526   \n",
       "1168 -0.928031 -0.997948 -0.924560 -1.002599 -0.922934 -1.007882 -0.920250   \n",
       "1169 -0.809993 -0.585303 -0.812286 -0.607715 -0.814149 -0.627920 -0.817472   \n",
       "1170 -0.928031 -0.997948 -0.924560 -1.002599 -0.922934 -1.007882 -0.920250   \n",
       "1171 -1.057756 -1.136073 -1.049699 -1.138767 -1.042246 -1.140115 -1.033539   \n",
       "1172 -0.873103 -0.808028 -0.874270 -0.810263 -0.873805 -0.810369 -0.875868   \n",
       "1173 -0.273562 -0.963417 -0.274306 -0.999195 -0.277245 -1.029642 -0.280225   \n",
       "1174 -0.849729 -1.054925 -0.854388 -1.058768 -0.858599 -1.058097 -0.863021   \n",
       "1175 -0.039822 -1.051472 -0.036893 -1.074087 -0.035112 -1.094921 -0.037296   \n",
       "1176 -0.928031 -0.997948 -0.924560 -1.002599 -0.922934 -1.007882 -0.920250   \n",
       "1177 -1.091649 -1.122260 -1.091801 -1.096214 -1.092544 -1.066466 -1.094271   \n",
       "1178 -1.076456 -0.973777 -1.073089 -0.905580 -1.069150 -0.838824 -1.059233   \n",
       "1179 -1.013346 -0.814934 -1.011104 -0.830688 -1.011833 -0.843845 -1.011348   \n",
       "1180 -0.908163 -0.122587 -0.908186 -0.115811 -0.905388 -0.104008 -0.907402   \n",
       "1181 -0.624170 -1.041112 -0.621654 -1.048556 -0.621144 -1.054749 -0.622428   \n",
       "1182 -1.076456 -0.973777 -1.073089 -0.905580 -1.069150 -0.838824 -1.059233   \n",
       "1183 -0.745715 -1.068737 -0.739775 -1.062173 -0.734608 -1.049728 -0.731045   \n",
       "1184 -0.825186 -0.890902 -0.821642 -0.878347 -0.815319 -0.863931 -0.809296   \n",
       "\n",
       "            y4        x5        y5  ...       y64       x65       y65  \\\n",
       "0     1.559810  1.321326  1.585019  ...  1.645717  1.414456  1.668856   \n",
       "1     1.587744  0.230476  1.625352  ...  1.874539  0.329436  1.892320   \n",
       "2     1.014280  2.508804  1.018743  ...  1.066253  2.486136  1.049401   \n",
       "3     1.196671  3.802224  1.202661  ...  1.248982  3.705673  1.261363   \n",
       "4     0.987989 -0.853389  0.996156  ...  0.975712 -0.686659  0.977104   \n",
       "5     1.157235 -0.018661  1.141355  ...  1.137040 -0.006298  1.095409   \n",
       "6     0.003734  1.909244 -0.000878  ... -0.035058  1.811334 -0.031770   \n",
       "7     0.164764  3.326068  0.162068  ...  0.164133  3.194290  0.158832   \n",
       "8     0.171337 -0.132753  0.170134  ...  0.201996 -0.095234  0.186765   \n",
       "9     0.145046  1.175802  0.120121  ...  0.172364  1.055377  0.155546   \n",
       "10    1.337983  0.424896  1.330114  ...  1.334584  0.459505  1.323802   \n",
       "11    0.585414 -0.163022  0.583145  ...  0.585561 -0.096346  0.581113   \n",
       "12    0.664286  0.689169  0.671878  ...  0.682687  0.712973  0.692845   \n",
       "13    0.711938  1.554165  0.704144  ...  0.662933  1.503392  0.655053   \n",
       "14    0.971558 -0.742790  0.970343  ...  0.942788 -0.608840  0.952457   \n",
       "15    0.831889  2.525103  0.829984  ...  0.883524  2.416099  0.901521   \n",
       "16    0.514758  0.310805  0.521839  ...  0.524652  0.359452  0.540035   \n",
       "17    1.098081  1.036098  1.086502  ...  1.137040  1.004239  1.139773   \n",
       "18    0.567339  2.590298  0.567012  ...  0.534529  2.529493  0.553180   \n",
       "19    0.445745  0.828872  0.504092  ...  0.409417  1.026473  0.431589   \n",
       "20    0.353728  0.148982  0.378253  ...  0.411064  0.242723  0.438162   \n",
       "21    0.684004  1.179294  0.691238  ...  0.720550  1.156542  0.745425   \n",
       "22    2.292661 -1.204975  2.254549  ...  2.322307 -1.158020  2.321174   \n",
       "23    2.195715  0.741557  2.138389  ...  2.152748  0.664058  2.148647   \n",
       "24    2.269657  1.315505  2.222282  ...  2.261397  1.239919  2.253806   \n",
       "25    1.893373  0.054683  1.846378  ...  1.889355  0.031500  1.887391   \n",
       "26    2.157922 -0.711357  2.109349  ...  2.223535 -0.720010  2.222587   \n",
       "27    1.926236  2.160710  1.878644  ...  1.904171  2.029227  1.905465   \n",
       "28    0.986346  2.244532  0.991316  ...  1.018513  2.271578  0.996822   \n",
       "29    0.524617  0.864962  0.571852  ...  0.575684  1.014244  0.602473   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1155 -1.287793 -1.224767 -1.293151  ... -1.281235 -1.256962 -1.277252   \n",
       "1156 -1.238498 -1.135124 -1.251204  ... -1.236787 -1.213605 -1.236175   \n",
       "1157 -1.013384 -1.147930 -1.004366  ... -1.014550 -1.132451 -0.999566   \n",
       "1158 -1.041318 -1.189841 -1.026952  ... -1.001380 -1.185813 -0.989707   \n",
       "1159 -0.681465 -0.625207 -0.618781  ... -0.514103 -0.440973 -0.509917   \n",
       "1160 -1.107045 -1.041988 -1.118912  ... -1.119907 -1.105770 -1.116227   \n",
       "1161 -1.108688 -0.821955 -1.122138  ... -1.124845 -0.901217 -1.119513   \n",
       "1162 -1.019957 -1.184020 -1.002752  ... -0.965163 -1.172472 -0.960131   \n",
       "1163 -0.671606 -0.883658 -0.675248  ... -0.677078 -0.887877 -0.675872   \n",
       "1164 -0.553298 -0.929061 -0.549408  ... -0.571721 -0.904552 -0.554281   \n",
       "1165 -1.072538 -0.863866 -1.081805  ... -1.042535 -0.921228 -1.042287   \n",
       "1166 -1.072538 -0.997749 -1.080192  ... -1.067228 -1.052409 -1.062004   \n",
       "1167 -0.919724 -0.710193 -0.902726  ... -0.897669 -0.685547 -0.905908   \n",
       "1168 -1.011741 -0.919748 -1.014046  ... -0.971748 -0.949020 -0.986421   \n",
       "1169 -0.646959 -0.825448 -0.663954  ... -0.649092 -0.892324 -0.649582   \n",
       "1170 -1.011741 -0.919748 -1.014046  ... -0.971748 -0.949020 -0.986421   \n",
       "1171 -1.143194 -1.024525 -1.144725  ... -1.167647 -1.042403 -1.178665   \n",
       "1172 -0.809632 -0.877837 -0.805927  ... -0.782435 -0.875648 -0.787604   \n",
       "1173 -1.054463 -0.287590 -1.075352  ... -1.065582 -0.437638 -1.070220   \n",
       "1174 -1.056106 -0.865031 -1.055992  ... -1.012903 -0.887877 -1.022569   \n",
       "1175 -1.113617 -0.045438 -1.125365  ... -1.119907 -0.166383 -1.135944   \n",
       "1176 -1.011741 -0.919748 -1.014046  ... -0.971748 -0.949020 -0.986421   \n",
       "1177 -1.039675 -1.086228 -1.018885  ... -1.080398 -1.019058 -1.043930   \n",
       "1178 -0.778412 -1.043152 -0.725261  ... -0.667200 -0.870090 -0.667656   \n",
       "1179 -0.855640 -1.014047 -0.867233  ... -0.884499 -1.045738 -0.900979   \n",
       "1180 -0.094856 -0.910434 -0.076704  ... -0.067982 -0.874536 -0.056416   \n",
       "1181 -1.057750 -0.625207 -1.059219  ... -1.077105 -0.645526 -1.083365   \n",
       "1182 -0.778412 -1.043152 -0.725261  ... -0.667200 -0.870090 -0.667656   \n",
       "1183 -1.033102 -0.727656 -1.015659  ... -1.078752 -0.738909 -1.098153   \n",
       "1184 -0.850711 -0.806821 -0.836580  ... -0.833467 -0.784489 -0.831968   \n",
       "\n",
       "           x66       y66       x67       y67       x68       y68    target  \n",
       "0     1.349429  1.694612  1.330916  1.686256  1.316106  1.672984 -0.688390  \n",
       "1     0.291346  1.836094  0.280228  1.817780  0.274470  1.798140 -0.688390  \n",
       "2     2.503292  1.024606  2.507415  1.027013  2.512734  1.026070 -0.688390  \n",
       "3     3.715750  1.281550  3.717917  1.280318  3.723037  1.271507 -0.688390  \n",
       "4    -0.704762  0.988829 -0.717190  0.991290 -0.735256  0.993562 -0.688390  \n",
       "5    -0.039939  1.136816 -0.048467  1.147170 -0.058307  1.154478 -0.688390  \n",
       "6     1.827201 -0.042200  1.838692 -0.039792  1.850600 -0.033697 -0.688390  \n",
       "7     3.218822  0.183846  3.233943  0.182662  3.250084  0.182482 -0.688390  \n",
       "8    -0.089519  0.188725 -0.089270  0.189157 -0.091356  0.188984 -0.688390  \n",
       "9     1.097022  0.136686  1.115564  0.135573  1.131483  0.138596 -0.688390  \n",
       "10    0.446847  1.348225  0.440041  1.351763  0.432881  1.349527 -0.688390  \n",
       "11   -0.081631  0.559505 -0.080203  0.557748 -0.082239  0.561203 -0.688390  \n",
       "12    0.734186  0.687977  0.738133  0.687649  0.738306  0.684734 -0.688390  \n",
       "13    1.522959  0.626180  1.534933  0.627570  1.544035  0.634346 -0.688390  \n",
       "14   -0.606729  0.941668 -0.611781  0.936083 -0.617873  0.935047 -0.688390  \n",
       "15    2.444698  0.879872  2.457544  0.876004  2.469427  0.873282 -0.688390  \n",
       "16    0.356702  0.526980  0.355034  0.522026  0.354245  0.518942 -0.688390  \n",
       "17    0.982086  1.154704  0.979553  1.150418  0.982190  1.144725 -0.688390  \n",
       "18    2.567521  0.559505  2.581088  0.556125  2.591369  0.551450 -0.688390  \n",
       "19    1.002369  0.463557  0.985220  0.470066  0.969654  0.466929 -0.688390  \n",
       "20    0.237259  0.455426  0.230357  0.450581  0.224326  0.442548 -0.688390  \n",
       "21    1.171392  0.705865  1.175636  0.699015  1.180488  0.694486 -0.688390  \n",
       "22   -1.159997  2.271923 -1.160360  2.267559 -1.159204  2.267884 -0.688390  \n",
       "23    0.691367  2.123936  0.699596  2.121421  0.707535  2.126473 -0.688390  \n",
       "24    1.257030  2.245903  1.262910  2.243203  1.269380  2.245128 -0.688390  \n",
       "25    0.034431  1.863740  0.035407  1.858373  0.038563  1.858280 -0.688390  \n",
       "26   -0.711523  2.177602 -0.708122  2.171758 -0.707905  2.171985 -0.688390  \n",
       "27    2.068340  1.863740  2.085779  1.859997  2.103601  1.863157 -0.688390  \n",
       "28    2.248631  1.034363  2.239926  1.033508  2.231241  1.024445 -0.688390  \n",
       "29    0.996735  0.595282  0.982953  0.591847  0.966235  0.582333 -0.688390  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1155 -1.249016 -1.287890 -1.245367 -1.288457 -1.241259 -1.288514  1.452665  \n",
       "1156 -1.199436 -1.252113 -1.194363 -1.254358 -1.188835 -1.252755  1.452665  \n",
       "1157 -1.136334 -0.980533 -1.143359 -0.979944 -1.148947 -0.982937  1.452665  \n",
       "1158 -1.190421 -1.000048 -1.194363 -1.002677 -1.195673 -1.004068  1.452665  \n",
       "1159 -0.444467 -0.526816 -0.447434 -0.530165 -0.450345 -0.535949  1.452665  \n",
       "1160 -1.092388 -1.133399 -1.086687 -1.134201 -1.080569 -1.134100  1.452665  \n",
       "1161 -0.882800 -1.138277 -0.874736 -1.140696 -0.866315 -1.140602  1.452665  \n",
       "1162 -1.172392 -0.967523 -1.176228 -0.966954 -1.178578 -0.968309  1.452665  \n",
       "1163 -0.880546 -0.695943 -0.880403 -0.697412 -0.878851 -0.696865  1.452665  \n",
       "1164 -0.914351 -0.521937 -0.921207 -0.522046 -0.926716 -0.522946  1.452665  \n",
       "1165 -0.899702 -1.048835 -0.891738 -1.051389 -0.883410 -1.052830  1.452665  \n",
       "1166 -1.058583 -1.068350 -1.056085 -1.074122 -1.052077 -1.075586  1.452665  \n",
       "1167 -0.684479 -0.915484 -0.685454 -0.913371 -0.686251 -0.913045  1.452665  \n",
       "1168 -0.945902 -0.990291 -0.941609 -0.988063 -0.938113 -0.987813  1.452665  \n",
       "1169 -0.876039 -0.668297 -0.869069 -0.669808 -0.860617 -0.667608  1.452665  \n",
       "1170 -0.945902 -0.990291 -0.941609 -0.988063 -0.938113 -0.987813  1.452665  \n",
       "1171 -1.027032 -1.182185 -1.022082 -1.176418 -1.020167 -1.169859  1.452665  \n",
       "1172 -0.881673 -0.800022 -0.883804 -0.799708 -0.885689 -0.800891  1.452665  \n",
       "1173 -0.421931 -1.082986 -0.407764 -1.083865 -0.391083 -1.082087  1.452665  \n",
       "1174 -0.868151 -1.034199 -0.862269 -1.035152 -0.857198 -1.034950  1.452665  \n",
       "1175 -0.156001 -1.136651 -0.144808 -1.135825 -0.133523 -1.134100  1.452665  \n",
       "1176 -0.945902 -0.990291 -0.941609 -0.988063 -0.938113 -0.987813  1.452665  \n",
       "1177 -1.004496 -1.029320 -1.006214 -1.030281 -1.011050 -1.030074  1.452665  \n",
       "1178 -0.890687 -0.702448 -0.900805 -0.703907 -0.909622 -0.706617  1.452665  \n",
       "1179 -1.046188 -0.895969 -1.043617 -0.890638 -1.041821 -0.885413  1.452665  \n",
       "1180 -0.873785 -0.090986 -0.878137 -0.090128 -0.883410 -0.092212  1.452665  \n",
       "1181 -0.643914 -1.084612 -0.641250 -1.083865 -0.639526 -1.083713  1.452665  \n",
       "1182 -0.890687 -0.702448 -0.900805 -0.703907 -0.909622 -0.706617  1.452665  \n",
       "1183 -0.750962 -1.081359 -0.753459 -1.075746 -0.755770 -1.073960  1.452665  \n",
       "1184 -0.782513 -0.850435 -0.784062 -0.850044 -0.783121 -0.848028  1.452665  \n",
       "\n",
       "[1185 rows x 137 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>y5</th>\n",
       "      <th>...</th>\n",
       "      <th>y62</th>\n",
       "      <th>x63</th>\n",
       "      <th>y63</th>\n",
       "      <th>x64</th>\n",
       "      <th>y64</th>\n",
       "      <th>x65</th>\n",
       "      <th>y65</th>\n",
       "      <th>x67</th>\n",
       "      <th>y67</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.361444</td>\n",
       "      <td>1.510731</td>\n",
       "      <td>1.347821</td>\n",
       "      <td>1.526705</td>\n",
       "      <td>1.338147</td>\n",
       "      <td>1.543050</td>\n",
       "      <td>1.331516</td>\n",
       "      <td>1.559810</td>\n",
       "      <td>1.321326</td>\n",
       "      <td>1.585019</td>\n",
       "      <td>...</td>\n",
       "      <td>1.628604</td>\n",
       "      <td>1.334749</td>\n",
       "      <td>1.638874</td>\n",
       "      <td>1.351820</td>\n",
       "      <td>1.645717</td>\n",
       "      <td>1.414456</td>\n",
       "      <td>1.668856</td>\n",
       "      <td>1.330916</td>\n",
       "      <td>1.686256</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.336498</td>\n",
       "      <td>1.483107</td>\n",
       "      <td>0.304608</td>\n",
       "      <td>1.521599</td>\n",
       "      <td>0.274866</td>\n",
       "      <td>1.553093</td>\n",
       "      <td>0.247679</td>\n",
       "      <td>1.587744</td>\n",
       "      <td>0.230476</td>\n",
       "      <td>1.625352</td>\n",
       "      <td>...</td>\n",
       "      <td>1.840849</td>\n",
       "      <td>0.289702</td>\n",
       "      <td>1.859211</td>\n",
       "      <td>0.300219</td>\n",
       "      <td>1.874539</td>\n",
       "      <td>0.329436</td>\n",
       "      <td>1.892320</td>\n",
       "      <td>0.280228</td>\n",
       "      <td>1.817780</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.507934</td>\n",
       "      <td>0.970321</td>\n",
       "      <td>2.511495</td>\n",
       "      <td>0.988845</td>\n",
       "      <td>2.513722</td>\n",
       "      <td>1.002400</td>\n",
       "      <td>2.511124</td>\n",
       "      <td>1.014280</td>\n",
       "      <td>2.508804</td>\n",
       "      <td>1.018743</td>\n",
       "      <td>...</td>\n",
       "      <td>1.064263</td>\n",
       "      <td>2.507876</td>\n",
       "      <td>1.066654</td>\n",
       "      <td>2.504862</td>\n",
       "      <td>1.066253</td>\n",
       "      <td>2.486136</td>\n",
       "      <td>1.049401</td>\n",
       "      <td>2.507415</td>\n",
       "      <td>1.027013</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.811030</td>\n",
       "      <td>1.151609</td>\n",
       "      <td>3.804986</td>\n",
       "      <td>1.167564</td>\n",
       "      <td>3.802761</td>\n",
       "      <td>1.186522</td>\n",
       "      <td>3.805188</td>\n",
       "      <td>1.196671</td>\n",
       "      <td>3.802224</td>\n",
       "      <td>1.202661</td>\n",
       "      <td>...</td>\n",
       "      <td>1.241956</td>\n",
       "      <td>3.716140</td>\n",
       "      <td>1.247528</td>\n",
       "      <td>3.715387</td>\n",
       "      <td>1.248982</td>\n",
       "      <td>3.705673</td>\n",
       "      <td>1.261363</td>\n",
       "      <td>3.717917</td>\n",
       "      <td>1.280318</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.896476</td>\n",
       "      <td>0.954782</td>\n",
       "      <td>-0.889474</td>\n",
       "      <td>0.966718</td>\n",
       "      <td>-0.880824</td>\n",
       "      <td>0.978966</td>\n",
       "      <td>-0.872365</td>\n",
       "      <td>0.987989</td>\n",
       "      <td>-0.853389</td>\n",
       "      <td>0.996156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977061</td>\n",
       "      <td>-0.717941</td>\n",
       "      <td>0.976217</td>\n",
       "      <td>-0.706298</td>\n",
       "      <td>0.975712</td>\n",
       "      <td>-0.686659</td>\n",
       "      <td>0.977104</td>\n",
       "      <td>-0.717190</td>\n",
       "      <td>0.991290</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.057353</td>\n",
       "      <td>1.194772</td>\n",
       "      <td>-0.054436</td>\n",
       "      <td>1.186287</td>\n",
       "      <td>-0.047979</td>\n",
       "      <td>1.173131</td>\n",
       "      <td>-0.033792</td>\n",
       "      <td>1.157235</td>\n",
       "      <td>-0.018661</td>\n",
       "      <td>1.141355</td>\n",
       "      <td>...</td>\n",
       "      <td>1.158046</td>\n",
       "      <td>-0.059403</td>\n",
       "      <td>1.148869</td>\n",
       "      <td>-0.050315</td>\n",
       "      <td>1.137040</td>\n",
       "      <td>-0.006298</td>\n",
       "      <td>1.095409</td>\n",
       "      <td>-0.048467</td>\n",
       "      <td>1.147170</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.920080</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>1.918548</td>\n",
       "      <td>0.011846</td>\n",
       "      <td>1.917162</td>\n",
       "      <td>0.008139</td>\n",
       "      <td>1.918984</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>1.909244</td>\n",
       "      <td>-0.000878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026578</td>\n",
       "      <td>1.836870</td>\n",
       "      <td>-0.031746</td>\n",
       "      <td>1.826337</td>\n",
       "      <td>-0.035058</td>\n",
       "      <td>1.811334</td>\n",
       "      <td>-0.031770</td>\n",
       "      <td>1.838692</td>\n",
       "      <td>-0.039792</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.354070</td>\n",
       "      <td>0.188193</td>\n",
       "      <td>3.351212</td>\n",
       "      <td>0.176948</td>\n",
       "      <td>3.346567</td>\n",
       "      <td>0.168828</td>\n",
       "      <td>3.339185</td>\n",
       "      <td>0.164764</td>\n",
       "      <td>3.326068</td>\n",
       "      <td>0.162068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164278</td>\n",
       "      <td>3.232154</td>\n",
       "      <td>0.165572</td>\n",
       "      <td>3.217200</td>\n",
       "      <td>0.164133</td>\n",
       "      <td>3.194290</td>\n",
       "      <td>0.158832</td>\n",
       "      <td>3.233943</td>\n",
       "      <td>0.182662</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.141499</td>\n",
       "      <td>0.162294</td>\n",
       "      <td>-0.139811</td>\n",
       "      <td>0.165034</td>\n",
       "      <td>-0.140387</td>\n",
       "      <td>0.168828</td>\n",
       "      <td>-0.140074</td>\n",
       "      <td>0.171337</td>\n",
       "      <td>-0.132753</td>\n",
       "      <td>0.170134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198830</td>\n",
       "      <td>-0.085472</td>\n",
       "      <td>0.201746</td>\n",
       "      <td>-0.085256</td>\n",
       "      <td>0.201996</td>\n",
       "      <td>-0.095234</td>\n",
       "      <td>0.186765</td>\n",
       "      <td>-0.089270</td>\n",
       "      <td>0.189157</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.175621</td>\n",
       "      <td>0.212364</td>\n",
       "      <td>1.179410</td>\n",
       "      <td>0.192267</td>\n",
       "      <td>1.181404</td>\n",
       "      <td>0.168828</td>\n",
       "      <td>1.179685</td>\n",
       "      <td>0.145046</td>\n",
       "      <td>1.175802</td>\n",
       "      <td>0.120121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174150</td>\n",
       "      <td>1.114858</td>\n",
       "      <td>0.172149</td>\n",
       "      <td>1.097091</td>\n",
       "      <td>0.172364</td>\n",
       "      <td>1.055377</td>\n",
       "      <td>0.155546</td>\n",
       "      <td>1.115564</td>\n",
       "      <td>0.135573</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.414800</td>\n",
       "      <td>1.350162</td>\n",
       "      <td>0.415712</td>\n",
       "      <td>1.347986</td>\n",
       "      <td>0.421082</td>\n",
       "      <td>1.343863</td>\n",
       "      <td>0.422868</td>\n",
       "      <td>1.337983</td>\n",
       "      <td>0.424896</td>\n",
       "      <td>1.330114</td>\n",
       "      <td>...</td>\n",
       "      <td>1.335739</td>\n",
       "      <td>0.438185</td>\n",
       "      <td>1.336320</td>\n",
       "      <td>0.445617</td>\n",
       "      <td>1.334584</td>\n",
       "      <td>0.459505</td>\n",
       "      <td>1.323802</td>\n",
       "      <td>0.440041</td>\n",
       "      <td>1.351763</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.188247</td>\n",
       "      <td>0.562854</td>\n",
       "      <td>-0.184252</td>\n",
       "      <td>0.571833</td>\n",
       "      <td>-0.181327</td>\n",
       "      <td>0.580592</td>\n",
       "      <td>-0.175111</td>\n",
       "      <td>0.585414</td>\n",
       "      <td>-0.163022</td>\n",
       "      <td>0.583145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587123</td>\n",
       "      <td>-0.082072</td>\n",
       "      <td>0.584871</td>\n",
       "      <td>-0.084129</td>\n",
       "      <td>0.585561</td>\n",
       "      <td>-0.096346</td>\n",
       "      <td>0.581113</td>\n",
       "      <td>-0.080203</td>\n",
       "      <td>0.557748</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.684769</td>\n",
       "      <td>0.640549</td>\n",
       "      <td>0.689380</td>\n",
       "      <td>0.650129</td>\n",
       "      <td>0.690119</td>\n",
       "      <td>0.657589</td>\n",
       "      <td>0.687988</td>\n",
       "      <td>0.664286</td>\n",
       "      <td>0.689169</td>\n",
       "      <td>0.671878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677615</td>\n",
       "      <td>0.740817</td>\n",
       "      <td>0.680241</td>\n",
       "      <td>0.736413</td>\n",
       "      <td>0.682687</td>\n",
       "      <td>0.712973</td>\n",
       "      <td>0.692845</td>\n",
       "      <td>0.738133</td>\n",
       "      <td>0.687649</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.530905</td>\n",
       "      <td>0.726877</td>\n",
       "      <td>1.539623</td>\n",
       "      <td>0.723319</td>\n",
       "      <td>1.546359</td>\n",
       "      <td>0.717847</td>\n",
       "      <td>1.551087</td>\n",
       "      <td>0.711938</td>\n",
       "      <td>1.554165</td>\n",
       "      <td>0.704144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672679</td>\n",
       "      <td>1.531970</td>\n",
       "      <td>0.667086</td>\n",
       "      <td>1.522015</td>\n",
       "      <td>0.662933</td>\n",
       "      <td>1.503392</td>\n",
       "      <td>0.655053</td>\n",
       "      <td>1.534933</td>\n",
       "      <td>0.627570</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.780776</td>\n",
       "      <td>0.937516</td>\n",
       "      <td>-0.777200</td>\n",
       "      <td>0.951399</td>\n",
       "      <td>-0.775548</td>\n",
       "      <td>0.962227</td>\n",
       "      <td>-0.764915</td>\n",
       "      <td>0.971558</td>\n",
       "      <td>-0.742790</td>\n",
       "      <td>0.970343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937574</td>\n",
       "      <td>-0.611396</td>\n",
       "      <td>0.938397</td>\n",
       "      <td>-0.607111</td>\n",
       "      <td>0.942788</td>\n",
       "      <td>-0.608840</td>\n",
       "      <td>0.952457</td>\n",
       "      <td>-0.611781</td>\n",
       "      <td>0.936083</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.575719</td>\n",
       "      <td>0.859821</td>\n",
       "      <td>2.568802</td>\n",
       "      <td>0.850976</td>\n",
       "      <td>2.560511</td>\n",
       "      <td>0.841711</td>\n",
       "      <td>2.546162</td>\n",
       "      <td>0.831889</td>\n",
       "      <td>2.525103</td>\n",
       "      <td>0.829984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.876698</td>\n",
       "      <td>2.461404</td>\n",
       "      <td>0.879202</td>\n",
       "      <td>2.448506</td>\n",
       "      <td>0.883524</td>\n",
       "      <td>2.416099</td>\n",
       "      <td>0.901521</td>\n",
       "      <td>2.457544</td>\n",
       "      <td>0.876004</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.332992</td>\n",
       "      <td>0.492066</td>\n",
       "      <td>0.332676</td>\n",
       "      <td>0.502047</td>\n",
       "      <td>0.328673</td>\n",
       "      <td>0.506943</td>\n",
       "      <td>0.317754</td>\n",
       "      <td>0.514758</td>\n",
       "      <td>0.310805</td>\n",
       "      <td>0.521839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518020</td>\n",
       "      <td>0.357709</td>\n",
       "      <td>0.519098</td>\n",
       "      <td>0.359956</td>\n",
       "      <td>0.524652</td>\n",
       "      <td>0.359452</td>\n",
       "      <td>0.540035</td>\n",
       "      <td>0.355034</td>\n",
       "      <td>0.522026</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.064595</td>\n",
       "      <td>1.146429</td>\n",
       "      <td>1.054271</td>\n",
       "      <td>1.131820</td>\n",
       "      <td>1.044546</td>\n",
       "      <td>1.114547</td>\n",
       "      <td>1.043038</td>\n",
       "      <td>1.098081</td>\n",
       "      <td>1.036098</td>\n",
       "      <td>1.086502</td>\n",
       "      <td>...</td>\n",
       "      <td>1.130075</td>\n",
       "      <td>0.982244</td>\n",
       "      <td>1.134070</td>\n",
       "      <td>0.983252</td>\n",
       "      <td>1.137040</td>\n",
       "      <td>1.004239</td>\n",
       "      <td>1.139773</td>\n",
       "      <td>0.979553</td>\n",
       "      <td>1.150418</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.603767</td>\n",
       "      <td>0.545589</td>\n",
       "      <td>2.601548</td>\n",
       "      <td>0.556514</td>\n",
       "      <td>2.596773</td>\n",
       "      <td>0.565528</td>\n",
       "      <td>2.590543</td>\n",
       "      <td>0.567339</td>\n",
       "      <td>2.590298</td>\n",
       "      <td>0.567012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522956</td>\n",
       "      <td>2.587218</td>\n",
       "      <td>0.530608</td>\n",
       "      <td>2.573616</td>\n",
       "      <td>0.534529</td>\n",
       "      <td>2.529493</td>\n",
       "      <td>0.553180</td>\n",
       "      <td>2.581088</td>\n",
       "      <td>0.556125</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.795795</td>\n",
       "      <td>0.243442</td>\n",
       "      <td>0.806332</td>\n",
       "      <td>0.313115</td>\n",
       "      <td>0.814110</td>\n",
       "      <td>0.379731</td>\n",
       "      <td>0.817628</td>\n",
       "      <td>0.445745</td>\n",
       "      <td>0.828872</td>\n",
       "      <td>0.504092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409429</td>\n",
       "      <td>0.987911</td>\n",
       "      <td>0.410574</td>\n",
       "      <td>1.004667</td>\n",
       "      <td>0.409417</td>\n",
       "      <td>1.026473</td>\n",
       "      <td>0.431589</td>\n",
       "      <td>0.985220</td>\n",
       "      <td>0.470066</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.169374</td>\n",
       "      <td>0.274520</td>\n",
       "      <td>0.164265</td>\n",
       "      <td>0.301201</td>\n",
       "      <td>0.157893</td>\n",
       "      <td>0.326168</td>\n",
       "      <td>0.150741</td>\n",
       "      <td>0.353728</td>\n",
       "      <td>0.148982</td>\n",
       "      <td>0.378253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394622</td>\n",
       "      <td>0.237563</td>\n",
       "      <td>0.402352</td>\n",
       "      <td>0.243863</td>\n",
       "      <td>0.411064</td>\n",
       "      <td>0.242723</td>\n",
       "      <td>0.438162</td>\n",
       "      <td>0.230357</td>\n",
       "      <td>0.450581</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.215357</td>\n",
       "      <td>0.659541</td>\n",
       "      <td>1.208648</td>\n",
       "      <td>0.668852</td>\n",
       "      <td>1.202459</td>\n",
       "      <td>0.677675</td>\n",
       "      <td>1.192533</td>\n",
       "      <td>0.684004</td>\n",
       "      <td>1.179294</td>\n",
       "      <td>0.691238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710521</td>\n",
       "      <td>1.179465</td>\n",
       "      <td>0.714771</td>\n",
       "      <td>1.174862</td>\n",
       "      <td>0.720550</td>\n",
       "      <td>1.156542</td>\n",
       "      <td>0.745425</td>\n",
       "      <td>1.175636</td>\n",
       "      <td>0.699015</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.203843</td>\n",
       "      <td>2.403359</td>\n",
       "      <td>-1.206415</td>\n",
       "      <td>2.370941</td>\n",
       "      <td>-1.208347</td>\n",
       "      <td>2.333102</td>\n",
       "      <td>-1.207560</td>\n",
       "      <td>2.292661</td>\n",
       "      <td>-1.204975</td>\n",
       "      <td>2.254549</td>\n",
       "      <td>...</td>\n",
       "      <td>2.317989</td>\n",
       "      <td>-1.161122</td>\n",
       "      <td>2.317974</td>\n",
       "      <td>-1.160526</td>\n",
       "      <td>2.322307</td>\n",
       "      <td>-1.158020</td>\n",
       "      <td>2.321174</td>\n",
       "      <td>-1.160360</td>\n",
       "      <td>2.267559</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.738529</td>\n",
       "      <td>2.356742</td>\n",
       "      <td>0.744348</td>\n",
       "      <td>2.307964</td>\n",
       "      <td>0.747435</td>\n",
       "      <td>2.252758</td>\n",
       "      <td>0.746384</td>\n",
       "      <td>2.195715</td>\n",
       "      <td>0.741557</td>\n",
       "      <td>2.138389</td>\n",
       "      <td>...</td>\n",
       "      <td>2.155103</td>\n",
       "      <td>0.697746</td>\n",
       "      <td>2.151898</td>\n",
       "      <td>0.689074</td>\n",
       "      <td>2.152748</td>\n",
       "      <td>0.664058</td>\n",
       "      <td>2.148647</td>\n",
       "      <td>0.699596</td>\n",
       "      <td>2.121421</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.335732</td>\n",
       "      <td>2.411991</td>\n",
       "      <td>1.336126</td>\n",
       "      <td>2.367537</td>\n",
       "      <td>1.333468</td>\n",
       "      <td>2.321385</td>\n",
       "      <td>1.326844</td>\n",
       "      <td>2.269657</td>\n",
       "      <td>1.315505</td>\n",
       "      <td>2.222282</td>\n",
       "      <td>...</td>\n",
       "      <td>2.258758</td>\n",
       "      <td>1.264474</td>\n",
       "      <td>2.260423</td>\n",
       "      <td>1.258269</td>\n",
       "      <td>2.261397</td>\n",
       "      <td>1.239919</td>\n",
       "      <td>2.253806</td>\n",
       "      <td>1.262910</td>\n",
       "      <td>2.243203</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.075878</td>\n",
       "      <td>2.028697</td>\n",
       "      <td>0.074212</td>\n",
       "      <td>1.986269</td>\n",
       "      <td>0.070164</td>\n",
       "      <td>1.939750</td>\n",
       "      <td>0.063146</td>\n",
       "      <td>1.893373</td>\n",
       "      <td>0.054683</td>\n",
       "      <td>1.846378</td>\n",
       "      <td>...</td>\n",
       "      <td>1.885273</td>\n",
       "      <td>0.038075</td>\n",
       "      <td>1.885520</td>\n",
       "      <td>0.036473</td>\n",
       "      <td>1.889355</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>1.887391</td>\n",
       "      <td>0.035407</td>\n",
       "      <td>1.858373</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.682605</td>\n",
       "      <td>2.306672</td>\n",
       "      <td>-0.690655</td>\n",
       "      <td>2.260305</td>\n",
       "      <td>-0.698346</td>\n",
       "      <td>2.209238</td>\n",
       "      <td>-0.705351</td>\n",
       "      <td>2.157922</td>\n",
       "      <td>-0.711357</td>\n",
       "      <td>2.109349</td>\n",
       "      <td>...</td>\n",
       "      <td>2.215980</td>\n",
       "      <td>-0.707740</td>\n",
       "      <td>2.216026</td>\n",
       "      <td>-0.709679</td>\n",
       "      <td>2.223535</td>\n",
       "      <td>-0.720010</td>\n",
       "      <td>2.222587</td>\n",
       "      <td>-0.708122</td>\n",
       "      <td>2.171758</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.180699</td>\n",
       "      <td>2.056322</td>\n",
       "      <td>2.181690</td>\n",
       "      <td>2.018609</td>\n",
       "      <td>2.180350</td>\n",
       "      <td>1.973227</td>\n",
       "      <td>2.173592</td>\n",
       "      <td>1.926236</td>\n",
       "      <td>2.160710</td>\n",
       "      <td>1.878644</td>\n",
       "      <td>...</td>\n",
       "      <td>1.903371</td>\n",
       "      <td>2.086230</td>\n",
       "      <td>1.900319</td>\n",
       "      <td>2.068667</td>\n",
       "      <td>1.904171</td>\n",
       "      <td>2.029227</td>\n",
       "      <td>1.905465</td>\n",
       "      <td>2.085779</td>\n",
       "      <td>1.859997</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.237966</td>\n",
       "      <td>0.923704</td>\n",
       "      <td>2.240166</td>\n",
       "      <td>0.947995</td>\n",
       "      <td>2.241176</td>\n",
       "      <td>0.968923</td>\n",
       "      <td>2.241332</td>\n",
       "      <td>0.986346</td>\n",
       "      <td>2.244532</td>\n",
       "      <td>0.991316</td>\n",
       "      <td>...</td>\n",
       "      <td>1.013258</td>\n",
       "      <td>2.238113</td>\n",
       "      <td>1.020613</td>\n",
       "      <td>2.247879</td>\n",
       "      <td>1.018513</td>\n",
       "      <td>2.271578</td>\n",
       "      <td>0.996822</td>\n",
       "      <td>2.239926</td>\n",
       "      <td>1.033508</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.843712</td>\n",
       "      <td>0.355668</td>\n",
       "      <td>0.846096</td>\n",
       "      <td>0.415241</td>\n",
       "      <td>0.849202</td>\n",
       "      <td>0.473466</td>\n",
       "      <td>0.852666</td>\n",
       "      <td>0.524617</td>\n",
       "      <td>0.864962</td>\n",
       "      <td>0.571852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560798</td>\n",
       "      <td>0.986777</td>\n",
       "      <td>0.568428</td>\n",
       "      <td>1.000159</td>\n",
       "      <td>0.575684</td>\n",
       "      <td>1.014244</td>\n",
       "      <td>0.602473</td>\n",
       "      <td>0.982953</td>\n",
       "      <td>0.591847</td>\n",
       "      <td>-0.688390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>-1.212024</td>\n",
       "      <td>-1.251752</td>\n",
       "      <td>-1.213432</td>\n",
       "      <td>-1.268125</td>\n",
       "      <td>-1.216535</td>\n",
       "      <td>-1.279044</td>\n",
       "      <td>-1.220407</td>\n",
       "      <td>-1.287793</td>\n",
       "      <td>-1.224767</td>\n",
       "      <td>-1.293151</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.281949</td>\n",
       "      <td>-1.244998</td>\n",
       "      <td>-1.281421</td>\n",
       "      <td>-1.247315</td>\n",
       "      <td>-1.281235</td>\n",
       "      <td>-1.256962</td>\n",
       "      <td>-1.277252</td>\n",
       "      <td>-1.245367</td>\n",
       "      <td>-1.288457</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>-1.108010</td>\n",
       "      <td>-1.186143</td>\n",
       "      <td>-1.111683</td>\n",
       "      <td>-1.206850</td>\n",
       "      <td>-1.117108</td>\n",
       "      <td>-1.223807</td>\n",
       "      <td>-1.124637</td>\n",
       "      <td>-1.238498</td>\n",
       "      <td>-1.135124</td>\n",
       "      <td>-1.251204</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.237526</td>\n",
       "      <td>-1.193992</td>\n",
       "      <td>-1.238669</td>\n",
       "      <td>-1.198848</td>\n",
       "      <td>-1.236787</td>\n",
       "      <td>-1.213605</td>\n",
       "      <td>-1.236175</td>\n",
       "      <td>-1.194363</td>\n",
       "      <td>-1.254358</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>-1.152421</td>\n",
       "      <td>-1.039386</td>\n",
       "      <td>-1.151447</td>\n",
       "      <td>-1.031535</td>\n",
       "      <td>-1.151031</td>\n",
       "      <td>-1.022946</td>\n",
       "      <td>-1.149164</td>\n",
       "      <td>-1.013384</td>\n",
       "      <td>-1.147930</td>\n",
       "      <td>-1.004366</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.015409</td>\n",
       "      <td>-1.142987</td>\n",
       "      <td>-1.013399</td>\n",
       "      <td>-1.136857</td>\n",
       "      <td>-1.014550</td>\n",
       "      <td>-1.132451</td>\n",
       "      <td>-0.999566</td>\n",
       "      <td>-1.143359</td>\n",
       "      <td>-0.979944</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>-1.175795</td>\n",
       "      <td>-1.053198</td>\n",
       "      <td>-1.179516</td>\n",
       "      <td>-1.051960</td>\n",
       "      <td>-1.181443</td>\n",
       "      <td>-1.048054</td>\n",
       "      <td>-1.184201</td>\n",
       "      <td>-1.041318</td>\n",
       "      <td>-1.189841</td>\n",
       "      <td>-1.026952</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.003892</td>\n",
       "      <td>-1.191725</td>\n",
       "      <td>-1.003533</td>\n",
       "      <td>-1.188704</td>\n",
       "      <td>-1.001380</td>\n",
       "      <td>-1.185813</td>\n",
       "      <td>-0.989707</td>\n",
       "      <td>-1.194363</td>\n",
       "      <td>-1.002677</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>-0.613652</td>\n",
       "      <td>-0.904715</td>\n",
       "      <td>-0.627501</td>\n",
       "      <td>-0.832391</td>\n",
       "      <td>-0.638690</td>\n",
       "      <td>-0.755132</td>\n",
       "      <td>-0.638779</td>\n",
       "      <td>-0.681465</td>\n",
       "      <td>-0.625207</td>\n",
       "      <td>-0.618781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.528397</td>\n",
       "      <td>-0.442511</td>\n",
       "      <td>-0.520106</td>\n",
       "      <td>-0.439171</td>\n",
       "      <td>-0.514103</td>\n",
       "      <td>-0.440973</td>\n",
       "      <td>-0.509917</td>\n",
       "      <td>-0.447434</td>\n",
       "      <td>-0.530165</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>-1.022696</td>\n",
       "      <td>-1.051472</td>\n",
       "      <td>-1.022800</td>\n",
       "      <td>-1.072385</td>\n",
       "      <td>-1.024700</td>\n",
       "      <td>-1.091574</td>\n",
       "      <td>-1.031203</td>\n",
       "      <td>-1.107045</td>\n",
       "      <td>-1.041988</td>\n",
       "      <td>-1.118912</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.120709</td>\n",
       "      <td>-1.085181</td>\n",
       "      <td>-1.121923</td>\n",
       "      <td>-1.090645</td>\n",
       "      <td>-1.119907</td>\n",
       "      <td>-1.105770</td>\n",
       "      <td>-1.116227</td>\n",
       "      <td>-1.086687</td>\n",
       "      <td>-1.134201</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>-0.799475</td>\n",
       "      <td>-1.048018</td>\n",
       "      <td>-0.800590</td>\n",
       "      <td>-1.070683</td>\n",
       "      <td>-0.804791</td>\n",
       "      <td>-1.091574</td>\n",
       "      <td>-0.810464</td>\n",
       "      <td>-1.108688</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>-1.122138</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.127290</td>\n",
       "      <td>-0.874358</td>\n",
       "      <td>-1.126856</td>\n",
       "      <td>-0.882128</td>\n",
       "      <td>-1.124845</td>\n",
       "      <td>-0.901217</td>\n",
       "      <td>-1.119513</td>\n",
       "      <td>-0.874736</td>\n",
       "      <td>-1.140696</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>-1.172289</td>\n",
       "      <td>-1.046292</td>\n",
       "      <td>-1.174837</td>\n",
       "      <td>-1.040045</td>\n",
       "      <td>-1.176764</td>\n",
       "      <td>-1.031315</td>\n",
       "      <td>-1.178362</td>\n",
       "      <td>-1.019957</td>\n",
       "      <td>-1.184020</td>\n",
       "      <td>-1.002752</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.966050</td>\n",
       "      <td>-1.175857</td>\n",
       "      <td>-0.965714</td>\n",
       "      <td>-1.172925</td>\n",
       "      <td>-0.965163</td>\n",
       "      <td>-1.172472</td>\n",
       "      <td>-0.960131</td>\n",
       "      <td>-1.176228</td>\n",
       "      <td>-0.966954</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>-0.877777</td>\n",
       "      <td>-0.654365</td>\n",
       "      <td>-0.880118</td>\n",
       "      <td>-0.662182</td>\n",
       "      <td>-0.881993</td>\n",
       "      <td>-0.666418</td>\n",
       "      <td>-0.882876</td>\n",
       "      <td>-0.671606</td>\n",
       "      <td>-0.883658</td>\n",
       "      <td>-0.675248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678120</td>\n",
       "      <td>-0.880025</td>\n",
       "      <td>-0.679604</td>\n",
       "      <td>-0.879874</td>\n",
       "      <td>-0.677078</td>\n",
       "      <td>-0.887877</td>\n",
       "      <td>-0.675872</td>\n",
       "      <td>-0.880403</td>\n",
       "      <td>-0.697412</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>-0.942056</td>\n",
       "      <td>-0.562857</td>\n",
       "      <td>-0.936255</td>\n",
       "      <td>-0.561758</td>\n",
       "      <td>-0.932292</td>\n",
       "      <td>-0.557619</td>\n",
       "      <td>-0.930761</td>\n",
       "      <td>-0.553298</td>\n",
       "      <td>-0.929061</td>\n",
       "      <td>-0.549408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569530</td>\n",
       "      <td>-0.924230</td>\n",
       "      <td>-0.572724</td>\n",
       "      <td>-0.918196</td>\n",
       "      <td>-0.571721</td>\n",
       "      <td>-0.904552</td>\n",
       "      <td>-0.554281</td>\n",
       "      <td>-0.921207</td>\n",
       "      <td>-0.522046</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>-0.822849</td>\n",
       "      <td>-1.034206</td>\n",
       "      <td>-0.832168</td>\n",
       "      <td>-1.050258</td>\n",
       "      <td>-0.843392</td>\n",
       "      <td>-1.063118</td>\n",
       "      <td>-0.852510</td>\n",
       "      <td>-1.072538</td>\n",
       "      <td>-0.863866</td>\n",
       "      <td>-1.081805</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.048315</td>\n",
       "      <td>-0.890226</td>\n",
       "      <td>-1.046285</td>\n",
       "      <td>-0.897908</td>\n",
       "      <td>-1.042535</td>\n",
       "      <td>-0.921228</td>\n",
       "      <td>-1.042287</td>\n",
       "      <td>-0.891738</td>\n",
       "      <td>-1.051389</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>-0.963092</td>\n",
       "      <td>-1.020394</td>\n",
       "      <td>-0.970171</td>\n",
       "      <td>-1.043450</td>\n",
       "      <td>-0.977911</td>\n",
       "      <td>-1.061444</td>\n",
       "      <td>-0.985654</td>\n",
       "      <td>-1.072538</td>\n",
       "      <td>-0.997749</td>\n",
       "      <td>-1.080192</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.071350</td>\n",
       "      <td>-1.055711</td>\n",
       "      <td>-1.069305</td>\n",
       "      <td>-1.056832</td>\n",
       "      <td>-1.067228</td>\n",
       "      <td>-1.052409</td>\n",
       "      <td>-1.062004</td>\n",
       "      <td>-1.056085</td>\n",
       "      <td>-1.074122</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>-0.734028</td>\n",
       "      <td>-0.978956</td>\n",
       "      <td>-0.724572</td>\n",
       "      <td>-0.958345</td>\n",
       "      <td>-0.717062</td>\n",
       "      <td>-0.939254</td>\n",
       "      <td>-0.713526</td>\n",
       "      <td>-0.919724</td>\n",
       "      <td>-0.710193</td>\n",
       "      <td>-0.902726</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.895301</td>\n",
       "      <td>-0.686204</td>\n",
       "      <td>-0.895008</td>\n",
       "      <td>-0.684883</td>\n",
       "      <td>-0.897669</td>\n",
       "      <td>-0.685547</td>\n",
       "      <td>-0.905908</td>\n",
       "      <td>-0.685454</td>\n",
       "      <td>-0.913371</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>-0.928031</td>\n",
       "      <td>-0.997948</td>\n",
       "      <td>-0.924560</td>\n",
       "      <td>-1.002599</td>\n",
       "      <td>-0.922934</td>\n",
       "      <td>-1.007882</td>\n",
       "      <td>-0.920250</td>\n",
       "      <td>-1.011741</td>\n",
       "      <td>-0.919748</td>\n",
       "      <td>-1.014046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.970986</td>\n",
       "      <td>-0.941232</td>\n",
       "      <td>-0.970647</td>\n",
       "      <td>-0.945247</td>\n",
       "      <td>-0.971748</td>\n",
       "      <td>-0.949020</td>\n",
       "      <td>-0.986421</td>\n",
       "      <td>-0.941609</td>\n",
       "      <td>-0.988063</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>-0.809993</td>\n",
       "      <td>-0.585303</td>\n",
       "      <td>-0.812286</td>\n",
       "      <td>-0.607715</td>\n",
       "      <td>-0.814149</td>\n",
       "      <td>-0.627920</td>\n",
       "      <td>-0.817472</td>\n",
       "      <td>-0.646959</td>\n",
       "      <td>-0.825448</td>\n",
       "      <td>-0.663954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.650150</td>\n",
       "      <td>-0.868690</td>\n",
       "      <td>-0.650006</td>\n",
       "      <td>-0.875366</td>\n",
       "      <td>-0.649092</td>\n",
       "      <td>-0.892324</td>\n",
       "      <td>-0.649582</td>\n",
       "      <td>-0.869069</td>\n",
       "      <td>-0.669808</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>-0.928031</td>\n",
       "      <td>-0.997948</td>\n",
       "      <td>-0.924560</td>\n",
       "      <td>-1.002599</td>\n",
       "      <td>-0.922934</td>\n",
       "      <td>-1.007882</td>\n",
       "      <td>-0.920250</td>\n",
       "      <td>-1.011741</td>\n",
       "      <td>-0.919748</td>\n",
       "      <td>-1.014046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.970986</td>\n",
       "      <td>-0.941232</td>\n",
       "      <td>-0.970647</td>\n",
       "      <td>-0.945247</td>\n",
       "      <td>-0.971748</td>\n",
       "      <td>-0.949020</td>\n",
       "      <td>-0.986421</td>\n",
       "      <td>-0.941609</td>\n",
       "      <td>-0.988063</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>-1.057756</td>\n",
       "      <td>-1.136073</td>\n",
       "      <td>-1.049699</td>\n",
       "      <td>-1.138767</td>\n",
       "      <td>-1.042246</td>\n",
       "      <td>-1.140115</td>\n",
       "      <td>-1.033539</td>\n",
       "      <td>-1.143194</td>\n",
       "      <td>-1.024525</td>\n",
       "      <td>-1.144725</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.156906</td>\n",
       "      <td>-1.022840</td>\n",
       "      <td>-1.161387</td>\n",
       "      <td>-1.028654</td>\n",
       "      <td>-1.167647</td>\n",
       "      <td>-1.042403</td>\n",
       "      <td>-1.178665</td>\n",
       "      <td>-1.022082</td>\n",
       "      <td>-1.176418</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>-0.873103</td>\n",
       "      <td>-0.808028</td>\n",
       "      <td>-0.874270</td>\n",
       "      <td>-0.810263</td>\n",
       "      <td>-0.873805</td>\n",
       "      <td>-0.810369</td>\n",
       "      <td>-0.875868</td>\n",
       "      <td>-0.809632</td>\n",
       "      <td>-0.877837</td>\n",
       "      <td>-0.805927</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.781775</td>\n",
       "      <td>-0.884559</td>\n",
       "      <td>-0.781551</td>\n",
       "      <td>-0.882128</td>\n",
       "      <td>-0.782435</td>\n",
       "      <td>-0.875648</td>\n",
       "      <td>-0.787604</td>\n",
       "      <td>-0.883804</td>\n",
       "      <td>-0.799708</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>-0.273562</td>\n",
       "      <td>-0.963417</td>\n",
       "      <td>-0.274306</td>\n",
       "      <td>-0.999195</td>\n",
       "      <td>-0.277245</td>\n",
       "      <td>-1.029642</td>\n",
       "      <td>-0.280225</td>\n",
       "      <td>-1.054463</td>\n",
       "      <td>-0.287590</td>\n",
       "      <td>-1.075352</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.064768</td>\n",
       "      <td>-0.409641</td>\n",
       "      <td>-1.067661</td>\n",
       "      <td>-0.424518</td>\n",
       "      <td>-1.065582</td>\n",
       "      <td>-0.437638</td>\n",
       "      <td>-1.070220</td>\n",
       "      <td>-0.407764</td>\n",
       "      <td>-1.083865</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>-0.849729</td>\n",
       "      <td>-1.054925</td>\n",
       "      <td>-0.854388</td>\n",
       "      <td>-1.058768</td>\n",
       "      <td>-0.858599</td>\n",
       "      <td>-1.058097</td>\n",
       "      <td>-0.863021</td>\n",
       "      <td>-1.056106</td>\n",
       "      <td>-0.865031</td>\n",
       "      <td>-1.055992</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.015409</td>\n",
       "      <td>-0.859623</td>\n",
       "      <td>-1.013399</td>\n",
       "      <td>-0.866349</td>\n",
       "      <td>-1.012903</td>\n",
       "      <td>-0.887877</td>\n",
       "      <td>-1.022569</td>\n",
       "      <td>-0.862269</td>\n",
       "      <td>-1.035152</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>-0.039822</td>\n",
       "      <td>-1.051472</td>\n",
       "      <td>-0.036893</td>\n",
       "      <td>-1.074087</td>\n",
       "      <td>-0.035112</td>\n",
       "      <td>-1.094921</td>\n",
       "      <td>-0.037296</td>\n",
       "      <td>-1.113617</td>\n",
       "      <td>-0.045438</td>\n",
       "      <td>-1.125365</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.117418</td>\n",
       "      <td>-0.145545</td>\n",
       "      <td>-1.118635</td>\n",
       "      <td>-0.156264</td>\n",
       "      <td>-1.119907</td>\n",
       "      <td>-0.166383</td>\n",
       "      <td>-1.135944</td>\n",
       "      <td>-0.144808</td>\n",
       "      <td>-1.135825</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>-0.928031</td>\n",
       "      <td>-0.997948</td>\n",
       "      <td>-0.924560</td>\n",
       "      <td>-1.002599</td>\n",
       "      <td>-0.922934</td>\n",
       "      <td>-1.007882</td>\n",
       "      <td>-0.920250</td>\n",
       "      <td>-1.011741</td>\n",
       "      <td>-0.919748</td>\n",
       "      <td>-1.014046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.970986</td>\n",
       "      <td>-0.941232</td>\n",
       "      <td>-0.970647</td>\n",
       "      <td>-0.945247</td>\n",
       "      <td>-0.971748</td>\n",
       "      <td>-0.949020</td>\n",
       "      <td>-0.986421</td>\n",
       "      <td>-0.941609</td>\n",
       "      <td>-0.988063</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>-1.091649</td>\n",
       "      <td>-1.122260</td>\n",
       "      <td>-1.091801</td>\n",
       "      <td>-1.096214</td>\n",
       "      <td>-1.092544</td>\n",
       "      <td>-1.066466</td>\n",
       "      <td>-1.094271</td>\n",
       "      <td>-1.039675</td>\n",
       "      <td>-1.086228</td>\n",
       "      <td>-1.018885</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.084512</td>\n",
       "      <td>-1.004705</td>\n",
       "      <td>-1.084104</td>\n",
       "      <td>-1.002730</td>\n",
       "      <td>-1.080398</td>\n",
       "      <td>-1.019058</td>\n",
       "      <td>-1.043930</td>\n",
       "      <td>-1.006214</td>\n",
       "      <td>-1.030281</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>-1.076456</td>\n",
       "      <td>-0.973777</td>\n",
       "      <td>-1.073089</td>\n",
       "      <td>-0.905580</td>\n",
       "      <td>-1.069150</td>\n",
       "      <td>-0.838824</td>\n",
       "      <td>-1.059233</td>\n",
       "      <td>-0.778412</td>\n",
       "      <td>-1.043152</td>\n",
       "      <td>-0.725261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.673184</td>\n",
       "      <td>-0.899294</td>\n",
       "      <td>-0.666449</td>\n",
       "      <td>-0.887764</td>\n",
       "      <td>-0.667200</td>\n",
       "      <td>-0.870090</td>\n",
       "      <td>-0.667656</td>\n",
       "      <td>-0.900805</td>\n",
       "      <td>-0.703907</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>-1.013346</td>\n",
       "      <td>-0.814934</td>\n",
       "      <td>-1.011104</td>\n",
       "      <td>-0.830688</td>\n",
       "      <td>-1.011833</td>\n",
       "      <td>-0.843845</td>\n",
       "      <td>-1.011348</td>\n",
       "      <td>-0.855640</td>\n",
       "      <td>-1.014047</td>\n",
       "      <td>-0.867233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.875558</td>\n",
       "      <td>-1.046643</td>\n",
       "      <td>-0.880210</td>\n",
       "      <td>-1.048942</td>\n",
       "      <td>-0.884499</td>\n",
       "      <td>-1.045738</td>\n",
       "      <td>-0.900979</td>\n",
       "      <td>-1.043617</td>\n",
       "      <td>-0.890638</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>-0.908163</td>\n",
       "      <td>-0.122587</td>\n",
       "      <td>-0.908186</td>\n",
       "      <td>-0.115811</td>\n",
       "      <td>-0.905388</td>\n",
       "      <td>-0.104008</td>\n",
       "      <td>-0.907402</td>\n",
       "      <td>-0.094856</td>\n",
       "      <td>-0.910434</td>\n",
       "      <td>-0.076704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067710</td>\n",
       "      <td>-0.876625</td>\n",
       "      <td>-0.066276</td>\n",
       "      <td>-0.871984</td>\n",
       "      <td>-0.067982</td>\n",
       "      <td>-0.874536</td>\n",
       "      <td>-0.056416</td>\n",
       "      <td>-0.878137</td>\n",
       "      <td>-0.090128</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>-0.624170</td>\n",
       "      <td>-1.041112</td>\n",
       "      <td>-0.621654</td>\n",
       "      <td>-1.048556</td>\n",
       "      <td>-0.621144</td>\n",
       "      <td>-1.054749</td>\n",
       "      <td>-0.622428</td>\n",
       "      <td>-1.057750</td>\n",
       "      <td>-0.625207</td>\n",
       "      <td>-1.059219</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.074640</td>\n",
       "      <td>-0.643133</td>\n",
       "      <td>-1.075883</td>\n",
       "      <td>-0.645433</td>\n",
       "      <td>-1.077105</td>\n",
       "      <td>-0.645526</td>\n",
       "      <td>-1.083365</td>\n",
       "      <td>-0.641250</td>\n",
       "      <td>-1.083865</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>-1.076456</td>\n",
       "      <td>-0.973777</td>\n",
       "      <td>-1.073089</td>\n",
       "      <td>-0.905580</td>\n",
       "      <td>-1.069150</td>\n",
       "      <td>-0.838824</td>\n",
       "      <td>-1.059233</td>\n",
       "      <td>-0.778412</td>\n",
       "      <td>-1.043152</td>\n",
       "      <td>-0.725261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.673184</td>\n",
       "      <td>-0.899294</td>\n",
       "      <td>-0.666449</td>\n",
       "      <td>-0.887764</td>\n",
       "      <td>-0.667200</td>\n",
       "      <td>-0.870090</td>\n",
       "      <td>-0.667656</td>\n",
       "      <td>-0.900805</td>\n",
       "      <td>-0.703907</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>-0.745715</td>\n",
       "      <td>-1.068737</td>\n",
       "      <td>-0.739775</td>\n",
       "      <td>-1.062173</td>\n",
       "      <td>-0.734608</td>\n",
       "      <td>-1.049728</td>\n",
       "      <td>-0.731045</td>\n",
       "      <td>-1.033102</td>\n",
       "      <td>-0.727656</td>\n",
       "      <td>-1.015659</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.071350</td>\n",
       "      <td>-0.758745</td>\n",
       "      <td>-1.072594</td>\n",
       "      <td>-0.754764</td>\n",
       "      <td>-1.078752</td>\n",
       "      <td>-0.738909</td>\n",
       "      <td>-1.098153</td>\n",
       "      <td>-0.753459</td>\n",
       "      <td>-1.075746</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>-0.825186</td>\n",
       "      <td>-0.890902</td>\n",
       "      <td>-0.821642</td>\n",
       "      <td>-0.878347</td>\n",
       "      <td>-0.815319</td>\n",
       "      <td>-0.863931</td>\n",
       "      <td>-0.809296</td>\n",
       "      <td>-0.850711</td>\n",
       "      <td>-0.806821</td>\n",
       "      <td>-0.836580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.829489</td>\n",
       "      <td>-0.785948</td>\n",
       "      <td>-0.832525</td>\n",
       "      <td>-0.784069</td>\n",
       "      <td>-0.833467</td>\n",
       "      <td>-0.784489</td>\n",
       "      <td>-0.831968</td>\n",
       "      <td>-0.784062</td>\n",
       "      <td>-0.850044</td>\n",
       "      <td>1.452665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1185 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x1        y1        x2        y2        x3        y3        x4  \\\n",
       "0     1.361444  1.510731  1.347821  1.526705  1.338147  1.543050  1.331516   \n",
       "1     0.336498  1.483107  0.304608  1.521599  0.274866  1.553093  0.247679   \n",
       "2     2.507934  0.970321  2.511495  0.988845  2.513722  1.002400  2.511124   \n",
       "3     3.811030  1.151609  3.804986  1.167564  3.802761  1.186522  3.805188   \n",
       "4    -0.896476  0.954782 -0.889474  0.966718 -0.880824  0.978966 -0.872365   \n",
       "5    -0.057353  1.194772 -0.054436  1.186287 -0.047979  1.173131 -0.033792   \n",
       "6     1.920080  0.013811  1.918548  0.011846  1.917162  0.008139  1.918984   \n",
       "7     3.354070  0.188193  3.351212  0.176948  3.346567  0.168828  3.339185   \n",
       "8    -0.141499  0.162294 -0.139811  0.165034 -0.140387  0.168828 -0.140074   \n",
       "9     1.175621  0.212364  1.179410  0.192267  1.181404  0.168828  1.179685   \n",
       "10    0.414800  1.350162  0.415712  1.347986  0.421082  1.343863  0.422868   \n",
       "11   -0.188247  0.562854 -0.184252  0.571833 -0.181327  0.580592 -0.175111   \n",
       "12    0.684769  0.640549  0.689380  0.650129  0.690119  0.657589  0.687988   \n",
       "13    1.530905  0.726877  1.539623  0.723319  1.546359  0.717847  1.551087   \n",
       "14   -0.780776  0.937516 -0.777200  0.951399 -0.775548  0.962227 -0.764915   \n",
       "15    2.575719  0.859821  2.568802  0.850976  2.560511  0.841711  2.546162   \n",
       "16    0.332992  0.492066  0.332676  0.502047  0.328673  0.506943  0.317754   \n",
       "17    1.064595  1.146429  1.054271  1.131820  1.044546  1.114547  1.043038   \n",
       "18    2.603767  0.545589  2.601548  0.556514  2.596773  0.565528  2.590543   \n",
       "19    0.795795  0.243442  0.806332  0.313115  0.814110  0.379731  0.817628   \n",
       "20    0.169374  0.274520  0.164265  0.301201  0.157893  0.326168  0.150741   \n",
       "21    1.215357  0.659541  1.208648  0.668852  1.202459  0.677675  1.192533   \n",
       "22   -1.203843  2.403359 -1.206415  2.370941 -1.208347  2.333102 -1.207560   \n",
       "23    0.738529  2.356742  0.744348  2.307964  0.747435  2.252758  0.746384   \n",
       "24    1.335732  2.411991  1.336126  2.367537  1.333468  2.321385  1.326844   \n",
       "25    0.075878  2.028697  0.074212  1.986269  0.070164  1.939750  0.063146   \n",
       "26   -0.682605  2.306672 -0.690655  2.260305 -0.698346  2.209238 -0.705351   \n",
       "27    2.180699  2.056322  2.181690  2.018609  2.180350  1.973227  2.173592   \n",
       "28    2.237966  0.923704  2.240166  0.947995  2.241176  0.968923  2.241332   \n",
       "29    0.843712  0.355668  0.846096  0.415241  0.849202  0.473466  0.852666   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1155 -1.212024 -1.251752 -1.213432 -1.268125 -1.216535 -1.279044 -1.220407   \n",
       "1156 -1.108010 -1.186143 -1.111683 -1.206850 -1.117108 -1.223807 -1.124637   \n",
       "1157 -1.152421 -1.039386 -1.151447 -1.031535 -1.151031 -1.022946 -1.149164   \n",
       "1158 -1.175795 -1.053198 -1.179516 -1.051960 -1.181443 -1.048054 -1.184201   \n",
       "1159 -0.613652 -0.904715 -0.627501 -0.832391 -0.638690 -0.755132 -0.638779   \n",
       "1160 -1.022696 -1.051472 -1.022800 -1.072385 -1.024700 -1.091574 -1.031203   \n",
       "1161 -0.799475 -1.048018 -0.800590 -1.070683 -0.804791 -1.091574 -0.810464   \n",
       "1162 -1.172289 -1.046292 -1.174837 -1.040045 -1.176764 -1.031315 -1.178362   \n",
       "1163 -0.877777 -0.654365 -0.880118 -0.662182 -0.881993 -0.666418 -0.882876   \n",
       "1164 -0.942056 -0.562857 -0.936255 -0.561758 -0.932292 -0.557619 -0.930761   \n",
       "1165 -0.822849 -1.034206 -0.832168 -1.050258 -0.843392 -1.063118 -0.852510   \n",
       "1166 -0.963092 -1.020394 -0.970171 -1.043450 -0.977911 -1.061444 -0.985654   \n",
       "1167 -0.734028 -0.978956 -0.724572 -0.958345 -0.717062 -0.939254 -0.713526   \n",
       "1168 -0.928031 -0.997948 -0.924560 -1.002599 -0.922934 -1.007882 -0.920250   \n",
       "1169 -0.809993 -0.585303 -0.812286 -0.607715 -0.814149 -0.627920 -0.817472   \n",
       "1170 -0.928031 -0.997948 -0.924560 -1.002599 -0.922934 -1.007882 -0.920250   \n",
       "1171 -1.057756 -1.136073 -1.049699 -1.138767 -1.042246 -1.140115 -1.033539   \n",
       "1172 -0.873103 -0.808028 -0.874270 -0.810263 -0.873805 -0.810369 -0.875868   \n",
       "1173 -0.273562 -0.963417 -0.274306 -0.999195 -0.277245 -1.029642 -0.280225   \n",
       "1174 -0.849729 -1.054925 -0.854388 -1.058768 -0.858599 -1.058097 -0.863021   \n",
       "1175 -0.039822 -1.051472 -0.036893 -1.074087 -0.035112 -1.094921 -0.037296   \n",
       "1176 -0.928031 -0.997948 -0.924560 -1.002599 -0.922934 -1.007882 -0.920250   \n",
       "1177 -1.091649 -1.122260 -1.091801 -1.096214 -1.092544 -1.066466 -1.094271   \n",
       "1178 -1.076456 -0.973777 -1.073089 -0.905580 -1.069150 -0.838824 -1.059233   \n",
       "1179 -1.013346 -0.814934 -1.011104 -0.830688 -1.011833 -0.843845 -1.011348   \n",
       "1180 -0.908163 -0.122587 -0.908186 -0.115811 -0.905388 -0.104008 -0.907402   \n",
       "1181 -0.624170 -1.041112 -0.621654 -1.048556 -0.621144 -1.054749 -0.622428   \n",
       "1182 -1.076456 -0.973777 -1.073089 -0.905580 -1.069150 -0.838824 -1.059233   \n",
       "1183 -0.745715 -1.068737 -0.739775 -1.062173 -0.734608 -1.049728 -0.731045   \n",
       "1184 -0.825186 -0.890902 -0.821642 -0.878347 -0.815319 -0.863931 -0.809296   \n",
       "\n",
       "            y4        x5        y5  ...       y62       x63       y63  \\\n",
       "0     1.559810  1.321326  1.585019  ...  1.628604  1.334749  1.638874   \n",
       "1     1.587744  0.230476  1.625352  ...  1.840849  0.289702  1.859211   \n",
       "2     1.014280  2.508804  1.018743  ...  1.064263  2.507876  1.066654   \n",
       "3     1.196671  3.802224  1.202661  ...  1.241956  3.716140  1.247528   \n",
       "4     0.987989 -0.853389  0.996156  ...  0.977061 -0.717941  0.976217   \n",
       "5     1.157235 -0.018661  1.141355  ...  1.158046 -0.059403  1.148869   \n",
       "6     0.003734  1.909244 -0.000878  ... -0.026578  1.836870 -0.031746   \n",
       "7     0.164764  3.326068  0.162068  ...  0.164278  3.232154  0.165572   \n",
       "8     0.171337 -0.132753  0.170134  ...  0.198830 -0.085472  0.201746   \n",
       "9     0.145046  1.175802  0.120121  ...  0.174150  1.114858  0.172149   \n",
       "10    1.337983  0.424896  1.330114  ...  1.335739  0.438185  1.336320   \n",
       "11    0.585414 -0.163022  0.583145  ...  0.587123 -0.082072  0.584871   \n",
       "12    0.664286  0.689169  0.671878  ...  0.677615  0.740817  0.680241   \n",
       "13    0.711938  1.554165  0.704144  ...  0.672679  1.531970  0.667086   \n",
       "14    0.971558 -0.742790  0.970343  ...  0.937574 -0.611396  0.938397   \n",
       "15    0.831889  2.525103  0.829984  ...  0.876698  2.461404  0.879202   \n",
       "16    0.514758  0.310805  0.521839  ...  0.518020  0.357709  0.519098   \n",
       "17    1.098081  1.036098  1.086502  ...  1.130075  0.982244  1.134070   \n",
       "18    0.567339  2.590298  0.567012  ...  0.522956  2.587218  0.530608   \n",
       "19    0.445745  0.828872  0.504092  ...  0.409429  0.987911  0.410574   \n",
       "20    0.353728  0.148982  0.378253  ...  0.394622  0.237563  0.402352   \n",
       "21    0.684004  1.179294  0.691238  ...  0.710521  1.179465  0.714771   \n",
       "22    2.292661 -1.204975  2.254549  ...  2.317989 -1.161122  2.317974   \n",
       "23    2.195715  0.741557  2.138389  ...  2.155103  0.697746  2.151898   \n",
       "24    2.269657  1.315505  2.222282  ...  2.258758  1.264474  2.260423   \n",
       "25    1.893373  0.054683  1.846378  ...  1.885273  0.038075  1.885520   \n",
       "26    2.157922 -0.711357  2.109349  ...  2.215980 -0.707740  2.216026   \n",
       "27    1.926236  2.160710  1.878644  ...  1.903371  2.086230  1.900319   \n",
       "28    0.986346  2.244532  0.991316  ...  1.013258  2.238113  1.020613   \n",
       "29    0.524617  0.864962  0.571852  ...  0.560798  0.986777  0.568428   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1155 -1.287793 -1.224767 -1.293151  ... -1.281949 -1.244998 -1.281421   \n",
       "1156 -1.238498 -1.135124 -1.251204  ... -1.237526 -1.193992 -1.238669   \n",
       "1157 -1.013384 -1.147930 -1.004366  ... -1.015409 -1.142987 -1.013399   \n",
       "1158 -1.041318 -1.189841 -1.026952  ... -1.003892 -1.191725 -1.003533   \n",
       "1159 -0.681465 -0.625207 -0.618781  ... -0.528397 -0.442511 -0.520106   \n",
       "1160 -1.107045 -1.041988 -1.118912  ... -1.120709 -1.085181 -1.121923   \n",
       "1161 -1.108688 -0.821955 -1.122138  ... -1.127290 -0.874358 -1.126856   \n",
       "1162 -1.019957 -1.184020 -1.002752  ... -0.966050 -1.175857 -0.965714   \n",
       "1163 -0.671606 -0.883658 -0.675248  ... -0.678120 -0.880025 -0.679604   \n",
       "1164 -0.553298 -0.929061 -0.549408  ... -0.569530 -0.924230 -0.572724   \n",
       "1165 -1.072538 -0.863866 -1.081805  ... -1.048315 -0.890226 -1.046285   \n",
       "1166 -1.072538 -0.997749 -1.080192  ... -1.071350 -1.055711 -1.069305   \n",
       "1167 -0.919724 -0.710193 -0.902726  ... -0.895301 -0.686204 -0.895008   \n",
       "1168 -1.011741 -0.919748 -1.014046  ... -0.970986 -0.941232 -0.970647   \n",
       "1169 -0.646959 -0.825448 -0.663954  ... -0.650150 -0.868690 -0.650006   \n",
       "1170 -1.011741 -0.919748 -1.014046  ... -0.970986 -0.941232 -0.970647   \n",
       "1171 -1.143194 -1.024525 -1.144725  ... -1.156906 -1.022840 -1.161387   \n",
       "1172 -0.809632 -0.877837 -0.805927  ... -0.781775 -0.884559 -0.781551   \n",
       "1173 -1.054463 -0.287590 -1.075352  ... -1.064768 -0.409641 -1.067661   \n",
       "1174 -1.056106 -0.865031 -1.055992  ... -1.015409 -0.859623 -1.013399   \n",
       "1175 -1.113617 -0.045438 -1.125365  ... -1.117418 -0.145545 -1.118635   \n",
       "1176 -1.011741 -0.919748 -1.014046  ... -0.970986 -0.941232 -0.970647   \n",
       "1177 -1.039675 -1.086228 -1.018885  ... -1.084512 -1.004705 -1.084104   \n",
       "1178 -0.778412 -1.043152 -0.725261  ... -0.673184 -0.899294 -0.666449   \n",
       "1179 -0.855640 -1.014047 -0.867233  ... -0.875558 -1.046643 -0.880210   \n",
       "1180 -0.094856 -0.910434 -0.076704  ... -0.067710 -0.876625 -0.066276   \n",
       "1181 -1.057750 -0.625207 -1.059219  ... -1.074640 -0.643133 -1.075883   \n",
       "1182 -0.778412 -1.043152 -0.725261  ... -0.673184 -0.899294 -0.666449   \n",
       "1183 -1.033102 -0.727656 -1.015659  ... -1.071350 -0.758745 -1.072594   \n",
       "1184 -0.850711 -0.806821 -0.836580  ... -0.829489 -0.785948 -0.832525   \n",
       "\n",
       "           x64       y64       x65       y65       x67       y67    target  \n",
       "0     1.351820  1.645717  1.414456  1.668856  1.330916  1.686256 -0.688390  \n",
       "1     0.300219  1.874539  0.329436  1.892320  0.280228  1.817780 -0.688390  \n",
       "2     2.504862  1.066253  2.486136  1.049401  2.507415  1.027013 -0.688390  \n",
       "3     3.715387  1.248982  3.705673  1.261363  3.717917  1.280318 -0.688390  \n",
       "4    -0.706298  0.975712 -0.686659  0.977104 -0.717190  0.991290 -0.688390  \n",
       "5    -0.050315  1.137040 -0.006298  1.095409 -0.048467  1.147170 -0.688390  \n",
       "6     1.826337 -0.035058  1.811334 -0.031770  1.838692 -0.039792 -0.688390  \n",
       "7     3.217200  0.164133  3.194290  0.158832  3.233943  0.182662 -0.688390  \n",
       "8    -0.085256  0.201996 -0.095234  0.186765 -0.089270  0.189157 -0.688390  \n",
       "9     1.097091  0.172364  1.055377  0.155546  1.115564  0.135573 -0.688390  \n",
       "10    0.445617  1.334584  0.459505  1.323802  0.440041  1.351763 -0.688390  \n",
       "11   -0.084129  0.585561 -0.096346  0.581113 -0.080203  0.557748 -0.688390  \n",
       "12    0.736413  0.682687  0.712973  0.692845  0.738133  0.687649 -0.688390  \n",
       "13    1.522015  0.662933  1.503392  0.655053  1.534933  0.627570 -0.688390  \n",
       "14   -0.607111  0.942788 -0.608840  0.952457 -0.611781  0.936083 -0.688390  \n",
       "15    2.448506  0.883524  2.416099  0.901521  2.457544  0.876004 -0.688390  \n",
       "16    0.359956  0.524652  0.359452  0.540035  0.355034  0.522026 -0.688390  \n",
       "17    0.983252  1.137040  1.004239  1.139773  0.979553  1.150418 -0.688390  \n",
       "18    2.573616  0.534529  2.529493  0.553180  2.581088  0.556125 -0.688390  \n",
       "19    1.004667  0.409417  1.026473  0.431589  0.985220  0.470066 -0.688390  \n",
       "20    0.243863  0.411064  0.242723  0.438162  0.230357  0.450581 -0.688390  \n",
       "21    1.174862  0.720550  1.156542  0.745425  1.175636  0.699015 -0.688390  \n",
       "22   -1.160526  2.322307 -1.158020  2.321174 -1.160360  2.267559 -0.688390  \n",
       "23    0.689074  2.152748  0.664058  2.148647  0.699596  2.121421 -0.688390  \n",
       "24    1.258269  2.261397  1.239919  2.253806  1.262910  2.243203 -0.688390  \n",
       "25    0.036473  1.889355  0.031500  1.887391  0.035407  1.858373 -0.688390  \n",
       "26   -0.709679  2.223535 -0.720010  2.222587 -0.708122  2.171758 -0.688390  \n",
       "27    2.068667  1.904171  2.029227  1.905465  2.085779  1.859997 -0.688390  \n",
       "28    2.247879  1.018513  2.271578  0.996822  2.239926  1.033508 -0.688390  \n",
       "29    1.000159  0.575684  1.014244  0.602473  0.982953  0.591847 -0.688390  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1155 -1.247315 -1.281235 -1.256962 -1.277252 -1.245367 -1.288457  1.452665  \n",
       "1156 -1.198848 -1.236787 -1.213605 -1.236175 -1.194363 -1.254358  1.452665  \n",
       "1157 -1.136857 -1.014550 -1.132451 -0.999566 -1.143359 -0.979944  1.452665  \n",
       "1158 -1.188704 -1.001380 -1.185813 -0.989707 -1.194363 -1.002677  1.452665  \n",
       "1159 -0.439171 -0.514103 -0.440973 -0.509917 -0.447434 -0.530165  1.452665  \n",
       "1160 -1.090645 -1.119907 -1.105770 -1.116227 -1.086687 -1.134201  1.452665  \n",
       "1161 -0.882128 -1.124845 -0.901217 -1.119513 -0.874736 -1.140696  1.452665  \n",
       "1162 -1.172925 -0.965163 -1.172472 -0.960131 -1.176228 -0.966954  1.452665  \n",
       "1163 -0.879874 -0.677078 -0.887877 -0.675872 -0.880403 -0.697412  1.452665  \n",
       "1164 -0.918196 -0.571721 -0.904552 -0.554281 -0.921207 -0.522046  1.452665  \n",
       "1165 -0.897908 -1.042535 -0.921228 -1.042287 -0.891738 -1.051389  1.452665  \n",
       "1166 -1.056832 -1.067228 -1.052409 -1.062004 -1.056085 -1.074122  1.452665  \n",
       "1167 -0.684883 -0.897669 -0.685547 -0.905908 -0.685454 -0.913371  1.452665  \n",
       "1168 -0.945247 -0.971748 -0.949020 -0.986421 -0.941609 -0.988063  1.452665  \n",
       "1169 -0.875366 -0.649092 -0.892324 -0.649582 -0.869069 -0.669808  1.452665  \n",
       "1170 -0.945247 -0.971748 -0.949020 -0.986421 -0.941609 -0.988063  1.452665  \n",
       "1171 -1.028654 -1.167647 -1.042403 -1.178665 -1.022082 -1.176418  1.452665  \n",
       "1172 -0.882128 -0.782435 -0.875648 -0.787604 -0.883804 -0.799708  1.452665  \n",
       "1173 -0.424518 -1.065582 -0.437638 -1.070220 -0.407764 -1.083865  1.452665  \n",
       "1174 -0.866349 -1.012903 -0.887877 -1.022569 -0.862269 -1.035152  1.452665  \n",
       "1175 -0.156264 -1.119907 -0.166383 -1.135944 -0.144808 -1.135825  1.452665  \n",
       "1176 -0.945247 -0.971748 -0.949020 -0.986421 -0.941609 -0.988063  1.452665  \n",
       "1177 -1.002730 -1.080398 -1.019058 -1.043930 -1.006214 -1.030281  1.452665  \n",
       "1178 -0.887764 -0.667200 -0.870090 -0.667656 -0.900805 -0.703907  1.452665  \n",
       "1179 -1.048942 -0.884499 -1.045738 -0.900979 -1.043617 -0.890638  1.452665  \n",
       "1180 -0.871984 -0.067982 -0.874536 -0.056416 -0.878137 -0.090128  1.452665  \n",
       "1181 -0.645433 -1.077105 -0.645526 -1.083365 -0.641250 -1.083865  1.452665  \n",
       "1182 -0.887764 -0.667200 -0.870090 -0.667656 -0.900805 -0.703907  1.452665  \n",
       "1183 -0.754764 -1.078752 -0.738909 -1.098153 -0.753459 -1.075746  1.452665  \n",
       "1184 -0.784069 -0.833467 -0.784489 -0.831968 -0.784062 -0.850044  1.452665  \n",
       "\n",
       "[1185 rows x 101 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_streamed_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required ML packages\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression\n",
    "from sklearn import svm #support vector Machine\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
    "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
    "from sklearn.naive_bayes import GaussianNB #Naive bayes\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:136], y, test_size=0.25, random_state=27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainb, X_testb, y_trainb, y_testb = train_test_split(df_streamed_ft.iloc[:,:100], y, test_size=0.25, random_state=27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for rbf SVM is  0.835016835016835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model1=svm.SVC(kernel='rbf',C=1,gamma=0.1)\n",
    "model1.fit(X_train,y_train)\n",
    "prediction1= model1.predict(X_test)\n",
    "print('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction1,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for rbf SVM is  0.8282828282828283\n"
     ]
    }
   ],
   "source": [
    "model1b=svm.SVC(kernel='rbf',C=1,gamma=0.1)\n",
    "model1b.fit(X_trainb,y_trainb)\n",
    "prediction1b=model1b.predict(X_testb)\n",
    "print('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction1b,y_testb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Logistic Regression is 0.835016835016835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train,y_train)\n",
    "prediction2=model2.predict(X_test)\n",
    "print('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction2,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Logistic Regression is 0.8282828282828283\n"
     ]
    }
   ],
   "source": [
    "model2b = LogisticRegression()\n",
    "model2b.fit(X_trainb,y_trainb)\n",
    "prediction2b = model2b.predict(X_testb)\n",
    "print('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction2b,y_testb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Decision Tree is 0.7845117845117845\n"
     ]
    }
   ],
   "source": [
    "model3=DecisionTreeClassifier()\n",
    "model3.fit(X_train,y_train)\n",
    "prediction3=model3.predict(X_test)\n",
    "print('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction3,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Decision Tree is 0.7811447811447811\n"
     ]
    }
   ],
   "source": [
    "model3b=DecisionTreeClassifier()\n",
    "model3b.fit(X_trainb,y_trainb)\n",
    "prediction3b=model3b.predict(X_testb)\n",
    "print('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction3b,y_testb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the KNN is 0.8080808080808081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model4=KNeighborsClassifier() \n",
    "model4.fit(X_train,y_train)\n",
    "prediction4=model4.predict(X_test)\n",
    "print('The accuracy of the KNN is',metrics.accuracy_score(prediction4,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the KNN is 0.8114478114478114\n"
     ]
    }
   ],
   "source": [
    "model4b=KNeighborsClassifier() \n",
    "model4b.fit(X_trainb,y_trainb)\n",
    "prediction4b = model4b.predict(X_testb)\n",
    "print('The accuracy of the KNN is',metrics.accuracy_score(prediction4b,y_testb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Naive Bayes is 0.8114478114478114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model7 = GaussianNB() \n",
    "model7.fit(X_train,y_train)\n",
    "prediction7 = model7.predict(X_test)\n",
    "print('The accuracy of the Naive Bayes is',metrics.accuracy_score(prediction7,y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Naive Bayes is 0.8080808080808081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model7b = GaussianNB() \n",
    "model7b.fit(X_trainb,y_trainb)\n",
    "prediction7b = model7b.predict(X_testb)\n",
    "print('The accuracy of the Naive Bayes is',metrics.accuracy_score(prediction7b,y_testb)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the XGboost is 0.8114478114478114\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model5 = XGBClassifier()\n",
    "model5.fit(X_train,y_train)\n",
    "prediciton5 = model5.predict(X_test)\n",
    "print('The accuracy of the XGboost is',metrics.accuracy_score(prediciton5,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the XGboost is 0.8047138047138047\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model5b = XGBClassifier()\n",
    "model5b.fit(X_trainb,y_trainb)\n",
    "prediciton5b = model5b.predict(X_testb)\n",
    "print('The accuracy of the XGboost is',metrics.accuracy_score(prediciton5b,y_testb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.640213\n",
      "Will train until validation_0-logloss hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-logloss:0.597801\n",
      "[2]\tvalidation_0-logloss:0.562773\n",
      "[3]\tvalidation_0-logloss:0.534449\n",
      "[4]\tvalidation_0-logloss:0.511387\n",
      "[5]\tvalidation_0-logloss:0.492102\n",
      "[6]\tvalidation_0-logloss:0.476113\n",
      "[7]\tvalidation_0-logloss:0.464301\n",
      "[8]\tvalidation_0-logloss:0.452637\n",
      "[9]\tvalidation_0-logloss:0.444271\n",
      "[10]\tvalidation_0-logloss:0.436633\n",
      "[11]\tvalidation_0-logloss:0.4293\n",
      "[12]\tvalidation_0-logloss:0.424387\n",
      "[13]\tvalidation_0-logloss:0.418741\n",
      "[14]\tvalidation_0-logloss:0.414489\n",
      "[15]\tvalidation_0-logloss:0.410087\n",
      "[16]\tvalidation_0-logloss:0.406642\n",
      "[17]\tvalidation_0-logloss:0.4037\n",
      "[18]\tvalidation_0-logloss:0.400618\n",
      "[19]\tvalidation_0-logloss:0.397354\n",
      "[20]\tvalidation_0-logloss:0.39657\n",
      "[21]\tvalidation_0-logloss:0.393926\n",
      "[22]\tvalidation_0-logloss:0.391482\n",
      "[23]\tvalidation_0-logloss:0.390668\n",
      "[24]\tvalidation_0-logloss:0.389957\n",
      "[25]\tvalidation_0-logloss:0.388988\n",
      "[26]\tvalidation_0-logloss:0.387029\n",
      "[27]\tvalidation_0-logloss:0.386733\n",
      "[28]\tvalidation_0-logloss:0.386069\n",
      "[29]\tvalidation_0-logloss:0.385068\n",
      "[30]\tvalidation_0-logloss:0.385632\n",
      "[31]\tvalidation_0-logloss:0.385581\n",
      "[32]\tvalidation_0-logloss:0.385616\n",
      "[33]\tvalidation_0-logloss:0.385791\n",
      "[34]\tvalidation_0-logloss:0.386494\n",
      "[35]\tvalidation_0-logloss:0.386805\n",
      "[36]\tvalidation_0-logloss:0.387213\n",
      "[37]\tvalidation_0-logloss:0.387828\n",
      "[38]\tvalidation_0-logloss:0.388322\n",
      "[39]\tvalidation_0-logloss:0.388876\n",
      "Stopping. Best iteration:\n",
      "[29]\tvalidation_0-logloss:0.385068\n",
      "\n",
      "The accuracy of the XGBoost is 0.8249158249158249\n"
     ]
    }
   ],
   "source": [
    "# fit model on training data\n",
    "model6 = XGBClassifier()\n",
    "eval_set = [(X_test,y_test)]\n",
    "model6.fit(X_train,y_train, early_stopping_rounds=10, eval_metric=\"logloss\",\n",
    "eval_set=eval_set, verbose=True)\n",
    "prediciton6 = model6.predict(X_test)\n",
    "print('The accuracy of the XGBoost is',metrics.accuracy_score(prediciton6,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.640213\n",
      "Will train until validation_0-logloss hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-logloss:0.597763\n",
      "[2]\tvalidation_0-logloss:0.562734\n",
      "[3]\tvalidation_0-logloss:0.534402\n",
      "[4]\tvalidation_0-logloss:0.511728\n",
      "[5]\tvalidation_0-logloss:0.492292\n",
      "[6]\tvalidation_0-logloss:0.476072\n",
      "[7]\tvalidation_0-logloss:0.464249\n",
      "[8]\tvalidation_0-logloss:0.453967\n",
      "[9]\tvalidation_0-logloss:0.443664\n",
      "[10]\tvalidation_0-logloss:0.436056\n",
      "[11]\tvalidation_0-logloss:0.428675\n",
      "[12]\tvalidation_0-logloss:0.423898\n",
      "[13]\tvalidation_0-logloss:0.41931\n",
      "[14]\tvalidation_0-logloss:0.415246\n",
      "[15]\tvalidation_0-logloss:0.409983\n",
      "[16]\tvalidation_0-logloss:0.407494\n",
      "[17]\tvalidation_0-logloss:0.405578\n",
      "[18]\tvalidation_0-logloss:0.402158\n",
      "[19]\tvalidation_0-logloss:0.399584\n",
      "[20]\tvalidation_0-logloss:0.398819\n",
      "[21]\tvalidation_0-logloss:0.397023\n",
      "[22]\tvalidation_0-logloss:0.395548\n",
      "[23]\tvalidation_0-logloss:0.393958\n",
      "[24]\tvalidation_0-logloss:0.393005\n",
      "[25]\tvalidation_0-logloss:0.390919\n",
      "[26]\tvalidation_0-logloss:0.389907\n",
      "[27]\tvalidation_0-logloss:0.389541\n",
      "[28]\tvalidation_0-logloss:0.38848\n",
      "[29]\tvalidation_0-logloss:0.388113\n",
      "[30]\tvalidation_0-logloss:0.388269\n",
      "[31]\tvalidation_0-logloss:0.388961\n",
      "[32]\tvalidation_0-logloss:0.38898\n",
      "[33]\tvalidation_0-logloss:0.389126\n",
      "[34]\tvalidation_0-logloss:0.38953\n",
      "[35]\tvalidation_0-logloss:0.390527\n",
      "[36]\tvalidation_0-logloss:0.39088\n",
      "[37]\tvalidation_0-logloss:0.391058\n",
      "[38]\tvalidation_0-logloss:0.391741\n",
      "[39]\tvalidation_0-logloss:0.391836\n",
      "Stopping. Best iteration:\n",
      "[29]\tvalidation_0-logloss:0.388113\n",
      "\n",
      "The accuracy of the XGBoost is 0.8249158249158249\n"
     ]
    }
   ],
   "source": [
    "# fit model on training data\n",
    "model6b = XGBClassifier()\n",
    "eval_set = [(X_testb,y_testb)]\n",
    "model6b.fit(X_trainb,y_trainb, early_stopping_rounds=10, eval_metric=\"logloss\",\n",
    "eval_set = eval_set, verbose=True)\n",
    "prediciton6b = model6b.predict(X_testb)\n",
    "print('The accuracy of the XGBoost is',metrics.accuracy_score(prediciton6,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear Svm</th>\n",
       "      <td>0.866050</td>\n",
       "      <td>0.040509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Radial Svm</th>\n",
       "      <td>0.855886</td>\n",
       "      <td>0.023824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.859282</td>\n",
       "      <td>0.038899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.850319</td>\n",
       "      <td>0.044860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.831078</td>\n",
       "      <td>0.030612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.836747</td>\n",
       "      <td>0.024006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.864939</td>\n",
       "      <td>0.030765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      CV Mean       Std\n",
       "Linear Svm           0.866050  0.040509\n",
       "Radial Svm           0.855886  0.023824\n",
       "Logistic Regression  0.859282  0.038899\n",
       "KNN                  0.850319  0.044860\n",
       "Decision Tree        0.831078  0.030612\n",
       "Naive Bayes          0.836747  0.024006\n",
       "Random Forest        0.864939  0.030765"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "xyz=[]\n",
    "accuracy=[]\n",
    "std=[]\n",
    "classifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\n",
    "models=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\n",
    "for i in models:\n",
    "    model = i\n",
    "    cv_result = cross_val_score(model,X_train,y_train, cv = kfold,scoring = \"accuracy\")\n",
    "    cv_result=cv_result\n",
    "    xyz.append(cv_result.mean())\n",
    "    std.append(cv_result.std())\n",
    "    accuracy.append(cv_result)\n",
    "new_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \n",
    "new_models_dataframe2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:867: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 28 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f,ax=plt.subplots(7,2, figsize=(12,10))\n",
    "\n",
    "y_pred = cross_val_predict(svm.SVC(kernel='rbf'),X_train,y_train,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')\n",
    "ax[0,0].set_title('Matrix for rbf-SVM1')\n",
    "\n",
    "y_predb = cross_val_predict(svm.SVC(kernel='rbf'),X_trainb,y_trainb,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_trainb,y_predb),ax=ax[0,1],annot=True,fmt='2.0f')\n",
    "ax[0,1].set_title('Matrix for rbf-SVM1  - b')\n",
    "\n",
    "\n",
    "y_pred = cross_val_predict(svm.SVC(kernel='linear'),X_train,y_train,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')\n",
    "ax[1,0].set_title('Matrix for Linear-SVM1')\n",
    "\n",
    "y_predb = cross_val_predict(svm.SVC(kernel='linear'),X_trainb,y_trainb,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_trainb,y_predb),ax=ax[1,1],annot=True,fmt='2.0f')\n",
    "ax[1,1].set_title('Matrix for Linear-SVM1  - b')\n",
    "\n",
    "y_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X_train,y_train,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')\n",
    "ax[2,0].set_title('Matrix for KNN1')\n",
    "\n",
    "y_predb = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X_trainb,y_trainb,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_trainb,y_predb),ax=ax[2,1],annot=True,fmt='2.0f')\n",
    "ax[2,1].set_title('Matrix for KNN1  - b')\n",
    "\n",
    "y_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X_train,y_train,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[3,0],annot=True,fmt='2.0f')\n",
    "ax[3,0].set_title('Matrix for Random-Forests1')\n",
    "\n",
    "y_predb = cross_val_predict(RandomForestClassifier(n_estimators=100),X_trainb,y_trainb,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_trainb,y_predb),ax=ax[3,1],annot=True,fmt='2.0f')\n",
    "ax[3,1].set_title('Matrix for Random-Forests1  - b')\n",
    "\n",
    "y_pred = cross_val_predict(LogisticRegression(),X_train,y_train,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[4,0],annot=True,fmt='2.0f')\n",
    "ax[4,0].set_title('Matrix for Logistic Regression1')\n",
    "\n",
    "y_predb = cross_val_predict(LogisticRegression(),X_trainb,y_trainb,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_trainb,y_predb),ax=ax[4,1],annot=True,fmt='2.0f')\n",
    "ax[4,1].set_title('Matrix for Logistic Regression1  - b')\n",
    "\n",
    "y_pred = cross_val_predict(DecisionTreeClassifier(),X_train,y_train,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[5,0],annot=True,fmt='2.0f')\n",
    "ax[5,0].set_title('Matrix for Decision Tree1')\n",
    "\n",
    "y_predb = cross_val_predict(DecisionTreeClassifier(),X_train,y_train,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_trainb,y_predb),ax=ax[5,1],annot=True,fmt='2.0f')\n",
    "ax[5,1].set_title('Matrix for Decision Tree1  - b')\n",
    "\n",
    "y_pred = cross_val_predict(GaussianNB(),X_train,y_train,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[6,0],annot=True,fmt='2.0f')\n",
    "ax[6,0].set_title('Matrix for Naive Bayes1')\n",
    "\n",
    "y_predb = cross_val_predict(GaussianNB(),X_trainb,y_trainb,cv=10)\n",
    "sns.heatmap(confusion_matrix(y_trainb,y_predb),ax=ax[6,1],annot=True,fmt='2.0f')\n",
    "ax[6,1].set_title('Matrix for Naive Bayes1 - b')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.2,wspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(path):\n",
    "    image = face_recognition.load_image_file(path)\n",
    "    pic = cv2.imread(str(path))\n",
    "    \n",
    "    \n",
    "    gray_image = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n",
    "    gray_image = cv2.cvtColor(gray_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    faces = detector(gray_image)\n",
    "    landmark_listed = []\n",
    "\n",
    "    faceFound = False\n",
    "    for face in faces:\n",
    "        \n",
    "        faceFound = True\n",
    "\n",
    "        landmarks = predictor(gray_image, face)\n",
    "        for n in range(0, 68):\n",
    "            x = landmarks.part(n).x\n",
    "            y = landmarks.part(n).y\n",
    "            cv2.circle(image, (x, y), 4, (255, 0, 0), -1)\n",
    "            landmark_listed.append(x)\n",
    "            landmark_listed.append(y)\n",
    "        break\n",
    "    \n",
    "    if faceFound != True:\n",
    "        print(\"Face found in the picture\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    X = np.asarray(landmark_listed, dtype=np.float32)\n",
    "    \n",
    "    X = scaler.fit_transform(X.reshape(-1,1))\n",
    "    \n",
    "    X_b = np.delete(X.reshape(1,-1), [10,11,22,23,26,27,28,29,44,45,56,57,64,65,68,69,72,73,76,77,86,87,100,101,106,107,110,111,112,113,116,117,130,131,134,135], 1)\n",
    "\n",
    "    \n",
    "    #prediction\n",
    "    prediction = []\n",
    "    \n",
    "    pred = int(model1.predict(X.reshape(1,-1)))\n",
    "    prediction.append(pred)\n",
    "    \n",
    "    pred = int(model2.predict(X.reshape(1,-1)))\n",
    "    prediction.append(pred)\n",
    "    \n",
    "    \n",
    "    pred = int(model4.predict(X.reshape(1,-1)))\n",
    "    prediction.append(pred)\n",
    "    \n",
    "    pred = int(model4b.predict(X_b.reshape(1,-1)))\n",
    "    prediction.append(pred)\n",
    "    \n",
    "    pred = int(model7.predict(X.reshape(1,-1)))\n",
    "    prediction.append(pred)\n",
    "    \n",
    "    if mode(prediction) == 0:\n",
    "        print(\"happy face\")\n",
    "    else:\n",
    "        print(\"sad face\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy face\n"
     ]
    }
   ],
   "source": [
    "predict('2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
